{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Lenticular Lens Documentation A platform for addressing various aspects of the disambiguation of Entities . These aspects include the creation , manipulation , validation , provenance and visualisation of links. Developped in the context of RISIS & the Golden Agents project \u00b6 Work in progress At present, this documentation is not fully in the image of current Lenticular Lens . However, the currently work in progress UI is meant to be inline with the documentation.","title":"Home"},{"location":"#the-lenticular-lens-documentation-a-platform-for-addressing-various-aspects-of-the-disambiguation-of-entities-these-aspects-include-the-creation-manipulation-validation-provenance-and-visualisation-of-links-developped-in-the-context-of-risis-the-golden-agents-project","text":"Work in progress At present, this documentation is not fully in the image of current Lenticular Lens . However, the currently work in progress UI is meant to be inline with the documentation.","title":" The Lenticular Lens Documentation  A platform for addressing various aspects of the  disambiguation of Entities .  These aspects include the creation,  manipulation,  validation,  provenance and  visualisation  of links.  Developped in the context of RISIS &amp; the Golden Agents project"},{"location":"01.Introduction/","text":"INTRODUCTION \u00b6 Time and again, researchers are presented with problems for which they postulate and test hypotheses in order to provide us with robust explanations for research questions successfully investigated. Often enough, solid explanations for complex problems require exploring a multitude of datasources. This is the case, for example, in domains such as e-science (multiple scientific datasets), e-commerce (multiple product vendors), tourism (multiple data providers), e-social science, digital humanities, etc. 1. Context \u00b6 DATA INTEGRATION. The use of various datasources come at the expense of heterogeneity which obfuscates the path to data integration and thereby hinders accurately addressing complex problems. Dealing with multiple datasources or data providers highlights the freedom at which providers document facets of the same entity. Indeed, this feature of oriented freedom for entity descriptions can explain, to a certain extent, the inherent difficulty to integrate heterogeneous datasources. In the Semantic Web, this problem is partially circumvented as any pair of resources can be linked regardless to the uniformity of data representation or vocabulary used, i.e. uniformizing data/schema is not a prerequisite for data integration provided that links between co-referent entities across heterogeneous datasets exist. RESULT QUALITY. The quality of supporting evidence for accepting or rejecting the hypothesis under investigation for a complex problem greatly depends on the correctness of the links integrating the underlying datasources. This begs for questions regarding the aboutness and correctness of the links, such as: How to create correct links? More specifically, are there reliable tools or macthing algorithms for linking co-referent descriptions of entity scattered across various heterogeneous datasets? Can different tools or algorithms be combined? How to judge the applicability of links? Can their quality be estimated? Can they be improved, manipulated, visualised for validation purposes, reproduced? More specifically, is there a platform that supports all of the aforementioned concerns? LINK CONSTRUCTION. Through the years, a number of entity matching tools have been developed and tested. However, some of theses tools have been developed for specific datasets, while others have limited applicability as they have mainly been tested in domain specific areas using synthetic or simplistic data, generally from at most two datasets. For example, as research in social sciences is increasingly based on multiple heterogeneous datasources, it becomes problematic to be limited to the integration of two datasets. Furthermore, in practice, the heterogeneity, messiness, incompleteness of data raise the bar higher in terms of entity matching complexity. In other words, most matching algorithms have been successfully applied in limited and controlled environments. This motivates the need for having the means to (re)use and combine generic matching approaches in order to solve specific problems. 2. Disambiguation Proposal \u00b6 In this document, we present a tool that supports disambiguation over multiple datasets: the Lenticular Lens . As a domain agnostic approach, the Lenticular Lens tool reuses existing matching approaches to allow for the user to reach their goals in ways that alleviates some of the main aforementioned issues: User-dependent and context-specific link discovery. User-based explicit concept mapping. Integration of more than two datasets. Combining entity matching algorithms and results. Structure-based evaluation of identity link networks. Enabling visualisation to support link/cluster validation. Metadata for documenting the link aboutness and enabling reproducibility. The context dependent link discovery idea developed and implemented in the Lenticular Lens [ Idrissou 2017 , 2018 , 2019 ] is an extension of the Linkset and Lens concepts introduced by the [ OpenPhacts ] project in the quests for building an infrastructure for integrating pharmacological data. Our extension and broadening of these concepts enabled us to design and build a flexible tool for undertaking entity disambiguation in a broader perspective. 2.1 Workflow \u00b6 The Lenticular Lens tool offers the users a number of matching-related mandatory and optional tasks to achieve their integration-goals. A workflow depicting possible sequences of executions for those tasks is presented in Figure 1. The process starts with (1) Data Selection and is followed by (2) Link Creation (Linkset). The next optional steps include: (3) Link Manipulation (Lens) for combining previously obtained links; (4) Link Validation for the curation of the obtained links; (5) Link Visualisation to support the verification of the quality of the obtained links; (6) Export Linksets/Lenses to extract the obtained links from the tool and finally (7) Export Integrated Data of Interest to support the user on the extraction of specific descriptions (property values) of integrated entities. Fig. 1: The Lenticular Lens\u2019 workflow. 2.1 Dataflow \u00b6 The data used as input to the Lenticular Lens originates from a Timbuctoo Endpoint. The pulled data is then stored and manipulated in PostgreSQL for efficiency purposes. The resultant links generated by the Lenticular Lens tool can be injected back into the Timbuctoo Enpoint, which in turn also enables communication with a SPARQL Enpoint. Fig. 2: The Lenticular Lens\u2019 dataflow. 3. GUI Menus \u00b6 As a preview of what can be done with the Lenticular Lens tool, we list here the main menus composing the tool and provide a brief description of what can be done in each of the menu. CONTEXT This menu aims to specify the scope of the research by giving the researcher the opportunity to remind herself and other users the why of the research. This is done by providing a title, describing the goal and possibly linking the work done in this tool to a published result. DATA This menu provides means to: Select a GraphQL endpoint so that remote datasources can be located and made available to the user. The default selection is the Golden Agent\u2019s endpoint ; Select Datasources and Entity-types from the available list at the remote location so that information can be extracted and integrated in order to conduct an experiment; Define restrictions over selected Entity-types. Explore data based on pre-defined specifications (Datasource, Entity-type and Property-value Restrictions). LINKSET This menu provides means to: Specify the conditions in which to use a matching method or combined matching methods for the creation of a LINKSET, which is a set of links sharing the same specifications. Run the specification i.e create set of links and clusters. Get statistics on the result such as the total number of links found, how they cluster\u2026 Validate the resulting links by annotating them with flags such as accept or reject. Export the result in various format. LENS This menu provides means to: Specify the conditions in which to combine LINKSET(S) and/or LENS(ES) for the creation of a new LENS, specifically, the use of set like operations (Union, Intersection, Difference, Composition and In-Set) over linksets and lenses. Run the specification i.e combine Linkset(s) and/or Lens(es) and cluster the links. Get statistics on the result such as the total number of links found, how they cluster\u2026 Visualise and Validate the resulting links by annotating them with flags such as accept or reject. Export the result in various format. DATA INTEGRATION The menu here enables the user to materialise an entity-based integration of her selected datasources for the extraction of information vital to her analysis. The rest of the manual will discuss Terminology Link Annotation Ontology Algorithms Link Construction (it includes the RESEARCH , DATA and CREATE options) Link Manipulation (it includes the MANIPULATE , and VALIDATION options) Link Export (it is about the EXPORT option) and Data Integration (it is about the EXTRACT option)","title":"1. Introduction"},{"location":"01.Introduction/#introduction","text":"Time and again, researchers are presented with problems for which they postulate and test hypotheses in order to provide us with robust explanations for research questions successfully investigated. Often enough, solid explanations for complex problems require exploring a multitude of datasources. This is the case, for example, in domains such as e-science (multiple scientific datasets), e-commerce (multiple product vendors), tourism (multiple data providers), e-social science, digital humanities, etc.","title":"INTRODUCTION"},{"location":"01.Introduction/#1-context","text":"DATA INTEGRATION. The use of various datasources come at the expense of heterogeneity which obfuscates the path to data integration and thereby hinders accurately addressing complex problems. Dealing with multiple datasources or data providers highlights the freedom at which providers document facets of the same entity. Indeed, this feature of oriented freedom for entity descriptions can explain, to a certain extent, the inherent difficulty to integrate heterogeneous datasources. In the Semantic Web, this problem is partially circumvented as any pair of resources can be linked regardless to the uniformity of data representation or vocabulary used, i.e. uniformizing data/schema is not a prerequisite for data integration provided that links between co-referent entities across heterogeneous datasets exist. RESULT QUALITY. The quality of supporting evidence for accepting or rejecting the hypothesis under investigation for a complex problem greatly depends on the correctness of the links integrating the underlying datasources. This begs for questions regarding the aboutness and correctness of the links, such as: How to create correct links? More specifically, are there reliable tools or macthing algorithms for linking co-referent descriptions of entity scattered across various heterogeneous datasets? Can different tools or algorithms be combined? How to judge the applicability of links? Can their quality be estimated? Can they be improved, manipulated, visualised for validation purposes, reproduced? More specifically, is there a platform that supports all of the aforementioned concerns? LINK CONSTRUCTION. Through the years, a number of entity matching tools have been developed and tested. However, some of theses tools have been developed for specific datasets, while others have limited applicability as they have mainly been tested in domain specific areas using synthetic or simplistic data, generally from at most two datasets. For example, as research in social sciences is increasingly based on multiple heterogeneous datasources, it becomes problematic to be limited to the integration of two datasets. Furthermore, in practice, the heterogeneity, messiness, incompleteness of data raise the bar higher in terms of entity matching complexity. In other words, most matching algorithms have been successfully applied in limited and controlled environments. This motivates the need for having the means to (re)use and combine generic matching approaches in order to solve specific problems.","title":"1. Context"},{"location":"01.Introduction/#2-disambiguation-proposal","text":"In this document, we present a tool that supports disambiguation over multiple datasets: the Lenticular Lens . As a domain agnostic approach, the Lenticular Lens tool reuses existing matching approaches to allow for the user to reach their goals in ways that alleviates some of the main aforementioned issues: User-dependent and context-specific link discovery. User-based explicit concept mapping. Integration of more than two datasets. Combining entity matching algorithms and results. Structure-based evaluation of identity link networks. Enabling visualisation to support link/cluster validation. Metadata for documenting the link aboutness and enabling reproducibility. The context dependent link discovery idea developed and implemented in the Lenticular Lens [ Idrissou 2017 , 2018 , 2019 ] is an extension of the Linkset and Lens concepts introduced by the [ OpenPhacts ] project in the quests for building an infrastructure for integrating pharmacological data. Our extension and broadening of these concepts enabled us to design and build a flexible tool for undertaking entity disambiguation in a broader perspective.","title":"2. Disambiguation Proposal"},{"location":"01.Introduction/#21-workflow","text":"The Lenticular Lens tool offers the users a number of matching-related mandatory and optional tasks to achieve their integration-goals. A workflow depicting possible sequences of executions for those tasks is presented in Figure 1. The process starts with (1) Data Selection and is followed by (2) Link Creation (Linkset). The next optional steps include: (3) Link Manipulation (Lens) for combining previously obtained links; (4) Link Validation for the curation of the obtained links; (5) Link Visualisation to support the verification of the quality of the obtained links; (6) Export Linksets/Lenses to extract the obtained links from the tool and finally (7) Export Integrated Data of Interest to support the user on the extraction of specific descriptions (property values) of integrated entities. Fig. 1: The Lenticular Lens\u2019 workflow.","title":"2.1 Workflow"},{"location":"01.Introduction/#21-dataflow","text":"The data used as input to the Lenticular Lens originates from a Timbuctoo Endpoint. The pulled data is then stored and manipulated in PostgreSQL for efficiency purposes. The resultant links generated by the Lenticular Lens tool can be injected back into the Timbuctoo Enpoint, which in turn also enables communication with a SPARQL Enpoint. Fig. 2: The Lenticular Lens\u2019 dataflow.","title":"2.1 Dataflow"},{"location":"01.Introduction/#3-gui-menus","text":"As a preview of what can be done with the Lenticular Lens tool, we list here the main menus composing the tool and provide a brief description of what can be done in each of the menu. CONTEXT This menu aims to specify the scope of the research by giving the researcher the opportunity to remind herself and other users the why of the research. This is done by providing a title, describing the goal and possibly linking the work done in this tool to a published result. DATA This menu provides means to: Select a GraphQL endpoint so that remote datasources can be located and made available to the user. The default selection is the Golden Agent\u2019s endpoint ; Select Datasources and Entity-types from the available list at the remote location so that information can be extracted and integrated in order to conduct an experiment; Define restrictions over selected Entity-types. Explore data based on pre-defined specifications (Datasource, Entity-type and Property-value Restrictions). LINKSET This menu provides means to: Specify the conditions in which to use a matching method or combined matching methods for the creation of a LINKSET, which is a set of links sharing the same specifications. Run the specification i.e create set of links and clusters. Get statistics on the result such as the total number of links found, how they cluster\u2026 Validate the resulting links by annotating them with flags such as accept or reject. Export the result in various format. LENS This menu provides means to: Specify the conditions in which to combine LINKSET(S) and/or LENS(ES) for the creation of a new LENS, specifically, the use of set like operations (Union, Intersection, Difference, Composition and In-Set) over linksets and lenses. Run the specification i.e combine Linkset(s) and/or Lens(es) and cluster the links. Get statistics on the result such as the total number of links found, how they cluster\u2026 Visualise and Validate the resulting links by annotating them with flags such as accept or reject. Export the result in various format. DATA INTEGRATION The menu here enables the user to materialise an entity-based integration of her selected datasources for the extraction of information vital to her analysis. The rest of the manual will discuss Terminology Link Annotation Ontology Algorithms Link Construction (it includes the RESEARCH , DATA and CREATE options) Link Manipulation (it includes the MANIPULATE , and VALIDATION options) Link Export (it is about the EXPORT option) and Data Integration (it is about the EXTRACT option)","title":"3. GUI Menus"},{"location":"02.Terminology/","text":".katex img { display: block; position: absolute; width: 100%; height: inherit; } TERMINOLOGY \u00b6 Before diving into how to use the Lenticular Lens , we define a number of terms we believe to be important for a better comprehension of what is offered by the tool. For some of the concepts defined here, we opt for a broader scope which fits best problems encounter in every day life . RDF \u00b6 As described by the W3C , RDF is a graph-based data model for interchanging data on the Web and it stands for Resource Description Framework. In other words, Lexico , the Oxford supported dictionary, emphasises on the semantic aspect by defining it as a model for encoding semantic relationships between items of data so that these relationships can be interpreted computationally . Expressing a relationship between a pair of RDF resources is done as a sequence of three terms \u27e8 subject relation object . \u27e9 \\lang \\text{subject relation object .}\\rang \u27e8 subject relation object . \u27e9 . The sequence of terms is called a triple where each of its terms is separated by whitespace and the sequence is terminated by \u2019.\u2019 . The example below illustrates two simple triple syntax. The triples, not only do they provid Spiderman\u2019s mane and assert that Spiderman and Peter Parker are the same, but they also provid identification for Spiderman , Peter Parker and the vocabulary terms name and sameAs . In RDF, any triple is an RDF STATEMENT . There exist various syntaxes for representing triple statements. For further reading, see N-Triples , Notation3 , N-Quads , Turtle , RDF XML , RDFa , JSON-LD \u2026 Example 1 : Triples in N-TRIPLE syntax <http://example.org/hero#Spiderman> <http://xmlns.com/foaf/0.1/name> \"Peter-Parker\" . <http://example.org/hero#Spiderman> <http://www.w3.org/2002/07/owl#sameAs> <http://example.org/person#Peter-Parker> . Example 2 : Triples in Turtle syntax @prefix ex: <http://example.org/#> . @prefix rel: <http://www.perceive.net/schemas/relationship/> . ex: green-goblin rel: enemyOf ex: Spiderman . ex: spiderman foaf: name ex: Peter-Parker . Resource \u00b6 \u201cAnything can be a resource, including physical things, documents, abstract concepts, numbers and strings.\u201d RDF 1.1 Referents \u00b6 IRI - URI - URL - URN constitute ways in which things (resources) in the real world can be referred to in the digital world. \u201cThe resource denoted by an IRI is called its referent , and the resource denoted by a literal is called its literal value.\u201d RDF 1.1 . In the IRI example provided below, we illustrate six IRIs referents of resources of various types (hero, villain and vocabulary). Example 3 : IRIs A resource's REFERENT is an IRI. A resource's LITERAL VALUE is a LITERAL. http://example.org/villain#Green-Goblin http://example.org/villain#Thanos http://example.org/hero#Spiderman http://example.org/person#Peter-Parker http://www.perceive.net/schemas/relationship/enemyOf http://xmlns.com/foaf/0.1/name Fusion provides nice wording of all these terms as provided below. In short, URL, URN and URI are all specific types of an IRI. URI \u00b6 \u201cA Uniform Resource Identifier is a compact sequence of characters that identifies an abstract or physical resource. The set of characters is limited to US-ASCII excluding some reserved characters ( ; / ? : @ = & ). Characters outside the set of allowed characters can be represented using Percent-Encoding. A URI can be used as a locator , a name , or both. If a URI is a locator, it describes a resource\u2019s primary access mechanism. If a URI is a name, it identifies a resource by giving it a unique name. The exact specifications of syntax and semantics of a URI depend on the used Scheme that is defined by the characters before the first colon. [ RFC3986 ]\u201d URN \u00b6 \u201cA Uniform Resource Name is a URI in the scheme urn intended to serve as persistent, location-independent, resource identifier. Historically, the term also referred to any URI. [RFC3986] A URN consists of a Namespace Identifier (NID) and a Namespace Specific String (NSS): urn: : The syntax and semantics of the NSS is specific for each NID. Beside the registered NIDs, there exist several more NIDs, that did not go through the official registration process. [ RFC2141 ]\u201d URL \u00b6 \u201cA Uniform Resource Locator is a URI that, in addition to identifying a resource, provides a means of locating the resource by describing its primary access mechanism [RFC3986]. As there is no exact definition of URL by means of a set of Schemes, URL is a useful but informal concept, usually referring to a subset of URIs that do not contain URNs [ RFC3305 ].\u201d IRI \u00b6 \u201cAn Internationalized Resource Identifier is defined similarly to a URI, but the character set is extended to the Universal Coded Character Set. Therefore, it can contain any Latin and non Latin characters except the reserved characters. Instead of extending the definition of URI, the term IRI was introduced to allow for a clear distinction and avoid incompatibilities. IRIs are meant to replace URIs in identifying resources in situations where the Universal Coded Character Set is supported. By definition, every URI is an IRI. Furthermore, there is a defined surjective mapping of IRIs to URIs: Every IRI can be mapped to exactly one URI, but different IRIs might map to the same URI. Therefore, the conversion back from a URI to an IRI may not produce the original IRI. [ RFC3987 ]\u201d RDF Link \u00b6 It is also known as a correspondent triple [ Euzenat2013 ] or simply link in RDF and [ VoID ]. Links are triples in the form of \u27e8 subject relation object . \u27e9 \\lang \\text{subject relation object .}\\rang \u27e8 subject relation object . \u27e9 and are meant to exist only between two datasets. However, beside the incentive for providers to link their data to existing datasets, we do not see a good reason for a link not to exist within the same dataset. Thereby, in this document, an RDF link is a relation between two resources ( digital representations of entities presented as IRIs ) regardless of the datasource they stem from. This means that a link can involve a minimum of one dataset and a maximum of two. Identity Link \u00b6 An equality relation between two resources ( digital representations of the same real world entity presented as URIs ) regardless of the datasource they stem from. In this document we often use the term link to denote identity link unless otherwise stated. Mainly described using the predicate owl:sameAs , such encoded semantic relationship entails full equality between two resources. This semantic interpretation applies independently of context even though in real life problems, the equality between resources may depend not only on their intrinsic properties but also on the purpose or task for which they are used. In this regard, we present in the Export menu section the Lenticular Lens approach on the use and documentation of identity links in practice. Identity Link Network \u00b6 A set of co-referent entities regardless of the data they stem from. This can also be viewed as an Identity Cluster or Identity Network of co-referent entities. Link Validation \u00b6 The process of accepting or rejecting a link. This process is documented with the validation status (accepted or rejected) of the link and the supporting reasons. Named-graph \u00b6 According to Wikipedia , named graphs are a key concept of Semantic Web architecture in which a set of Resource Description Framework statements (a graph) are identified using a IRI, allowing descriptions to be made of that set of statements such as context, provenance information or other such metadata. Example 4 : Named-graph The example below illustrates a turtle named graph syntax of a linkset of three links using the owl:sameAs linktype. In the example, http://example.org/#linkset-1 is the IRI of the named graph. For more detail on Turtle RDF syntax, See https://www.w3.org/TR/turtle/#simple-triples . In this page, EXAMPLE 9 illustrates different ways of writing triples in Turtle. @base <http://example.org/> . @prefix ex: <http://example.org/#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix foaf: <http://xmlns.com/foaf/0.1/> . @prefix sim: <http://purl.org/ontology/similarity/> . ex: linkset-1 { ex: Chiara owl: sameAs ex: Latronico . ex: Al owl: sameAs ex: Al_Idrissou . ex: Al owl: sameAs ex: Al_Koudous . } Default Graph \u00b6 In a triple-store jargon, any triple without a specific named-graph ends up in the default graph . In the example below, the triple at line 4 is located in the default graph while triples at line 8 and 9 are in an explicitly stated graph , in the named-graph: ex:linkset-1. Example 5 : Default Named-graph @prefix ex: <http://example.org/#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . ex: Chiara owl: sameAs ex: Latronico . ex: graph-1 { ex: Al owl: sameAs ex: Al_Idrissou . ex: Al owl: sameAs ex: Al_Koudous . } Linkset \u00b6 In [ VoID ], a linkset is a collection of RDF links between two datasets where all subjects stem from one dataset and all objects from the other dataset. In here, we alleviate such strict restriction on the number of linked datasets to be one , two or more . Therefore, here a linkset is a collection of RDF links between dataset(s) , using the same link predicate (regardless of the link being an equality predicate or not). This predicate is called the linktype of the linkset. Lens \u00b6 Content-wise , a lens is a type of RDF linkset (in the sense that it is a collection of links sharing the same linktype) also involving one, two or more datasets. However, context-wise , it differs from a linkset as it is generated using different process (set-like link manipulation operators such as Union, Intersection, Difference or Transitivity) as compared to how a linkset comes about (matching algorithms). Linkset / Lens Specification \u00b6 In the Lenticular Lens , a Linkset / Lens specification can be viewed as a reproducibility metadata. It provides information on how set of links came to be. In other worlds, it explicitly defines the context in which a link hold and supports decisions making during the validation process (accepting or rejecting a link). Matching Algorithm \u00b6 A set of rules followed by a computer for finding pairs of matching resources. An algorithm can be further categorised as automated or semi-automated depending on whether it requires user assistance or not. Most matching algorithms employed in the Lenticular Lens are semi-automated. Fuzzy Logic \u00b6 From the family of Many-Valued Logics, Fuzzy Logic is a computing approach based on degrees of Truth stemmed form the unit [0, 1] truth space. Modelling reasoning with vague propositions is often addressed with Fuzzy Logic. A proposition, statement or event is said to be vague ( the tomato is ripe ) when it is hard or sometimes impossible to establish whether the proposition is completely true or false due to the involvement of vague concepts ( ripe ), thereby begging for the use of degrees of truth as oppose to binary truth values {0, 1} . \u201cFor example, we cannot exactly say whether a tomato is ripe or not, but rather can only say that the tomato is ripe to some degree\u201d [ Lukasiewicz2008 ]. Degrees of Truth is not to be confused with the Degrees of Certainty / Confidence as the latter is not an assignment of truth value but rather an evaluation of a weighted proposition (proposition with an assigned truth value) regardless of its truth value space ({0, 1} or [0, 1]). Turtle Prefixes \u00b6 Example 6 : Prefixes used in this manual. Below are the prefixes used in this document @prefix A: <http://example.org/A#> . @prefix B: <http://example.org/B#> . @prefix C: <http://example.org/C#> . @prefix dataset: <http://example.org/dataset#> . @prefix dcterms: <http://purl.org/dc/terms/> . @prefix ex: <http://example.org/#> . @prefix foaf: <http://xmlns.com/foaf/0.1/> . @prefix format: <http://www.w3.org/ns/formats/> . @prefix grid: <http://www.grid.ac/ontology/> . @prefix gridC: <http://vivoweb.org/ontology/core#> . @prefix hero: <http://example.org/hero#> . @prefix institutes: <http://www.grid.ac/institutes/> . @prefix law: <http://www.opendatacommons.org/> @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix person: <http://example.org/person#> . @prefix prov: <http://www.w3.org/ns/prov#> . @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> @prefix rel: <http://www.perceive.net/schemas/relationship/> . @prefix sim: <http://purl.org/ontology/similarity/> . @prefix singleton: <http://lenticularlens.org/singleton#> @prefix void: <http://rdfs.org/ns/void#> . @prefix voidPlus: <http://lenticularlens.org/voidPlus#> . @prefix validation: <http://vocabulary/validation#> . @prefix wiki: <http://en.wikipedia.org/wiki/> @prefix wikidata: <http://www.wikidata.org/entity/> .","title":"2. Terminology"},{"location":"02.Terminology/#terminology","text":"Before diving into how to use the Lenticular Lens , we define a number of terms we believe to be important for a better comprehension of what is offered by the tool. For some of the concepts defined here, we opt for a broader scope which fits best problems encounter in every day life .","title":"TERMINOLOGY"},{"location":"02.Terminology/#rdf","text":"As described by the W3C , RDF is a graph-based data model for interchanging data on the Web and it stands for Resource Description Framework. In other words, Lexico , the Oxford supported dictionary, emphasises on the semantic aspect by defining it as a model for encoding semantic relationships between items of data so that these relationships can be interpreted computationally . Expressing a relationship between a pair of RDF resources is done as a sequence of three terms \u27e8 subject relation object . \u27e9 \\lang \\text{subject relation object .}\\rang \u27e8 subject relation object . \u27e9 . The sequence of terms is called a triple where each of its terms is separated by whitespace and the sequence is terminated by \u2019.\u2019 . The example below illustrates two simple triple syntax. The triples, not only do they provid Spiderman\u2019s mane and assert that Spiderman and Peter Parker are the same, but they also provid identification for Spiderman , Peter Parker and the vocabulary terms name and sameAs . In RDF, any triple is an RDF STATEMENT . There exist various syntaxes for representing triple statements. For further reading, see N-Triples , Notation3 , N-Quads , Turtle , RDF XML , RDFa , JSON-LD \u2026 Example 1 : Triples in N-TRIPLE syntax <http://example.org/hero#Spiderman> <http://xmlns.com/foaf/0.1/name> \"Peter-Parker\" . <http://example.org/hero#Spiderman> <http://www.w3.org/2002/07/owl#sameAs> <http://example.org/person#Peter-Parker> . Example 2 : Triples in Turtle syntax @prefix ex: <http://example.org/#> . @prefix rel: <http://www.perceive.net/schemas/relationship/> . ex: green-goblin rel: enemyOf ex: Spiderman . ex: spiderman foaf: name ex: Peter-Parker .","title":"RDF"},{"location":"02.Terminology/#resource","text":"\u201cAnything can be a resource, including physical things, documents, abstract concepts, numbers and strings.\u201d RDF 1.1","title":"Resource"},{"location":"02.Terminology/#referents","text":"IRI - URI - URL - URN constitute ways in which things (resources) in the real world can be referred to in the digital world. \u201cThe resource denoted by an IRI is called its referent , and the resource denoted by a literal is called its literal value.\u201d RDF 1.1 . In the IRI example provided below, we illustrate six IRIs referents of resources of various types (hero, villain and vocabulary). Example 3 : IRIs A resource's REFERENT is an IRI. A resource's LITERAL VALUE is a LITERAL. http://example.org/villain#Green-Goblin http://example.org/villain#Thanos http://example.org/hero#Spiderman http://example.org/person#Peter-Parker http://www.perceive.net/schemas/relationship/enemyOf http://xmlns.com/foaf/0.1/name Fusion provides nice wording of all these terms as provided below. In short, URL, URN and URI are all specific types of an IRI.","title":"Referents"},{"location":"02.Terminology/#uri","text":"\u201cA Uniform Resource Identifier is a compact sequence of characters that identifies an abstract or physical resource. The set of characters is limited to US-ASCII excluding some reserved characters ( ; / ? : @ = & ). Characters outside the set of allowed characters can be represented using Percent-Encoding. A URI can be used as a locator , a name , or both. If a URI is a locator, it describes a resource\u2019s primary access mechanism. If a URI is a name, it identifies a resource by giving it a unique name. The exact specifications of syntax and semantics of a URI depend on the used Scheme that is defined by the characters before the first colon. [ RFC3986 ]\u201d","title":"URI"},{"location":"02.Terminology/#urn","text":"\u201cA Uniform Resource Name is a URI in the scheme urn intended to serve as persistent, location-independent, resource identifier. Historically, the term also referred to any URI. [RFC3986] A URN consists of a Namespace Identifier (NID) and a Namespace Specific String (NSS): urn: : The syntax and semantics of the NSS is specific for each NID. Beside the registered NIDs, there exist several more NIDs, that did not go through the official registration process. [ RFC2141 ]\u201d","title":"URN"},{"location":"02.Terminology/#url","text":"\u201cA Uniform Resource Locator is a URI that, in addition to identifying a resource, provides a means of locating the resource by describing its primary access mechanism [RFC3986]. As there is no exact definition of URL by means of a set of Schemes, URL is a useful but informal concept, usually referring to a subset of URIs that do not contain URNs [ RFC3305 ].\u201d","title":"URL"},{"location":"02.Terminology/#iri","text":"\u201cAn Internationalized Resource Identifier is defined similarly to a URI, but the character set is extended to the Universal Coded Character Set. Therefore, it can contain any Latin and non Latin characters except the reserved characters. Instead of extending the definition of URI, the term IRI was introduced to allow for a clear distinction and avoid incompatibilities. IRIs are meant to replace URIs in identifying resources in situations where the Universal Coded Character Set is supported. By definition, every URI is an IRI. Furthermore, there is a defined surjective mapping of IRIs to URIs: Every IRI can be mapped to exactly one URI, but different IRIs might map to the same URI. Therefore, the conversion back from a URI to an IRI may not produce the original IRI. [ RFC3987 ]\u201d","title":"IRI"},{"location":"02.Terminology/#rdf-link","text":"It is also known as a correspondent triple [ Euzenat2013 ] or simply link in RDF and [ VoID ]. Links are triples in the form of \u27e8 subject relation object . \u27e9 \\lang \\text{subject relation object .}\\rang \u27e8 subject relation object . \u27e9 and are meant to exist only between two datasets. However, beside the incentive for providers to link their data to existing datasets, we do not see a good reason for a link not to exist within the same dataset. Thereby, in this document, an RDF link is a relation between two resources ( digital representations of entities presented as IRIs ) regardless of the datasource they stem from. This means that a link can involve a minimum of one dataset and a maximum of two.","title":"RDF Link"},{"location":"02.Terminology/#identity-link","text":"An equality relation between two resources ( digital representations of the same real world entity presented as URIs ) regardless of the datasource they stem from. In this document we often use the term link to denote identity link unless otherwise stated. Mainly described using the predicate owl:sameAs , such encoded semantic relationship entails full equality between two resources. This semantic interpretation applies independently of context even though in real life problems, the equality between resources may depend not only on their intrinsic properties but also on the purpose or task for which they are used. In this regard, we present in the Export menu section the Lenticular Lens approach on the use and documentation of identity links in practice.","title":"Identity Link"},{"location":"02.Terminology/#identity-link-network","text":"A set of co-referent entities regardless of the data they stem from. This can also be viewed as an Identity Cluster or Identity Network of co-referent entities.","title":"Identity Link Network"},{"location":"02.Terminology/#link-validation","text":"The process of accepting or rejecting a link. This process is documented with the validation status (accepted or rejected) of the link and the supporting reasons.","title":"Link Validation"},{"location":"02.Terminology/#named-graph","text":"According to Wikipedia , named graphs are a key concept of Semantic Web architecture in which a set of Resource Description Framework statements (a graph) are identified using a IRI, allowing descriptions to be made of that set of statements such as context, provenance information or other such metadata. Example 4 : Named-graph The example below illustrates a turtle named graph syntax of a linkset of three links using the owl:sameAs linktype. In the example, http://example.org/#linkset-1 is the IRI of the named graph. For more detail on Turtle RDF syntax, See https://www.w3.org/TR/turtle/#simple-triples . In this page, EXAMPLE 9 illustrates different ways of writing triples in Turtle. @base <http://example.org/> . @prefix ex: <http://example.org/#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix foaf: <http://xmlns.com/foaf/0.1/> . @prefix sim: <http://purl.org/ontology/similarity/> . ex: linkset-1 { ex: Chiara owl: sameAs ex: Latronico . ex: Al owl: sameAs ex: Al_Idrissou . ex: Al owl: sameAs ex: Al_Koudous . }","title":"Named-graph"},{"location":"02.Terminology/#default-graph","text":"In a triple-store jargon, any triple without a specific named-graph ends up in the default graph . In the example below, the triple at line 4 is located in the default graph while triples at line 8 and 9 are in an explicitly stated graph , in the named-graph: ex:linkset-1. Example 5 : Default Named-graph @prefix ex: <http://example.org/#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . ex: Chiara owl: sameAs ex: Latronico . ex: graph-1 { ex: Al owl: sameAs ex: Al_Idrissou . ex: Al owl: sameAs ex: Al_Koudous . }","title":"Default Graph"},{"location":"02.Terminology/#linkset","text":"In [ VoID ], a linkset is a collection of RDF links between two datasets where all subjects stem from one dataset and all objects from the other dataset. In here, we alleviate such strict restriction on the number of linked datasets to be one , two or more . Therefore, here a linkset is a collection of RDF links between dataset(s) , using the same link predicate (regardless of the link being an equality predicate or not). This predicate is called the linktype of the linkset.","title":"Linkset"},{"location":"02.Terminology/#lens","text":"Content-wise , a lens is a type of RDF linkset (in the sense that it is a collection of links sharing the same linktype) also involving one, two or more datasets. However, context-wise , it differs from a linkset as it is generated using different process (set-like link manipulation operators such as Union, Intersection, Difference or Transitivity) as compared to how a linkset comes about (matching algorithms).","title":"Lens"},{"location":"02.Terminology/#linkset-lens-specification","text":"In the Lenticular Lens , a Linkset / Lens specification can be viewed as a reproducibility metadata. It provides information on how set of links came to be. In other worlds, it explicitly defines the context in which a link hold and supports decisions making during the validation process (accepting or rejecting a link).","title":"Linkset / Lens Specification"},{"location":"02.Terminology/#matching-algorithm","text":"A set of rules followed by a computer for finding pairs of matching resources. An algorithm can be further categorised as automated or semi-automated depending on whether it requires user assistance or not. Most matching algorithms employed in the Lenticular Lens are semi-automated.","title":"Matching Algorithm"},{"location":"02.Terminology/#fuzzy-logic","text":"From the family of Many-Valued Logics, Fuzzy Logic is a computing approach based on degrees of Truth stemmed form the unit [0, 1] truth space. Modelling reasoning with vague propositions is often addressed with Fuzzy Logic. A proposition, statement or event is said to be vague ( the tomato is ripe ) when it is hard or sometimes impossible to establish whether the proposition is completely true or false due to the involvement of vague concepts ( ripe ), thereby begging for the use of degrees of truth as oppose to binary truth values {0, 1} . \u201cFor example, we cannot exactly say whether a tomato is ripe or not, but rather can only say that the tomato is ripe to some degree\u201d [ Lukasiewicz2008 ]. Degrees of Truth is not to be confused with the Degrees of Certainty / Confidence as the latter is not an assignment of truth value but rather an evaluation of a weighted proposition (proposition with an assigned truth value) regardless of its truth value space ({0, 1} or [0, 1]).","title":"Fuzzy Logic"},{"location":"02.Terminology/#turtle-prefixes","text":"Example 6 : Prefixes used in this manual. Below are the prefixes used in this document @prefix A: <http://example.org/A#> . @prefix B: <http://example.org/B#> . @prefix C: <http://example.org/C#> . @prefix dataset: <http://example.org/dataset#> . @prefix dcterms: <http://purl.org/dc/terms/> . @prefix ex: <http://example.org/#> . @prefix foaf: <http://xmlns.com/foaf/0.1/> . @prefix format: <http://www.w3.org/ns/formats/> . @prefix grid: <http://www.grid.ac/ontology/> . @prefix gridC: <http://vivoweb.org/ontology/core#> . @prefix hero: <http://example.org/hero#> . @prefix institutes: <http://www.grid.ac/institutes/> . @prefix law: <http://www.opendatacommons.org/> @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix person: <http://example.org/person#> . @prefix prov: <http://www.w3.org/ns/prov#> . @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> @prefix rel: <http://www.perceive.net/schemas/relationship/> . @prefix sim: <http://purl.org/ontology/similarity/> . @prefix singleton: <http://lenticularlens.org/singleton#> @prefix void: <http://rdfs.org/ns/void#> . @prefix voidPlus: <http://lenticularlens.org/voidPlus#> . @prefix validation: <http://vocabulary/validation#> . @prefix wiki: <http://en.wikipedia.org/wiki/> @prefix wikidata: <http://www.wikidata.org/entity/> .","title":"Turtle Prefixes"},{"location":"03.Ontology/","text":"ONTOLOGY \u00b6 This section presents an Ontology meant for describing processes of generation and validation of links in detail, so that decisions made such as resource selections and matching options are made explicit. Those processes are implemented in the Lenticular Lens tool and result in the creation of Linksets or Lenses according to a specification. we call the proposed ontology VoID+ as it is proposed as an extension of the [ VoID ] vocabulary. The next section presents the motivation for such extension while Section 3.2 and Section 3.3 respectively present the main elements proposed as extension for VoID and the complete and detailed description of the VoID+ extension. 1. Motivation to extend VoID \u00b6 The model presented in Figure 1 is created based on the [ VoID ] documentation and owl-ontology description, where ellipses represent classes, thick arrows represent subclass or sub-property relations (hierarchy) and thin arrows represent properties. In this model, only classes and relations of interest are exhibit. Fig 1: Excerpt of VoId Ontology regarding void:Dataset and void:Linkset. The [ VoID ] vocabulary provides means to describe datasets and linksets, but with limitations. Too strict linkset definition : The first important limitation that motivates our proposal for extension is that void:Linkset is (1) too restrictive as it is directed and holds between exactly two non-identical datasets. This semantic does not allow a linkset to hold for a dataset deduplication. Data Partitioning : The next limitation is the ambiguous descriptions of subsets or partition of a void:Dataset . This concept is in our theory very important and requires more clarity and expressivity. Direction : (2) The reading direction of the property void:subset is \u201chas subset\u201d but it can easily be misused as it can also be read as \u201cis subset of\u201d as well. Example 1: The Direction of the void:subset property The extract below are from the VoID documentation page . The first annotation on the Aggregate Dataset ex:Aggregate_DS reads as \u201cex:Aggregate_DS has subset ex:DS_A and ex:DS_B\u201d , which is the correct reading for stating that \u201cex:Aggregate_DS is composed of Datases A and B. Now, to state that a linkset is a part of a larger dataset, they again use the same void:subset property. However, the annotation statement reads in the opposite direction as \u201cDBpedia_Geonames is subset of DBpedia\u201d . To our understanding, the latter reading can not be correct. ex: Aggregate_DS a void: Dataset ; dcterms: title \"Aggregate Dataset\" ; dcterms: description \"An aggregate of the A and B datasets.\" ; void: sparqlEndpoint <http://example.org/sparql> ; void: subset ex: DS_A ; void: subset ex: DS_B . ex: DBpedia_Geonames a void: Linkset ; void: target : DBpedia ; void: target : Geonames ; void: subset : DBpedia ; void: triples 252000 . Combination : VoID allows a dataset to originate from a combination of class-partitions and/or property-partitions. However, (3) it falls short in providing means to make clear how several partitions of a void:Dataset ought to be combined. In Example 2, ex:myData is a void:Dataset based on class and property partitions, stating that the dataset is of entities of type ex:AIStudent who had an internship or exchange program. From this, how one would differ between a Partition formed of entities that are [AIStudents and (had an internship or had an exchange program)] from another formed by entities that are [AIStudents or had an internship or had an exchange program] ? Example 2: Interpreting a partition ex: myData a void: Dataset ; void: classPartition [ void: class ex: AIStudent ] ; void: propertyPartition [ void: property ex: hasIntership ] ; void: propertyPartition [ void: property ex: hasExchangeProgram ] . Value-range restriction : Moreover, (4) it also falls short on providing means to restrict entities based on the value-range of a property, or language for that matter. How one would differ between a Partition formed of entities that are [AIStudents and born before the year 2000] from another formed by entities that are [AIStudents or born before the year 2000] ? Reproducibility , (5) it also does not describe the details of how void:Linkset is generated. Others : Furthermore, (6) it does not provide means to describe Lenses or Validations . While a description of the classes and properties exhibited in Fig 1 is made available in the table below, limitations of the VoID vocabulary and possible solutions are further discussed in the next section. Vocabulary Description void:Dataset A set of RDF triples that are published, maintained or aggregated by a single provider. void:Linkset A collection of RDF links between two void:Datasets. void:subset has subset. Domain : void:Dataset Range : void:Dataset void:classPartition A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only entities of an explicitly defined rdfs:Class. Domain : void:Dataset Range : void:Dataset void:propertyPartition A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only triples composed with an explicitly defined rdf:Property. Domain : void:Dataset Range : void:Dataset void:class A relation that assigns to a void:Dataset a rdfs:Class that is the rdf:type of all entities (subject) in a class-based partition. Domain : void:Dataset Range : rdfs:Class ( exactly 1 ) void:property A relation that assigns to a void:Dataset a rdf:Property that is the predicate of all triples in a property-based partition. Domain : void:Dataset Range : rdfs:Property ( exactly 1 ) void:target A relation that assigns one of the two datasets linked by the Linkset. Domain : void:Linkset Range : void:Dataset ( exactly 2 ) void:subjectsTarget A relation that assigns the dataset describing the subjects of triples contained in the Linkset. Domain : void:Linkset Range : void:Dataset ( exactly 1 ) void:objectsTarget A relation that assigns the dataset describing the objects of the triples contained in the Linkset. Domain : void:Linkset Range : void:Dataset ( exactly 1 ) 2. VoID+ Main Elements \u00b6 This section presents the main elements proposed as extension for VoID . It provides a simplified overview (Fig. 2) of VoID+ by (i) only describing the classes and properties that are of importance in this work and (ii) omitting hierarchical relations among classes. While the complete and detailed description of the VoID+ extension is provided in the next section, the complete ontology overview (figure) and the documentation extracted from the owl file are available in section 3.4. Fig 2: The Lenticular Lens Ontology The partial model of the simplified oververview depicted in Fig. 2 highlights in yellow the use of [ VoID ] terms and in blue the new VoID+ terminology . In order to describe the proposed ontology, we dissect Fig 2 into four parts. First, a Resource Selection is elucidated. Second, we go about describing a LinksetFormulation and show how it connects with a Resource Selection . The third step highlights that the description of a Linkset metadata involves specifying the Resource Selections used at the source and target positions of an entity matching process, the Linkset Formulation , eventual validations plus statistics and authority information. The fourth and final step focuses on the annotation of a Lens by describing the combination of one or more Linksets and/or Lenses. 2.1 Resource Selection \u00b6 This step concerns the selection of the resources under scrutiny , that can potentially end up co-referent entities across or within datasources during an entity matching process. To therefore perform a matching, one first needs not only to select datasource(s) but also restrict which resources will undergo the matching. The first way of doing so is by applying a type (class) restriction. This is mandatory in the Lenticular Lens process as matching algorithms are not fully automated. Down this line, further restrictions can be applied by forcing the value of a number of properties to lie within a certain range. A Resource Selection is thereby, the annotation of such process. In the ontology excerpt depicted in Figure 3 we propose the entity type Resource Selection , which is a void+:Partition based on a void:classPartition and/or void:propertyPartition . While a void:classPartition solely consists in specifying the type of entity under scrutiny, the void:propertyPartition entails a little more. It consists in specifying a property or property path and a restriction that the selected property should undergo for the selection of the right entities for the further down the road entity matching process. Those restrictions can be combined using a Formula Description given by void+:hasFormulaDescription . Fig 3: Selecting a matching resource Example 3 : Resource Selection In this example, the entity resource:ResourceSelection-2 is a void+:ResourceSelection (and a void:Dataset ) subset of resource:index_op_doopregister_raw_20190830 and also a collection (partition) of entities of type pnv:PersonName where each entity passed the filter test of (1) name in the English language which appears without trailing dots \"%...%\"@en and (2) birthdates within the interval [1600, 1699] . the entity resource:ResourceSelection-2 lists all three entities of type void+:PropertyConstraint and elaborates on the logic expression that binds all restrictions. For example, the property restriction described by resource:PropertyConstraint-PHce78383e3ff6e9dd73b6 documents that, applying the date function \"minimal_date\"@en over dates in the format the \"YYYY-MM-DD\"@en with the year restriction of 1600 makes sure that only persons born on 1600 onwards are admitted. Turtle Syntax When ever a literal in RDF syntax conatins quote or new line characters, the litreal should be in a three quote syntax ( \"...\"\"@en ). In the example below, we deliberately wrote the literal value of void+:hasFormulaDescription is in a single quote ( \"...\"\"@en ) instead of a triple quote ( \"\"\"%...%=\"\"\"@en ) as the syntax highliter is somewhat buggy. ### RESOURCE 2 resource: ResourceSelection-2 a void: dataset , void+ : ResourceSelection ; rdfs: label \"Baptisms in the 17th Century\" @ en ; void: subset resource: index_op_doopregister_raw_20190830 ; void: classPartition [ void: class pnv: PersonName ] ; void: propertyPartition resource: PropertyConstraint-PHea6802ef02f99a848859 ; void: propertyPartition resource: PropertyConstraint-PHce78383e3ff6e9dd73b6 ; void: propertyPartition resource: PropertyConstraint-PH2580641bbdd572759cb9 ; void+ : hasFormulaDescription \" resource: PropertyConstraint-PHce78383e3ff6e9dd73b6 AND resource: PropertyConstraint-PH2580641bbdd572759cb9 AND ( resource: PropertyConstraint-PHea6802ef02f99a848859 ) \"@en . resource: PropertyConstraint-PHea6802ef02f99a848859 a void+ : PropertyConstraint ; void: property [ a rdfs: Sequence ; rdf : _ 1 pnv: literalName ] ; void+ : hasFilterFunction \"not_ilike\" @ en ; void+ : hasValueFunction \"%...%\" @ en . resource: PropertyConstraint-PHce78383e3ff6e9dd73b6 a void+ : PropertyConstraint ; void: property [ a rdfs: Sequence ; rdf : _ 1 saa: isInRecord ; rdf : _ 2 saa: IndexOpDoopregisters ; rdf : _ 3 saa: birthDate ] ; void+ : hasFilterFunction \"minimal_date\" @ en ; void+ : hasValueFunction 1600 ; void+ : hasFormatFunction \"YYYY-MM-DD\" @ en . resource: PropertyConstraint-PH2580641bbdd572759cb9 a void+ : PropertyConstraint ; void: property [ a rdfs: Sequence ; rdf : _ 1 saa: isInRecord ; rdf : _ 2 saa: IndexOpDoopregisters ; rdf : _ 3 saa: birthDate ] ; void+ : hasFilterFunction \"maximum_date\" @ en ; void+ : hasValueFunction \"1699\" @ en ; void+ : hasFormatFunction \"YYYY-MM-DD\" @ en . 2.2 Linkset Formulation \u00b6 For simple matching problems, finding co-referents can be done using a single matching algorithm (matcher). However, time and again the data reality often imposes the use of more than one matcher instead. In this latter scenario, clearly reporting on how these matchers work together for detecting co-referents is essential. A Linkset Formulation entity is a resource for just doing the aforementioned, as depicted in Figure 4. Once resources of type Resource Restriction are created, one can go ahead and used them for specifying the restricted collections to be used in a particular Matching Method , which also specifies the Matching Algorithm and its arguments such as threshold, range and operator. In the end, all Matching Methods used in a matching process are documented in the Linkset Formulation resource as well as how they bind together in a logic expression given by the predicate void+:hasFormulaDescription . Fig 4: Specifying the way in which methods are logically combined Example 4 : Linkset Logic Expression In Example 2.8, the resource:PHb99da2ecd91ad533af65 is a void+:MatchingFormulation listing eight void+:MatchingMethods used for creating a linkset. They their logic combination is described as void+:hasFormulaDescription . Among the void+:MatchingMethods , the resource:TIME_DELTA-PHfdc744f6bd0ced4e283a is the only method detailes in this example, documenting the four void+:ResourceSelections involved, as well as the chosen void+:MatchingAlgorithm , namely resource:TIME_DELTA , besides the threshold (20), threshold-unit (\u201cYear\u201d@en), threshold-operator (>=) and threshold-range \u201c\u2115\u201d of the matching method. Turtle Syntax When ever a literal in RDF syntax conatins quote or new line characters, the litreal should be in a three quote syntax ( \"...\"\"@en ). In the example below, we deliberately wrote the literal value of void+:hasFormulaDescription is in a single quote ( \"...\"\"@en ) instead of a triple quote ( \"\"\"%...%=\"\"\"@en ) as the syntax highliter is somewhat buggy. ###################################################### # LINKSET LOGIC EXPRESSION # ###################################################### resource: PHb99da2ecd91ad533af65 a void+ : MatchingFormulation ; void+ : hasMethod resource: TIME_DELTA-PHfdc744f6bd0ced4e283a ; void+ : hasMethod resource: Exact-PH6491d1db6855098a70be ; void+ : hasMethod resource: LL_SOUNDEX-PH0ad3ad579d7a29347753 ; void+ : hasMethod resource: BLOOTHOOFT_REDUCT-PH10433274b57dafdd1335 ; void+ : hasMethod resource: Exact-PH4d4187a08c3ba4c1cf0d ; void+ : hasMethod resource: TIME_DELTA-PHe40547b9d3b6381347b4 ; void+ : hasMethod resource: LEVENSHTEIN_APPROX-PH10f4c17bbf933cae647f ; void+ : hasMethod resource: BLOOTHOOFT_REDUCT-PH98a9575087817b951447 ; void+ : hasFormulaDescription \" resource: TIME_DELTA-PHe40547b9d3b6381347b4 AND resource: TIME_DELTA-PHfdc744f6bd0ced4e283a AND ( resource: Exact-PH4d4187a08c3ba4c1cf0d OR ( resource: BLOOTHOOFT_REDUCT-PH98a9575087817b951447 AND resource: BLOOTHOOFT_REDUCT-PH10433274b57dafdd1335 ) OR ( resource: Exact-PH6491d1db6855098a70be AND ( resource: LL_SOUNDEX-PH0ad3ad579d7a29347753 AND resource: LEVENSHTEIN_APPROX-PH10f4c17bbf933cae647f ) ) ) \"@en . ###################################################### # METHOD SIGNATURES # ###################################################### ### METHOD SPECIFICATIONS TIME_DELTA resource: TIME_DELTA-PHe40547b9d3b6381347b4 a void+ : MatchingMethod ; void+ : hasAlgorithm resource: TIME_DELTA ; void+ : hasThresholdRange \"\u2115\" ; ### SOURCE PREDICATE CONFIGURATION void+ : hasSubjResourceSelection resource: ResourceSelection-PHbe38976fdf884b6c4a8e ; void+ : hasSubjResourceSelection resource: ResourceSelection-PHe8fa664d04ad00aaa697 ; ### TARGET PREDICATE CONFIGURATION void+ : hasObjResourceSelection resource: ResourceSelection-PH71818c17d54a8fbec22b ; void+ : hasObjResourceSelection resource: ResourceSelection-PHc8a3c6e494d230b79a6b . \u2022\u2022\u2022 2.3 Linkset \u00b6 This step documents a linkset metadata including WHAT - HOW - WHEN - WHO and other processes explaining the aboutness of links. The Linkset Formulation specifies HOW entities are matched and Resource Selection specifies WHAT to match as subject and object targets. Also some statistic on the matching results can be reported such as the number of links found, the numbers of entities linked, WHO created the linkset and WHEN . Finally, a Validation entity can also be specified, comprising metadata with statitics and auhtority information on the validation process. Observe that when one or more validations are provided, statistics on this matter can be included in the linkset metadata, including eventual contradictions if one validation says a link is correct while another says it is not. As discussed earlier in this section, according to the [ VoID ] documentation, the void:Linkset definition expects as datasources exactly one source and one target, different from each other. This means it is more restrictive than the void+:Linkset here proposed, since the latter also expects a linkset to contain links within a datasource or across more than two. Therefore, we do not directly reuse that concept (and its correspoding properties void:subjects/objectsTarget). Naturally, one could still use void:Linkset for other purposes, but at the risk of abusing the VoID vocabulary if its instances do not really fit the required restrictions. Moreover, a void:Linkset (i.e. a resource representing a linkset\u2019s metadata) is also not an instance of void+:Linkset since the later requires the description of the processes underlying the creation of the links, which is not the case for the first. Fig 5: Specifying the linkset\u2019s context Example 5: A linkset annotation ### LINKSET 15 linkset : 15 a void+ : Linkset ; void: feature format: Turtle ; cc: attributionName \"LenticularLens\" @ en ; cc: license <http://purl.org/NET/rdflicense/W3C1.0> ; dcterms: created \"2020-09-26T09:37:23.933624\" ^^ <http://www.w3.org/2001/XMLSchema#dateTime> ; dcterms: creator \"AL IDRISSOU\" ; dcterms: creator \"GoldenAgents\" ; void: linkPredicate owl: sameAs ; rdfs: label \"Linkset 9\" @ en ; dcterms: description \"LINSET-15-9: Test Baptism against Marriage and burial with several nested methods \" @ en ; ### VOID LINKSET STATS void: entities 12580 ; ### LENTICULAR LENS LINKSET STATS ### SOURCE ENTITY TYPE SELECTION(S) void+ : subjectsTarget resource: ResourceSelection-2 ; ### TARGET ENTITY TYPE SELECTION(S) void+ : objectsTarget resource: ResourceSelection-1 ; void+ : objectsTarget resource: ResourceSelection-3 ; ### THE LOGIC FORMULA void+ : hasLogicFormulation resource: PHb6e5e320dc08d7d9dd98 . 2.4 Lens \u00b6 Another process relevant to document is the creation of Lenses. In short, a lens is the result of a set-like operation over one or more Linkset and or Lens. Therefore, the entity Lens documents them as void+:hasTarget Fig 6: Specifying the linkset\u2019s context Example 6: A lens annotation 3. VoID+ Ontology \u00b6 VoID provides interesting means to annotate datasets in the broad sense \u2013 such as subset, class and property partitions, and many more. However, it does not offer a variety of dataset-types to deal with. To alleviate this limitation, VoID+ proposes a range of dataset subtypes of importance for annotation clarity , along with their relations enriched with (more precise) cardinality, domain and range restrictions that supports communicating the intended meaning of those vocabularies. Hopefully, this helps avoiding miss-usage and allows for reasoning so that possible miss-usage that lead to inconsistencies can be detected. To this end, VoID+ offers: Partition : for enabling better clarity on how a dataset is derived from another dataset based on one or more type of partitions. LinkDataset : for discriminating among sets of links depending on the (i) number of datasets involved, (ii) importance of link direction and (iii) how a link or set of links came to be. Validation : a separation of concern for clearly allowing multiple validations of the same link or set of links. IntegrationView : For expressing the result of one\u2019s view on data integration. The remainder of this section discuses the complete and detailed description of the VoID+ extension through these aforementioned discriminating types. It first addresses the void+:Partition of void:Dataset and then tackles the void+:LinkDataset which is considered a special sub-type of void:Dataset. Finally Validation and IntegrationView are discussed in the last two subsections. The documentation extracted from the owl file is presented in the next section. 3.1 Partitions of void:Dataset \u00b6 The VoID vocabulary defines void:Dataset as a dataset superset but does make available a granularity of subsets for refined work. VoID+ extends the void:Dataset class by distinguishes between five subsets of void:Dataset . A void:Dataset that is defined as a subset of another one is called void+:Partition . Three types of void+:Partition are distinguished according to the type of partition that is used: (i) void+:ClassPartition : the dataset is the result of an rdfs:Class based-partition; (ii) void+:PropertyPartition : the dataset is the result of a rdf:Property or by a rdf:Seq \u2013 defining a sequence of two or more of properties \u2013 partition; (iii) void+:Language : the dataset is the result of a language partition given by a standardised ISO language code. Another special type of partition is the void+:ResourceSelection . It is defined by one or more partions of the type above described. Fig. 7 provides a straightforward understanding of how the class void:Dataset is extended. On its the left hand side, Fig. 7 presents a view on the classes, their hierarchy and how they are connected via object properties. On the one hand, the property void:subset and its sub-properties are extended such that they provide a directional non-ambiguous reading. On the other hand, void:class and void:property are grouped into a superclass void+:partionedBy which also includes void:language to allow for one more type of partition. In particular, void:property is extended with a more generic relation void+:property that allows for describing a sequence of properties like in a property path. Fig 7: VoID+ Extension on the void:Dataset 3.2 VoID+ Link-datasets \u00b6 The void+:LinkDataset concept is a specific type of void:Dataset allowing for distinguishing among sets of links depending on the (i) number of datasets involved, (ii) importance of link direction and (iii) how a link or set of links came to be. A quick take on this is to say that a void+:LinkDataset is a Dataset that contains links that may have been established among one (deduplication), two or many datasets. Figure 8 illustrates on the left hand side how the class void:Linkset is extended upward \u2013 with superclasses less restrictive with respect to its targets \u2013 by showing a hierarchical view of the classes as well as how they connect via the available object properties. When the creation of a void+:LinkDataset is such that it imposes that subjects always belong to the same datasets as well as objects, this is called void+:DirectedLinkDataset . This is the case for both void:Linkset and for our definition of void+:Linkset . The former, however, requires exactly one dataset as target for its subjects and another as target for its objects. In addition, our definition of void+:Linkset . is also a void+:DocumentedLinkDataset , which requires more detailed documentation, including targets described as void+:ResourceSelection (all values from) and a void+:formulation . A void+:Lens is also a void+:DocumentedLinkDataset which has one void:LensFormulation that specifies which void:LinkDatasets are combined and how. The right hand side of Figure 8 presents a hierarchy of properties in order to provide an understanding of how void:target and its sub-properties are extended. They actually are extended with superclasses having as range not only a void:Linkset but its superclass void+:LinkDataset . Fig 8: VoID+ Extension on the void:Linkset 3.3 Validation \u00b6 coming soon 3.4 IntegrationView \u00b6 3.5 VoID+ OWL \u00b6 Here, we provide a turtle description of the ontology. VoID+ @prefix : <http://lenticularlens.org/voidPlus/> . @prefix cc: <https://creativecommons.org/ns#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix xml: <http://www.w3.org/XML/1998/namespace> . @prefix xsd: <http://www.w3.org/2001/XMLSchema#> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix void: <http://rdfs.org/ns/void#> . @prefix void+ : <http://lenticularlens.org/voidPlus/> . @prefix dcterms: <http://purl.org/dc/terms/> . @base <http://lenticularlens.org/voidPlus> . <http://lenticularlens.org/voidPlus> rdf: type owl: Ontology . ################################################################# # Object Properties ################################################################# ### http://lenticularlens.org/voidPlus/hasAlgorithm void+ : hasAlgorithm rdf: type owl: ObjectProperty , owl: FunctionalProperty ; rdfs: domain void+ : MatchingMethod ; rdfs: range void+ : MatchingAlgorithm ; rdfs: comment \"relates a void+:MatchingMethod to its void+:MatchingAlgorithm.\" @ en . ### http://lenticularlens.org/voidPlus/hasClassPartition void+ : hasClassPartition rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasSubset , void: classPartition ; rdfs: comment \"relates a void:Dataset to its subset void+:ClassPartition. In practice, the main difference wrt void:classPartition is the name meant to avoid miss-usage as it can be read as \\\"is classPartition of\\\" as well.\" @ en . ### http://lenticularlens.org/voidPlus/hasFormulation void+ : hasFormulation rdf: type owl: ObjectProperty ; rdfs: domain void: Dataset ; rdfs: range void+ : Formulation ; rdfs: comment \"relates a void+:Dataset to its void+:Formulation.\" @ en . ### http://lenticularlens.org/voidPlus/hasItem void+ : hasItem rdf: type owl: ObjectProperty ; rdfs: domain void+ : Formulation ; rdfs: comment \"relates a void+:Formulation to its formula-items.\" @ en . ### http://lenticularlens.org/voidPlus/hasLanguagePartition void+ : hasLanguagePartition rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasSubset ; rdfs: comment \"relates a void:Dataset to its subset void+:LanguagePartition.\" @ en . ### http://lenticularlens.org/voidPlus/hasObjectResourceSelection void+ : hasObjectResourceSelection rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasResourceSelection ; rdfs: comment \"a relation to assign a void+:ResourceSelection as the source for the objects of triples under scrutiny.\" @ en . ### http://lenticularlens.org/voidPlus/hasPropertyPartition void+ : hasPropertyPartition rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasSubset , void: propertyPartition ; rdfs: comment \"relates a void:Dataset to its subset void+:PropertyPartition. In practice, the main difference wrt void:propertyPartition is the name meant to avoid miss-usage as it can be read as \\\"is propertyPartition of\\\" as well.\" @ en . ### http://lenticularlens.org/voidPlus/hasResourceSelection void+ : hasResourceSelection rdf: type owl: ObjectProperty ; rdfs: range void+ : ResourceSelection ; rdfs: comment \"a relation to assign a void+:ResourceSelection as the source for entities under scrutiny.For example, the selection of a void+:ResourceSelection for a void+:MatchingMethod.\" @ en . ### http://lenticularlens.org/voidPlus/hasSubjectResourceSelection void+ : hasSubjectResourceSelection rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasResourceSelection ; rdfs: comment \"a relation to assign a void+:ResourceSelection as the source for the subjects of triples under scrutiny.\" @ en . ### http://lenticularlens.org/voidPlus/hasSubset void+ : hasSubset rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void: subset ; owl: inverseOf void+ : subsetOf ; rdfs: domain void: Dataset ; rdfs: range void+ : Partition ; rdfs: comment \"relates a void:Dataset that has as subset a void+:Partition (dataset). In practice, the main difference wrt void:subset is the name meant to avoid miss-usage as it can be read as \\\"is subset of\\\" as well. For a similar purpose, an inverse relation void+:subsetOf is also defined.\" @ en . ### http://lenticularlens.org/voidPlus/hasTarget void+ : hasTarget rdf: type owl: ObjectProperty ; rdfs: domain void+ : LinkDataset ; rdfs: range void: Dataset ; rdfs: comment \"A relation that assigns one or more void:Datasets linked via a void+:LinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/hasValidation void+ : hasValidation rdf: type owl: ObjectProperty ; rdfs: domain void+ : LinkDataset ; rdfs: range void+ : Validation ; rdfs: comment \"A relation that assigns one or more void+:Validations to a void+:LinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/language void+ : language rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : partitionedBy ; rdfs: range void+ : Language ; rdfs: comment \"A relation that assigns to a void:Dataset a void+:Language (for ISO standard language codes) that is the rdf:language of all literals (objects) qualifying entities (subject) in a language-based partition.\" @ en . ### http://lenticularlens.org/voidPlus/linkPredicate void+ : linkPredicate rdf: type owl: ObjectProperty ; rdfs: subPropertyOf owl: topObjectProperty ; rdfs: comment \"a link predicate that holds for a void+:LinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/objectsTarget void+ : objectsTarget rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasTarget ; rdfs: domain void+ : DirectedLinkDataset ; rdfs: range void: Dataset ; rdfs: comment \"A relation that assigns void:Datasets describing the object of triples contained in the void+:DirectedLinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/partitionedBy void+ : partitionedBy rdf: type owl: ObjectProperty ; rdfs: domain void: Dataset ; rdfs: comment \"A relation that assigns a void:Dataset to the conditions under which it is partitioned.\" @ en . ### http://lenticularlens.org/voidPlus/property void+ : property rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : partitionedBy ; rdfs: range [ rdf: type owl: Class ; owl: unionOf ( rdf: Property rdf: Seq ) ] ; rdfs: comment \"A relation that assigns to a void:Dataset a rdf:Property or a rdf:Seq of properties that expresses the pattern of all triples in a property-based partition.\" @ en . ### http://lenticularlens.org/voidPlus/subjectsTarget void+ : subjectsTarget rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasTarget ; rdfs: domain void+ : DirectedLinkDataset ; rdfs: range void: Dataset ; rdfs: comment \"A relation that assigns void:Datasets describing the subjects of triples contained in the void+:DirectedLinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/subsetOf void+ : subsetOf rdf: type owl: ObjectProperty ; rdfs: comment \"relates a void+:Partition to its void:Dataset superset. Defined as the inverse relation of void+:hasSubset.\" @ en . ### http://purl.org/dc/terms/creator dcterms: creator rdf: type owl: ObjectProperty ; rdfs: comment \"The range for dcterms:creator is dcterms:Agent. All values used with this property have to be instances of the class [dcterms:Agent] . <span style=\\\"color: red\\\">must not be used with literal values</span>. You may use it only with non-literal values.\" @ en . ### http://purl.org/dc/terms/publisher dcterms: publisher rdf: type owl: ObjectProperty ; rdfs: range dcterms: Agent ; rdfs: comment \"The range for dcterms:publisher is dcterms:Agent. All values used with this property have to be instances of the class [dcterms:Agent] . <span style=\\\"color: red\\\">must not be used with literal values</span>. You may use it only with non-literal values.\" @ en . ### http://rdfs.org/ns/void#linkPredicate void: linkPredicate rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : linkPredicate ; rdfs: domain void: Linkset ; rdfs: range rdf: Property ; rdfs: comment \"a link predicate that holds for a void:Linkset.\" @ en . ### http://rdfs.org/ns/void#class void: class rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : partitionedBy ; rdfs: range rdfs: Class ; rdfs: comment \"A relation that assigns to a void:Dataset a rdfs:Class that is the rdf:type of all entities (subject) in a class-based partition.\" @ en . ### http://rdfs.org/ns/void#classPartition void: classPartition rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void: subset ; rdfs: comment \"A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only entities of an explicitly defined rdfs:Class.\" @ en . ### http://rdfs.org/ns/void#feature void: feature rdf: type owl: ObjectProperty ; rdfs: domain void: Dataset ; rdfs: range void: TechnicalFeature ; rdfs: comment \"relates a void:TechnicalFeature supported by a void:Datset.\" @ en . ### http://rdfs.org/ns/void#objectsTarget void: objectsTarget rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : objectsTarget , void: target ; rdfs: domain void: Linkset ; rdfs: comment \"A relation that assigns the void:Dataset describing the objects of triples contained in the void:Linkset.\" @ en . ### http://rdfs.org/ns/void#property void: property rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : property ; rdfs: domain void: Dataset ; rdfs: range rdf: Property ; rdfs: comment \"A relation that assigns to a void:Dataset a rdf:Property that is the predicate of all triples in a property-based partition.\" @ en . ### http://rdfs.org/ns/void#propertyPartition void: propertyPartition rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void: subset ; rdfs: comment \"A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only triples composed with an explicitly defined rdf:Property.\" @ en . ### http://rdfs.org/ns/void#subjectsTarget void: subjectsTarget rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : subjectsTarget , void: target ; rdfs: domain void: Linkset ; rdfs: comment \"A relation that assigns the void:Dataset describing the subjects of triples contained in the void:Linkset.\" @ en . ### http://rdfs.org/ns/void#subset void: subset rdf: type owl: ObjectProperty ; rdfs: domain void: Dataset ; rdfs: range void: Dataset ; rdfs: comment \"relates a void:Dataset that has subset another void:Dataset.\" @ en . ### http://rdfs.org/ns/void#target void: target rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasTarget ; rdfs: domain void: Linkset ; rdfs: range void: Dataset ; rdfs: comment \"A relation that assigns one of the two void:Datasets linked by the void:Linkset.\" @ en . ### http://www.w3.org/2000/01/rdf-schema#member rdfs: member rdf: type owl: ObjectProperty ; rdfs: comment \"rdfs:member is an instance of rdf:Property that is a super-property of all the container membership properties. In particular, properties called rdf:_1, rdf:_2, rdf:_3... etc., used for rdf:Seq are sub-properties of rdfs:member.\" @ en . ################################################################# # Data properties ################################################################# ### http://creativecommons.org/ns#attributionName <http://creativecommons.org/ns#attributionName> rdf: type owl: DatatypeProperty ; rdfs: comment \"The name the creator of a Work would like used when attributing re-use.\" @ en . ### http://creativecommons.org/ns#license <http://creativecommons.org/ns#license> rdf: type owl: DatatypeProperty ; rdfs: comment \"A work has license a License.\" @ en . ### http://lenticularlens.org/voidPlus/hasAccepted void+ : hasAccepted rdf: type owl: DatatypeProperty ; rdfs: domain [ rdf: type owl: Class ; owl: unionOf ( void+ : LinkDataset void+ : Validation ) ] ; rdfs: range xsd: integer ; rdfs: comment \"The amount of accepted links for a void+:LinkDataset or void+:Validation.\" @ en . ### http://lenticularlens.org/voidPlus/hasClusters void+ : hasClusters rdf: type owl: DatatypeProperty ; rdfs: domain void+ : LinkDataset ; rdfs: range xsd: integer ; rdfs: comment \"The amount of clusters computed for a void+:LinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/hasContradictions void+ : hasContradictions rdf: type owl: DatatypeProperty ; rdfs: domain void+ : LinkDataset ; rdfs: range xsd: integer ; rdfs: comment \"The amount of links of a void+:LinkDataset for which contradicting validations are found.\" @ en . ### http://lenticularlens.org/voidPlus/hasFilterFunction void+ : hasFilterFunction rdf: type owl: DatatypeProperty ; rdfs: domain void+ : PropertyPartition ; rdfs: comment \"It relates a property partition to a filter function, such as \\\"contains\\\" or \\\"minimal_date\\\". When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \\\"function\\\", a \\\"value\\\" and a \\\"format\\\".\" @ en . ### http://lenticularlens.org/voidPlus/hasFormatFunction void+ : hasFormatFunction rdf: type owl: DatatypeProperty ; rdfs: domain void+ : PropertyPartition ; rdfs: comment \"It relates a property partition to a format restriction which is applicable to the value used by an explicitly defined filter function, such as \\\"YYYY-MM-DD\\\" for filter \\\"minimal_date\\\". When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \\\"function\\\", a \\\"value\\\" and a \\\"format\\\".\" @ en . ### http://lenticularlens.org/voidPlus/hasFormulaDescription void+ : hasFormulaDescription rdf: type owl: DatatypeProperty ; rdfs: domain void+ : Formulation ; rdfs: range xsd: string ; rdfs: comment \"Describes how items listed in a formulation are combined. For example, using logic or set-like operators.\" @ en . ### http://lenticularlens.org/voidPlus/hasRejected void+ : hasRejected rdf: type owl: DatatypeProperty ; rdfs: domain [ rdf: type owl: Class ; owl: unionOf ( void+ : LinkDataset void+ : Validation ) ] ; rdfs: range xsd: integer ; rdfs: comment \"The amount of rejected links for a void+:LinkDataset or void+:Validation.\" @ en . ### http://lenticularlens.org/voidPlus/hasThreshold void+ : hasThreshold rdf: type owl: DatatypeProperty ; rdfs: domain void+ : MatchingMethod ; rdfs: comment \"Relates a void+:MatchingMethod to a link-acceptance-threshold value. Upon the choice of a void+:MatchingAlgorithm, the user should provide the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \\\"Year\\\"), a threshold operator (e.g. \\\">=\\\") and a threshold range (e.g. \\\"]0,1]\\\" or \\\"\u2115\\\").\" @ en . ### http://lenticularlens.org/voidPlus/hasThresholdAcceptanceOperator void+ : hasThresholdAcceptanceOperator rdf: type owl: DatatypeProperty ; rdfs: domain void+ : MatchingMethod ; rdfs: comment \"Relates a void+:MatchingMethod to a link-acceptance-threshold operator. Upon the choice of a void+:MatchingAlgorithm, the user should inform the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \\\"Year\\\"), a threshold operator (e.g. \\\">=\\\") and a threshold range (e.g. \\\"]0,1]\\\" or \\\"\u2115\\\").\" @ en . ### http://lenticularlens.org/voidPlus/hasThresholdRange void+ : hasThresholdRange rdf: type owl: DatatypeProperty ; rdfs: domain void+ : MatchingMethod ; rdfs: comment \"Relates a void+:MatchingMethod to a link-acceptance-threshold range. Upon the choice of a void+:MatchingAlgorithm, the user should inform the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \\\"Year\\\"), a threshold operator (e.g. \\\">=\\\") and a threshold range (e.g. \\\"]0,1]\\\" or \\\"\u2115\\\").\" @ en . ### http://lenticularlens.org/voidPlus/hasValidations void+ : hasValidations rdf: type owl: DatatypeProperty ; rdfs: domain [ rdf: type owl: Class ; owl: unionOf ( void+ : LinkDataset void+ : Validation ) ] ; rdfs: range xsd: integer ; rdfs: comment \"The amount of validated links in a void+:LinkDataset or void+:Validation.\" @ en . ### http://lenticularlens.org/voidPlus/hasValueFunction void+ : hasValueFunction rdf: type owl: DatatypeProperty ; rdfs: domain void+ : PropertyPartition ; rdfs: comment \"It relates a property partition to a value for a filter function, such as \\\"%van%\\\" for filter \\\"contains\\\" or \\\"01/01/1600\\\" for filter \\\"minimal_date\\\". When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \\\"function\\\", a \\\"value\\\" and a \\\"format\\\".\" @ en . ### http://lenticularlens.org/voidPlus/toBeValidated void+ : toBeValidated rdf: type owl: DatatypeProperty ; rdfs: domain [ rdf: type owl: Class ; owl: unionOf ( void+ : LinkDataset void+ : Validation ) ] ; rdfs: range xsd: integer ; rdfs: comment \"The amount of links yet to be validated in a void+:LinkDataset or void+:Validation.\" @ en . ### http://purl.org/dc/terms/created dcterms: created rdf: type owl: DatatypeProperty ; rdfs: range rdfs: Literal ; rdfs: comment \"The range defined for dcterms:created is the class of rdfs:Literal. Values used with this property therefore have to be literal values.\" @ en . ### http://purl.org/dc/terms/description dcterms: description rdf: type owl: DatatypeProperty ; rdfs: comment \"There is no range defined for dcterms:description. Therefore you can use it with literal values.\" @ en . ### http://rdfs.org/ns/void#distinctObjects void: distinctObjects rdf: type owl: DatatypeProperty ; rdfs: domain void: Dataset ; rdfs: range xsd: integer ; rdfs: comment \"The total number of distinct objects in a void:Dataset. In other words, the number of distinct resources that occur in the object position of triples in the dataset. Literals are included in this count.\" . ### http://rdfs.org/ns/void#distinctSubjects void: distinctSubjects rdf: type owl: DatatypeProperty ; rdfs: domain void: Dataset ; rdfs: range xsd: integer ; rdfs: comment \"The total number of distinct subjects in a void:Dataset. In other words, the number of distinct resources that occur in the subject position of triples in the dataset.\" . ### http://rdfs.org/ns/void#entities void: entities rdf: type owl: DatatypeProperty ; rdfs: domain void: Dataset ; rdfs: range xsd: integer ; rdfs: comment \"The total number of entities that are described in a void:Dataset.\" . ### http://rdfs.org/ns/void#triples void: triples rdf: type owl: DatatypeProperty ; rdfs: domain void: Dataset ; rdfs: range xsd: integer ; rdfs: comment \"The total number of triples contained in a void:Dataset.\" @ en . ### http://www.w3.org/2000/01/rdf-schema#label rdfs: label rdf: type owl: DatatypeProperty . ################################################################# # Classes ################################################################# ### http://lenticularlens.org/voidPlus/ClassPartition void+ : ClassPartition rdf: type owl: Class ; owl: equivalentClass [ rdf: type owl: Restriction ; owl: onProperty [ owl: inverseOf void+ : hasClassPartition ] ; owl: someValuesFrom void: Dataset ] , [ rdf: type owl: Restriction ; owl: onProperty void: class ; owl: qualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onClass rdfs: Class ] ; rdfs: subClassOf void+ : Partition ; rdfs: comment \"A void+:Partition that is a collection of resources partitioned on the basis of an rdfs:Class.\" @ en . ### http://lenticularlens.org/voidPlus/DirectedLinkDataset void+ : DirectedLinkDataset rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( [ rdf: type owl: Restriction ; owl: onProperty void+ : objectsTarget ; owl: someValuesFrom void: Dataset ] [ rdf: type owl: Restriction ; owl: onProperty void+ : subjectsTarget ; owl: someValuesFrom void: Dataset ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void+ : LinkDataset , [ rdf: type owl: Restriction ; owl: onProperty void: distinctObjects ; owl: qualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onDataRange xsd: integer ] , [ rdf: type owl: Restriction ; owl: onProperty void: distinctSubjects ; owl: qualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onDataRange xsd: integer ] ; rdfs: comment \"A void+:LinkDataset that imposes that subjects always belong to the same datasets as well as objects.\" @ en . ### http://lenticularlens.org/voidPlus/DocumentedLinkDataset void+ : DocumentedLinkDataset rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( void+ : LinkDataset [ rdf: type owl: Restriction ; owl: onProperty void+ : hasFormulation ; owl: someValuesFrom void+ : Formulation ] [ rdf: type owl: Restriction ; owl: onProperty void+ : hasTarget ; owl: someValuesFrom void+ : ResourceSelection ] [ rdf: type owl: Restriction ; owl: onProperty void+ : hasTarget ; owl: allValuesFrom void+ : ResourceSelection ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void+ : LinkDataset ; rdfs: comment \"A void+:LinkDataset that requires detailed documentation, namely it imposes that targets are described as void+:ResourceSelection and requires a void+:Formulation.\" @ en . ### http://lenticularlens.org/voidPlus/Formulation void+ : Formulation rdf: type owl: Class ; rdfs: comment \"A (reusable) formulation that lists a number of items (e.g. matching methods or linksets) and describes how they are meant to be combined (e.g. using logic or set operators).\" @ en . ### http://lenticularlens.org/voidPlus/IntegrationView void+ : IntegrationView rdf: type owl: Class ; rdfs: subClassOf void: Dataset , [ owl: intersectionOf ( [ rdf: type owl: Restriction ; owl: onProperty void+ : hasSubset ; owl: minQualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onClass void+ : LinkDataset ] [ rdf: type owl: Restriction ; owl: onProperty void+ : hasSubset ; owl: minQualifiedCardinality \"2\" ^^ xsd:nonNegativeInteger ; owl: onClass void+ : ResourceSelection ] ) ; rdf: type owl: Class ] ; rdfs: comment \"A set of RDF triples that are stemmed from two or more void+:ResourceSelections that are linked by at least one void+:LinkDataset. One may add void+:PropertyPartitions for both filtering or for data visualisation and analysis.\" @ en . ### http://lenticularlens.org/voidPlus/Language void+ : Language rdf: type owl: Class ; rdfs: comment \"A set of standardised language (ISO 639 not 3166) codes. For example, use nl, nld or dut to indicate the Dutch or Flemish languages.\" @ en . ### http://lenticularlens.org/voidPlus/LanguagePartition void+ : LanguagePartition rdf: type owl: Class ; owl: equivalentClass [ rdf: type owl: Restriction ; owl: onProperty void+ : language ; owl: someValuesFrom void+ : Language ] , [ rdf: type owl: Restriction ; owl: onProperty [ owl: inverseOf void+ : hasLanguagePartition ] ; owl: someValuesFrom void: Dataset ] ; rdfs: subClassOf void+ : Partition ; rdfs: comment \"A void+:Partition that is a collection of resources based on a void+:Language restriction, i.e. an ISO 639 standardised language code.\" @ en . ### http://lenticularlens.org/voidPlus/Lens void+ : Lens rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( void+ : DocumentedLinkDataset [ rdf: type owl: Restriction ; owl: onProperty void+ : hasFormulation ; owl: someValuesFrom void+ : LensFormulation ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void+ : DocumentedLinkDataset ; rdfs: comment \"Similar to a void+:Linkset (in the sense that it is a collection of links sharing the same linktype) also involving one, two or more datasets, but not necessarilly directed. Moreover, context-wise, it differs from a linkset as it is generated using different processes (set-like link manipulation operators such as Union, Intersection, Difference or Transitivity) as compared to how a linkset comes about (matching algorithms).\" @ en . ### http://lenticularlens.org/voidPlus/LensFormulation void+ : LensFormulation rdf: type owl: Class ; rdfs: subClassOf void+ : Formulation , [ rdf: type owl: Restriction ; owl: onProperty void+ : hasItem ; owl: minQualifiedCardinality \"2\" ^^ xsd:nonNegativeInteger ; owl: onClass void+ : LinkDataset ] ; rdfs: comment \"A resource that makes explicit all void+:LinkDatasets involved in the creation of a void+:Lens and how the they are combined.\" @ en . ### http://lenticularlens.org/voidPlus/LinkDataset void+ : LinkDataset rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( [ rdf: type owl: Restriction ; owl: onProperty void+ : hasTarget ; owl: someValuesFrom void: Dataset ] [ rdf: type owl: Restriction ; owl: onProperty void+ : linkPredicate ; owl: someValuesFrom rdf: Property ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void: Dataset , [ rdf: type owl: Restriction ; owl: onProperty dcterms: description ; owl: someValuesFrom rdfs: Literal ] ; rdfs: comment \"A collection of RDF links between dataset(s), using the same link predicate (regardless of the link being an equality predicate or not). This predicate is called the linktype of the linkset. The links may have been stabilished among one (deduplication), two or many datasets.\" @ en . ### http://lenticularlens.org/voidPlus/Linkset void+ : Linkset rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( void+ : DirectedLinkDataset [ rdf: type owl: Restriction ; owl: onProperty void+ : hasFormulation ; owl: someValuesFrom void+ : LinksetFormulation ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void+ : DirectedLinkDataset , void+ : DocumentedLinkDataset ; rdfs: comment \"A collection of RDF links between <em>one</em>, <em>two</em> or <em>more</em> void:Datasets. Only, all links share the same link-predicate and all subjects stem from a set of void datasets (D<sub>1</sub>) while all objects stem from either the same D<sub>1</sub> or a different set of void datasets D<sub>2</sub>.\" @ en . ### http://lenticularlens.org/voidPlus/LinksetFormulation void+ : LinksetFormulation rdf: type owl: Class ; rdfs: subClassOf void+ : Formulation , [ rdf: type owl: Restriction ; owl: onProperty void+ : hasItem ; owl: someValuesFrom void+ : MatchingMethod ] ; rdfs: comment \"A resource that makes explicit all void+:MatchingMethods involved in the creation of a void+:Linkset and how the methods are logically joint.\" @ en . ### http://lenticularlens.org/voidPlus/MatchingAlgorithm void+ : MatchingAlgorithm rdf: type owl: Class ; rdfs: comment \"A set of rules followed by a computer for finding pairs of matching resources. An algorithm can be further categorised as automated or semi-automated depending on whether it requires user assistance or not. Most matching algorithms employed in the Lenticular Lens are semi-automated.\" @ en . ### http://lenticularlens.org/voidPlus/MatchingMethod void+ : MatchingMethod rdf: type owl: Class ; owl: equivalentClass [ rdf: type owl: Restriction ; owl: onProperty void+ : hasAlgorithm ; owl: someValuesFrom void+ : MatchingAlgorithm ] ; rdfs: comment \"A resource that makes explicit all parameters/pre-requisites (datasets, entity-type and property-value restrictions, matching properties...) of a matching algorithm including the conditions in which the algorithm is to accept a discovered link (threshold).\" @ en . ### http://lenticularlens.org/voidPlus/Partition void+ : Partition rdf: type owl: Class ; owl: equivalentClass [ rdf: type owl: Restriction ; owl: onProperty void: subset ; owl: someValuesFrom void: Dataset ] ; rdfs: subClassOf void: Dataset ; rdfs: comment \"A void:Daset that is subset of another void:Daset.\" @ en . ### http://lenticularlens.org/voidPlus/PartitionFormulation void+ : PartitionFormulation rdf: type owl: Class ; rdfs: subClassOf void+ : Formulation , [ rdf: type owl: Restriction ; owl: onProperty void+ : hasItem ; owl: someValuesFrom void+ : Partition ] . ### http://lenticularlens.org/voidPlus/PropertyPartition void+ : PropertyPartition rdf: type owl: Class ; owl: equivalentClass [ rdf: type owl: Restriction ; owl: onProperty [ owl: inverseOf void+ : hasPropertyPartition ] ; owl: someValuesFrom void: Dataset ] , [ rdf: type owl: Restriction ; owl: onProperty void+ : property ; owl: qualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onClass [ rdf: type owl: Class ; owl: unionOf ( rdf: Property rdf: Seq ) ] ] ; rdfs: subClassOf void+ : Partition ; rdfs: comment \"A void+:Partition that is is a collection of resources described using either a rdf:Property or sequence of them (void+:PropertySequence or void+:TypedPropertySequence).\" @ en . ### http://lenticularlens.org/voidPlus/PropertySequence void+ : PropertySequence rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( [ rdf: type owl: Restriction ; owl: onProperty rdfs: member ; owl: allValuesFrom rdf: Property ] [ rdf: type owl: Restriction ; owl: onProperty rdfs: member ; owl: minQualifiedCardinality \"2\" ^^ xsd:nonNegativeInteger ; owl: onClass rdf: Property ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf rdf: Seq ; rdfs: comment \"A resource that represents a sequence of properties, analogously to a property path.\" @ en . ### http://lenticularlens.org/voidPlus/ResourceSelection void+ : ResourceSelection rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( void+ : Partition [ rdf: type owl: Restriction ; owl: onProperty void+ : hasFormulation ; owl: someValuesFrom void+ : PartitionFormulation ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void: Dataset , [ rdf: type owl: Restriction ; owl: onProperty void+ : hasSubset ; owl: someValuesFrom void+ : Partition ] , [ rdf: type owl: Restriction ; owl: onProperty void: subset ; owl: someValuesFrom void: Dataset ] , [ rdf: type owl: Restriction ; owl: onProperty dcterms: description ; owl: someValuesFrom rdfs: Literal ] ; rdfs: comment \"A collection of resources stemmed from the same void:Dataset. It is expected to have a void+:ClassPartition as partition, directly or indirectly (i.e. as a subset). It can also have void+:PropertyPartitions, where property-value(s) can be restricted, as well as void+:LanguagePartitions, where the language of the property-value(s) can be restricted.\" @ en . ### http://lenticularlens.org/voidPlus/TypedPropertySequence void+ : TypedPropertySequence rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( [ rdf: type owl: Restriction ; owl: onProperty rdfs: member ; owl: allValuesFrom [ rdf: type owl: Class ; owl: unionOf ( rdf: Property rdfs: Class ) ] ] [ rdf: type owl: Restriction ; owl: onProperty rdfs: member ; owl: minQualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onClass rdfs: Class ] [ rdf: type owl: Restriction ; owl: onProperty rdfs: member ; owl: minQualifiedCardinality \"2\" ^^ xsd:nonNegativeInteger ; owl: onClass rdf: Property ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf rdf: Seq ; rdfs: comment \"\"\"A resource that represents a sequence of properties and classes, as in a different type of property path with class restriction in between: property -> class -> property -> class -> property\"\"\"@en . ### http://lenticularlens.org/voidPlus/ValidatedLinkDataset void+:ValidatedLinkDataset rdf:type owl:Class ; owl:equivalentClass [ owl:intersectionOf ( void+:LinkDataset [ rdf:type owl:Restriction ; owl:onProperty void+:hasValidation ; owl:someValuesFrom void+:Validation ] ) ; rdf:type owl:Class ] ; rdfs:subClassOf void+:LinkDataset , [ rdf:type owl:Restriction ; owl:onProperty void+:hasContradictions ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] , [ rdf:type owl:Restriction ; owl:onProperty void+:hasValidations ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] , [ rdf:type owl:Restriction ; owl:onProperty void+:toBeValidated ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] ; rdfs:comment \"A voidPlus:LinkDataset that has been validated.\"@en . ### http://lenticularlens.org/voidPlus/Validation void+:Validation rdf:type owl:Class ; owl:equivalentClass [ rdf:type owl:Restriction ; owl:onProperty [ owl:inverseOf void+:hasValidation ] ; owl:someValuesFrom void+:LinkDataset ] ; rdfs:subClassOf void:Dataset , [ rdf:type owl:Restriction ; owl:onProperty void+:hasAccepted ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] , [ rdf:type owl:Restriction ; owl:onProperty void+:hasRejected ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] , [ rdf:type owl:Restriction ; owl:onProperty void+:hasValidations ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] , [ rdf:type owl:Restriction ; owl:onProperty void+:toBeValidated ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] ; rdfs:comment \"A void:Dataset that contains triples describing the validaty of links stemmed from a void+:LinkDataset.\"@en . ### http://purl.org/dc/terms/Agent dcterms:Agent rdf:type owl:Class ; rdfs:comment \"A resource that acts or has the power to act.\"@en . ### http://rdfs.org/ns/void#Dataset void:Dataset rdf:type owl:Class ; rdfs:comment \"A set of RDF triples that are published, maintained or aggregated by a single provider.\"@en . ### http://rdfs.org/ns/void#Linkset void:Linkset rdf:type owl:Class ; rdfs:subClassOf void+:DirectedLinkDataset , [ owl:intersectionOf ( [ rdf:type owl:Restriction ; owl:onProperty void:objectsTarget ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onClass void:Dataset ] [ rdf:type owl:Restriction ; owl:onProperty void:subjectsTarget ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onClass void:Dataset ] ) ; rdf:type owl:Class ] ; rdfs:comment \"A collection of RDF links between *two* void:Datasets.\"@en . ### http://rdfs.org/ns/void#TechnicalFeature void:TechnicalFeature rdf:type owl:Class ; rdfs:comment \"A technical feature of a void:Dataset, such as a supported RDF serialization format.\"@en . ### http://www.w3.org/1999/02/22-rdf-syntax-ns#Property rdf:Property rdf:type owl:Class ; rdfs:comment \"The class of RDF properties.\"@en . ### http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq rdf:Seq rdf:type owl:Class ; rdfs:comment \"The class of RDF 'Sequence' containers.\"@en . ### http://www.w3.org/2000/01/rdf-schema#Class rdfs:Class rdf:type owl:Class ; rdfs:comment \"This is the class of resources that are RDF classes.\"@en . ### Generated by the OWL API (version 4.5.9.2019-02-01T07:24:44Z) https://github.com/owlcs/owlapi 4. VoID+ Documentation \u00b6 Markdown documentation created by pyLODE 2.8.3 Ontology URI : http://lenticularlens.org/voidPlus Ontology RDF : download voidPlus.owl 4.1 Overview \u00b6 See Figure 7 and 8 above for an overview. Namespaces \u00b6 BASE <http://lenticularlens.org/voidPlus/> PREFIX cc : <https://creativecommons.org/ns#> PREFIX dcterms : <http://purl.org/dc/terms/> PREFIX owl : <http://www.w3.org/2002/07/owl#> PREFIX prov : <http://www.w3.org/ns/prov#> PREFIX rdf : <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX rdfs : <http://www.w3.org/2000/01/rdf-schema#> PREFIX sdo : <https://schema.org/> PREFIX skos : <http://www.w3.org/2004/02/skos/core#> PREFIX void : <http://rdfs.org/ns/void#> PREFIX void+ : <http://lenticularlens.org/voidPlus/> PREFIX xsd : <http://www.w3.org/2001/XMLSchema#> 4.2 Classes \u00b6 void:Dataset \u00b6 Property Value URI http://rdfs.org/ns/void#Dataset Description A set of RDF triples that are published, maintained or aggregated by a single provider. Sub-classes void+:ResourceSelection void+:LinkDataset void+:Validation void+:Partition void+:IntegrationView In domain of void:entities void:triples void:distinctSubjects void+:partitionedBy void+:hasSubset void:property void:feature void:distinctObjects void+:hasFormulation void:subset In range of void:target void+:subjectsTarget void+:hasTarget void:subset void+:objectsTarget Linkset \u00b6 Property Value URI http://lenticularlens.org/voidPlus/Linkset Description A collection of RDF links between one , two or more void:Datasets. Only, all links share the same link-predicate and all subjects stem from a set of void datasets (D 1 ) while all objects stem from either the same D 1 or a different set of void datasets D 2 . Super-classes void+:DocumentedLinkDataset void+:DirectedLinkDataset Equivalent to void+:DirectedLinkDataset and ( void+:hasFormulation some void+:LinksetFormulation ) void:Linkset \u00b6 Property Value URI http://rdfs.org/ns/void#Linkset Description A collection of RDF links between two void:Datasets. Super-classes void+:DirectedLinkDataset ( void:objectsTarget exactly 1 void:Dataset ) and ( void:subjectsTarget exactly 1 void:Dataset ) In domain of void:linkPredicate void:target void:objectsTarget void:subjectsTarget Lens \u00b6 Property Value URI http://lenticularlens.org/voidPlus/Lens Description Similar to a void+:Linkset (in the sense that it is a collection of links sharing the same linktype) also involving one, two or more datasets, but not necessarilly directed. Moreover, context-wise, it differs from a linkset as it is generated using different processes (set-like link manipulation operators such as Union, Intersection, Difference or Transitivity) as compared to how a linkset comes about (matching algorithms). Super-classes void+:DocumentedLinkDataset Equivalent to void+:DocumentedLinkDataset and ( void+:hasFormulation some void+:LensFormulation ) Partition \u00b6 Property Value URI http://lenticularlens.org/voidPlus/Partition Description A void:Daset that is subset of another void:Daset. Super-classes void:Dataset Equivalent to void:subset some void:Dataset Sub-classes void+:PropertyPartition void+:LanguagePartition void+:ClassPartition In range of void+:hasSubset ClassPartition \u00b6 Property Value URI http://lenticularlens.org/voidPlus/ClassPartition Description A void+:Partition that is a collection of resources partitioned on the basis of an rdfs:Class. Super-classes void+:Partition Equivalent to void:class exactly 1 rdfs:Class inverse ( void+:hasClassPartition ) some void:Dataset PropertyPartition \u00b6 Property Value URI http://lenticularlens.org/voidPlus/PropertyPartition Description A void+:Partition that is is a collection of resources described using either a rdf:Property or sequence of them (void+:PropertySequence or void+:TypedPropertySequence). Super-classes void+:Partition Equivalent to void:property exactly 1 rdf:Property inverse ( void+:has Property Partition ) some void:Dataset In domain of void+:hasValueFunction void+:hasFilterFunction void+:hasFormatFunction LanguagePartition \u00b6 Property Value URI http://lenticularlens.org/voidPlus/LanguagePartition Description A void+:Partition that is a collection of resources based on a void+:Language restriction, i.e. an ISO 639 standardised language code. Super-classes void+:Partition Equivalent to void+:language some void+:Language inverse ( void+:hasLanguagePartition ) some void:Dataset ResourceSelection \u00b6 Property Value URI http://lenticularlens.org/voidPlus/ResourceSelection Description A collection of resources stemmed from the same void:Dataset. It is expected to have a void+:ClassPartition as partition, directly or indirectly (i.e. as a subset). It can also have void+:PropertyPartitions, where property-value(s) can be restricted, as well as void+:LanguagePartitions, where the language of the property-value(s) can be restricted. Super-classes void:Dataset void:subset some void:Dataset void+:hasSubset some void+:Partition dcterms:description some rdfs:Literal Equivalent to void+:Partition and ( void+:hasFormulation some void+:PartitionFormulation ) In range of void+:hasResourceSelection LinkDataset \u00b6 Property Value URI http://lenticularlens.org/voidPlus/LinkDataset Description A collection of RDF links between dataset(s), using the same link predicate (regardless of the link being an equality predicate or not). This predicate is called the linktype of the linkset. The links may have been stabilished among one (deduplication), two or many datasets. Super-classes void:Dataset dcterms:description some rdfs:Literal Equivalent to ( void+:linkPredicate some rdf:Property ) and ( void+:hasTarget some void:Dataset ) Sub-classes void+:ValidatedLinkDataset void+:DocumentedLinkDataset void+:DirectedLinkDataset In domain of void+:hasValidation void+:hasClusters void+:hasContradictions void+:hasTarget DirectedLinkDataset \u00b6 Property Value URI http://lenticularlens.org/voidPlus/DirectedLinkDataset Description A void+:LinkDataset that imposes that subjects always belong to the same datasets as well as objects. Super-classes void+:LinkDataset void:distinctObjects exactly 1 void:distinctSubjects exactly 1 Sub-classes void+:Linkset void:Linkset In domain of void+:objectsTarget void+:subjectsTarget DocumentedLinkDataset \u00b6 Property Value URI http://lenticularlens.org/voidPlus/DocumentedLinkDataset Description A void+:LinkDataset that requires detailed documentation, namely it imposes that targets are described as void+:ResourceSelection and requires a void+:Formulation. Super-classes void+:LinkDataset Equivalent to void+:LinkDataset and ( void+:hasFormulation some void+:Formulation ) and ( void+:hasTarget some void+:ResourceSelection ) and ( void+:hasTarget only void+:ResourceSelection ) Sub-classes void+:Lens void+:Linkset ValidatedLinkDataset \u00b6 Property Value URI http://lenticularlens.org/voidPlus/ValidatedLinkDataset Description A void+:LinkDataset that has been validated. Super-classes void+:LinkDataset void+:hasContradictions exactly 1 void+:hasValidations exactly 1 void+:toBeValidated exactly 1 Equivalent to void+:LinkDataset and ( void+:hasValidation some void+:Validation ) Formulation \u00b6 Property Value URI http://lenticularlens.org/voidPlus/Formulation Description A (reusable) formulation that lists a number of items (e.g. matching methods or linksets) and describes how they are meant to be combined (e.g. using logic or set operators). Sub-classes void+:PartitionFormulation void+:LensFormulation void+:LinksetFormulation In domain of void+:hasItem void+:hasFormulaDescription In range of void+:hasFormulation LensFormulation \u00b6 Property Value URI http://lenticularlens.org/voidPlus/LensFormulation Description A resource that makes explicit all void+:LinkDatasets involved in the creation of a void+:Lens and how the they are combined. Super-classes void+:Formulation void+:hasItem min 2 void+:LinkDataset LinksetFormulation \u00b6 Property Value URI http://lenticularlens.org/voidPlus/LinksetFormulation Description A resource that makes explicit all void+:MatchingMethods involved in the creation of a void+:Linkset and how the methods are logically joint. Super-classes void+:Formulation void+:hasItem some void+:MatchingMethod PartitionFormulation \u00b6 Property Value URI http://lenticularlens.org/voidPlus/PartitionFormulation Super-classes void+:Formulation void+:hasItem some void+:Partition IntegrationView \u00b6 Property Value URI http://lenticularlens.org/voidPlus/IntegrationView Description A set of RDF triples that are stemmed from two or more void+:ResourceSelections that are linked by at least one void+:LinkDataset. One may add void+:PropertyPartitions for both filtering or for data visualisation and analysis. Super-classes void:Dataset ( void+:hasSubset min 2 void+:ResourceSelection ) and ( void+:hasSubset min 1 void+:LinkDataset ) Language \u00b6 Property Value URI http://lenticularlens.org/voidPlus/Language Description A set of standardised language (ISO 639 not 3166) codes. For example, use nl, nld or dut to indicate the Dutch or Flemish languages. In range of void+:language MatchingAlgorithm \u00b6 Property Value URI http://lenticularlens.org/voidPlus/MatchingAlgorithm Description A set of rules followed by a computer for finding pairs of matching resources. An algorithm can be further categorised as automated or semi-automated depending on whether it requires user assistance or not. Most matching algorithms employed in the Lenticular Lens are semi-automated. In range of void+:hasAlgorithm MatchingMethod \u00b6 Property Value URI http://lenticularlens.org/voidPlus/MatchingMethod Description A resource that makes explicit all arguments/pre-requisites (datasets, entity-type and property-value restrictions, matching properties\u2026) of a matching algorithm including the conditions in which the algorithm is to accept a discovered link (threshold). Equivalent to void+:hasAlgorithm some void+:MatchingAlgorithm In domain of void+:hasThresholdAcceptanceOperator void+:hasAlgorithm void+:hasThreshold void+:hasThresholdRange PropertySequence \u00b6 Property Value URI http://lenticularlens.org/voidPlus/PropertySequence Description A resource that represents a sequence of properties, analogously to a property path. Super-classes rdf:Seq Equivalent to rdfs:member only ( rdf:Property or rdfs:Class ) and rdfs:member min 2 rdf:Property and rdfs:member min 1 rdfs:Class TypedPropertySequence \u00b6 Property Value URI http://lenticularlens.org/voidPlus/TypedPropertySequence Description A resource that represents a sequence of properties and classes, as in a different type of property path with class restriction in between: property -> class -> property -> class -> property Super-classes rdf:Seq Equivalent to rdfs:member only rdf:Property and rdfs:member min 2 rdf:Property Validation \u00b6 Property Value URI http://lenticularlens.org/voidPlus/Validation Description A void:Dataset that contains triples describing the validaty of links stemmed from a void+:LinkDataset. Super-classes void:Dataset void+:toBeValidated exactly 1 void+:hasAccepted exactly 1 void+:hasRejected exactly 1 void+:hasValidations exactly 1 Equivalent to inverse ( void+:hasValidation ) some void+:LinkDataset In range of void+:hasValidation dcterms:Agent \u00b6 Property Value URI http://purl.org/dc/terms/Agent Description A resource that acts or has the power to act. In range of dcterms:publisher void:TechnicalFeature \u00b6 Property Value URI http://rdfs.org/ns/void#TechnicalFeature Description A technical feature of a void:Dataset, such as a supported RDF serialization format. In range of void:feature rdf:Property \u00b6 Property Value URI http://www.w3.org/1999/02/22-rdf-syntax-ns#Property Description The class of RDF properties. In range of void:linkPredicate void:property rdf:Seq \u00b6 Property Value URI http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq Description The class of RDF \u2018Sequence\u2019 containers. Sub-classes void+:TypedPropertySequence void+:PropertySequence rdfs:Class \u00b6 Property Value URI http://www.w3.org/2000/01/rdf-schema#Class Description This is the class of resources that are RDF classes. In range of void:class 4.3 Object Properties \u00b6 hasAlgorithm \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasAlgorithm Description relates a void+:MatchingMethod to its void+:MatchingAlgorithm. Domain(s) void+:MatchingMethod Range(s) void+:MatchingAlgorithm hasClassPartition \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasClassPartition Description relates a void:Dataset to its subset void+:ClassPartition. In practice, the main difference wrt void:classPartition is the name meant to avoid miss-usage as it can be read as \u201cis classPartition of\u201d as well. Super-properties void:classPartition void+:hasSubset hasLanguagePartition \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasLanguagePartition Description relates a void:Dataset to its subset void+:LanguagePartition. Super-properties void+:hasSubset hasPropertyPartition \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasPropertyPartition Description relates a void:Dataset to its subset void+:PropertyPartition. In practice, the main difference wrt void:propertyPartition is the name meant to avoid miss-usage as it can be read as \u201cis propertyPartition of\u201d as well. Super-properties void:propertyPartition void+:hasSubset hasFormulation \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasFormulation Description relates a void+:Dataset to its void+:Formulation. Domain(s) void:Dataset Range(s) void+:Formulation hasItem \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasItem Description relates a void+:Formulation to its formula-items. Domain(s) void+:Formulation hasObjectResourceSelection \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasObjectResourceSelection Description a relation to assign a void+:ResourceSelection as the source for the objects of triples under scrutiny. Super-properties void+:hasResourceSelection hasSubjectResourceSelection \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasSubjectResourceSelection Description a relation to assign a void+:ResourceSelection as the source for the subjects of triples under scrutiny. Super-properties void+:hasResourceSelection hasResourceSelection \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasResourceSelection Description a relation to assign a void+:ResourceSelection as the source for entities under scrutiny.For example, the selection of a void+:ResourceSelection for a void+:MatchingMethod. Range(s) void+:ResourceSelection hasSubset \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasSubset Description relates a void:Dataset that has as subset a void+:Partition (dataset). In practice, the main difference wrt void:subset is the name meant to avoid miss-usage as it can be read as \u201cis subset of\u201d as well. For a similar purpose, an inverse relation void+:subsetOf is also defined. Super-properties void:subset Domain(s) void:Dataset Range(s) void+:Partition hasTarget \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasTarget Description A relation that assigns one or more void:Datasets linked via a void+:LinkDataset. Domain(s) void+:LinkDataset Range(s) void:Dataset hasValidation \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasValidation Description A relation that assigns one or more void+:Validations to a void+:LinkDataset. Domain(s) void+:LinkDataset Range(s) void+:Validation language \u00b6 Property Value URI http://lenticularlens.org/voidPlus/language Description A relation that assigns to a void:Dataset a void+:Language (for ISO standard language codes) that is the rdf:language of all literals (objects) qualifying entities (subject) in a language-based partition. Super-properties void+:partitionedBy Range(s) void+:Language linkPredicate \u00b6 Property Value URI http://lenticularlens.org/voidPlus/linkPredicate Description a link predicate that holds for a void+:LinkDataset. Super-properties owl:topObjectProperty objectsTarget \u00b6 Property Value URI http://lenticularlens.org/voidPlus/objectsTarget Description A relation that assigns void:Datasets describing the object of triples contained in the void+:DirectedLinkDataset. Super-properties void+:hasTarget Domain(s) void+:DirectedLinkDataset Range(s) void:Dataset subjectsTarget \u00b6 Property Value URI http://lenticularlens.org/voidPlus/subjectsTarget Description A relation that assigns void:Datasets describing the subjects of triples contained in the void+:DirectedLinkDataset. Super-properties void+:hasTarget Domain(s) void+:DirectedLinkDataset Range(s) void:Dataset v partitionedBy \u00b6 Property Value URI http://lenticularlens.org/voidPlus/partitionedBy Description A relation that assigns a void:Dataset to the conditions under which it is partitioned. Domain(s) void:Dataset property \u00b6 Property Value URI http://lenticularlens.org/voidPlus/property Description A relation that assigns to a void:Dataset a rdf:Property or a rdf:Seq of properties that expresses the pattern of all triples in a property-based partition. Super-properties void+:partitionedBy Range(s) rdf:Property rdf:Seq subsetOf \u00b6 Property Value URI http://lenticularlens.org/voidPlus/subsetOf Description relates a void+:Partition to its void:Dataset superset. Defined as the inverse relation of void+:hasSubset. dcterms:creator \u00b6 Property Value URI http://purl.org/dc/terms/creator Description The range for dcterms:creator is dcterms:Agent. All values used with this property have to be instances of the class [dcterms:Agent] . must not be used with literal values . You may use it only with non-literal values. dcterms:publisher \u00b6 Property Value URI http://purl.org/dc/terms/publisher Description The range for dcterms:publisher is dcterms:Agent. All values used with this property have to be instances of the class [dcterms:Agent] . must not be used with literal values . You may use it only with non-literal values. Range(s) dcterms:Agent void:linkPredicate \u00b6 Property Value URI http://rdfs.org/ns/void#linkPredicate Description a link predicate that holds for a void:Linkset. Super-properties void+:linkPredicate Domain(s) void:Linkset Range(s) rdf:Property void:objectsTarget \u00b6 Property Value URI http://rdfs.org/ns/void#objectsTarget Description A relation that assigns the void:Dataset describing the objects of triples contained in the void:Linkset. Super-properties void:target void+:objectsTarget Domain(s) void:Linkset void:subjectsTarget \u00b6 Property Value URI http://rdfs.org/ns/void#subjectsTarget Description A relation that assigns the void:Dataset describing the subjects of triples contained in the void:Linkset. Super-properties void:target void+:subjectsTarget Domain(s) void:Linkset void:property \u00b6 Property Value URI http://rdfs.org/ns/void#property Description A relation that assigns to a void:Dataset a rdf:Property that is the predicate of all triples in a property-based partition. Super-properties void+:property Domain(s) void:Dataset Range(s) rdf:Property void:class \u00b6 Property Value URI http://rdfs.org/ns/void#class Description A relation that assigns to a void:Dataset a rdfs:Class that is the rdf:type of all entities (subject) in a class-based partition. Super-properties void+:partitionedBy Range(s) rdfs:Class void:classPartition \u00b6 Property Value URI http://rdfs.org/ns/void#classPartition Description A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only entities of an explicitly defined rdfs:Class. Super-properties void:subset void:propertyPartition \u00b6 Property Value URI http://rdfs.org/ns/void#propertyPartition Description A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only triples composed with an explicitly defined rdf:Property. Super-properties void:subset void:feature \u00b6 Property Value URI http://rdfs.org/ns/void#feature Description relates a void:TechnicalFeature supported by a void:Datset. Domain(s) void:Dataset Range(s) void:TechnicalFeature void:subset \u00b6 Property Value URI http://rdfs.org/ns/void#subset Description relates a void:Dataset that has subset another void:Dataset. Domain(s) void:Dataset Range(s) void:Dataset void:target \u00b6 Property Value URI http://rdfs.org/ns/void#target Description A relation that assigns one of the two void:Datasets linked by the void:Linkset. Super-properties void+:hasTarget Domain(s) void:Linkset Range(s) void:Dataset rdfs:member \u00b6 Property Value URI http://www.w3.org/2000/01/rdf-schema#member Description rdfs:member is an instance of rdf:Property that is a super-property of all the container membership properties. In particular, properties called rdf:_1, rdf:_2, rdf:_3\u2026 etc., used for rdf:Seq are sub-properties of rdfs:member. 4.4 Datatype Properties \u00b6 cc:attributionName \u00b6 Property Value URI http://creativecommons.org/ns#attributionName Description The name the creator of a Work would like used when attributing re-use. cc:license \u00b6 Property Value URI http://creativecommons.org/ns#license Description A work has license a License. hasAccepted \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasAccepted Description The amount of accepted links for a void+:LinkDataset or void+:Validation. Domain(s) ( void+:LinkDataset or void+:Validation ) Range(s) xsd:integer hasClusters \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasClusters Description The amount of clusters computed for a void+:LinkDataset. Domain(s) void+:LinkDataset Range(s) xsd:integer hasContradictions \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasContradictions Description The amount of links of a void+:LinkDataset for which contradicting validations are found. Domain(s) void+:LinkDataset Range(s) xsd:integer hasFilterFunction \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasFilterFunction Description It relates a property partition to a filter function, such as \u201ccontains\u201d or \u201cminimal_date\u201d. When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \u201cfunction\u201d, a \u201cvalue\u201d and a \u201cformat\u201d. Domain(s) void+:PropertyPartition hasFormatFunction \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasFormatFunction Description It relates a property partition to a format restriction which is applicable to the value used by an explicitly defined filter function, such as \u201cYYYY-MM-DD\u201d for filter \u201cminimal_date\u201d. When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \u201cfunction\u201d, a \u201cvalue\u201d and a \u201cformat\u201d. Domain(s) void+:PropertyPartition hasFormulaDescription \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasFormulaDescription Description Describes how items listed in a formulation are combined. For example, using logic or set-like operators. Domain(s) void+:Formulation Range(s) xsd:string hasRejected \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasRejected Description The amount of rejected links for a void+:LinkDataset or void+:Validation. Domain(s) ( void+:LinkDataset or void+:Validation ) Range(s) xsd:integer hasThreshold \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasThreshold Description Relates a void+:MatchingMethod to a link-acceptance-threshold value. Upon the choice of a void+:MatchingAlgorithm, the user should provide the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \u201cYear\u201d), a threshold operator (e.g. \u201c>=\u201d) and a threshold range (e.g. \u201c]0,1]\u201d or \u201c\u2115\u201d). Domain(s) void+:MatchingMethod hasThresholdAcceptanceOperator \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasThresholdAcceptanceOperator Description Relates a void+:MatchingMethod to a link-acceptance-threshold operator. Upon the choice of a void+:MatchingAlgorithm, the user should inform the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \u201cYear\u201d), a threshold operator (e.g. \u201c>=\u201d) and a threshold range (e.g. \u201c]0,1]\u201d or \u201c\u2115\u201d). Domain(s) void+:MatchingMethod hasThresholdRange \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasThresholdRange Description Relates a void+:MatchingMethod to a link-acceptance-threshold range. Upon the choice of a void+:MatchingAlgorithm, the user should inform the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \u201cYear\u201d), a threshold operator (e.g. \u201c>=\u201d) and a threshold range (e.g. \u201c]0,1]\u201d or \u201c\u2115\u201d). Domain(s) void+:MatchingMethod hasValidations \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasValidations Description The amount of validated links in a void+:LinkDataset or void+:Validation. Domain(s) ( void+:LinkDataset or void+:Validation ) Range(s) xsd:integer hasValueFunction \u00b6 Property Value URI http://lenticularlens.org/voidPlus/hasValueFunction Description It relates a property partition to a value for a filter function, such as \u201c%van%\u201d for filter \u201ccontains\u201d or \u201c01/01/1600\u201d for filter \u201cminimal_date\u201d. When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \u201cfunction\u201d, a \u201cvalue\u201d and a \u201cformat\u201d. Domain(s) void+:PropertyPartition toBeValidated \u00b6 Property Value URI http://lenticularlens.org/voidPlus/toBeValidated Description The amount of links yet to be validated in a void+:LinkDataset or void+:Validation. Domain(s) ( void+:LinkDataset or void+:Validation ) Range(s) xsd:integer dcterms:created \u00b6 Property Value URI http://purl.org/dc/terms/created Description The range defined for dcterms:created is the class of rdfs:Literal. Values used with this property therefore have to be literal values. Range(s) rdfs:Literal dcterms:description \u00b6 Property Value URI http://purl.org/dc/terms/description Description There is no range defined for dcterms:description. Therefore you can use it with literal values. void:distinctObjects \u00b6 Property Value URI http://rdfs.org/ns/void#distinctObjects Description The total number of distinct objects in a void:Dataset. In other words, the number of distinct resources that occur in the object position of triples in the dataset. Literals are included in this count. Domain(s) void:Dataset Range(s) xsd:integer void:distinctSubjects \u00b6 Property Value URI http://rdfs.org/ns/void#distinctSubjects Description The total number of distinct subjects in a void:Dataset. In other words, the number of distinct resources that occur in the subject position of triples in the dataset. Domain(s) void:Dataset Range(s) xsd:integer void:entities \u00b6 Property Value URI http://rdfs.org/ns/void#entities Description The total number of entities that are described in a void:Dataset. Domain(s) void:Dataset Range(s) xsd:integer void:triples \u00b6 Property Value URI http://rdfs.org/ns/void#triples Description The total number of triples contained in a void:Dataset. Domain(s) void:Dataset Range(s) xsd:integer","title":"3. Link Annotation Ontology"},{"location":"03.Ontology/#ontology","text":"This section presents an Ontology meant for describing processes of generation and validation of links in detail, so that decisions made such as resource selections and matching options are made explicit. Those processes are implemented in the Lenticular Lens tool and result in the creation of Linksets or Lenses according to a specification. we call the proposed ontology VoID+ as it is proposed as an extension of the [ VoID ] vocabulary. The next section presents the motivation for such extension while Section 3.2 and Section 3.3 respectively present the main elements proposed as extension for VoID and the complete and detailed description of the VoID+ extension.","title":"ONTOLOGY"},{"location":"03.Ontology/#1-motivation-to-extend-void","text":"The model presented in Figure 1 is created based on the [ VoID ] documentation and owl-ontology description, where ellipses represent classes, thick arrows represent subclass or sub-property relations (hierarchy) and thin arrows represent properties. In this model, only classes and relations of interest are exhibit. Fig 1: Excerpt of VoId Ontology regarding void:Dataset and void:Linkset. The [ VoID ] vocabulary provides means to describe datasets and linksets, but with limitations. Too strict linkset definition : The first important limitation that motivates our proposal for extension is that void:Linkset is (1) too restrictive as it is directed and holds between exactly two non-identical datasets. This semantic does not allow a linkset to hold for a dataset deduplication. Data Partitioning : The next limitation is the ambiguous descriptions of subsets or partition of a void:Dataset . This concept is in our theory very important and requires more clarity and expressivity. Direction : (2) The reading direction of the property void:subset is \u201chas subset\u201d but it can easily be misused as it can also be read as \u201cis subset of\u201d as well. Example 1: The Direction of the void:subset property The extract below are from the VoID documentation page . The first annotation on the Aggregate Dataset ex:Aggregate_DS reads as \u201cex:Aggregate_DS has subset ex:DS_A and ex:DS_B\u201d , which is the correct reading for stating that \u201cex:Aggregate_DS is composed of Datases A and B. Now, to state that a linkset is a part of a larger dataset, they again use the same void:subset property. However, the annotation statement reads in the opposite direction as \u201cDBpedia_Geonames is subset of DBpedia\u201d . To our understanding, the latter reading can not be correct. ex: Aggregate_DS a void: Dataset ; dcterms: title \"Aggregate Dataset\" ; dcterms: description \"An aggregate of the A and B datasets.\" ; void: sparqlEndpoint <http://example.org/sparql> ; void: subset ex: DS_A ; void: subset ex: DS_B . ex: DBpedia_Geonames a void: Linkset ; void: target : DBpedia ; void: target : Geonames ; void: subset : DBpedia ; void: triples 252000 . Combination : VoID allows a dataset to originate from a combination of class-partitions and/or property-partitions. However, (3) it falls short in providing means to make clear how several partitions of a void:Dataset ought to be combined. In Example 2, ex:myData is a void:Dataset based on class and property partitions, stating that the dataset is of entities of type ex:AIStudent who had an internship or exchange program. From this, how one would differ between a Partition formed of entities that are [AIStudents and (had an internship or had an exchange program)] from another formed by entities that are [AIStudents or had an internship or had an exchange program] ? Example 2: Interpreting a partition ex: myData a void: Dataset ; void: classPartition [ void: class ex: AIStudent ] ; void: propertyPartition [ void: property ex: hasIntership ] ; void: propertyPartition [ void: property ex: hasExchangeProgram ] . Value-range restriction : Moreover, (4) it also falls short on providing means to restrict entities based on the value-range of a property, or language for that matter. How one would differ between a Partition formed of entities that are [AIStudents and born before the year 2000] from another formed by entities that are [AIStudents or born before the year 2000] ? Reproducibility , (5) it also does not describe the details of how void:Linkset is generated. Others : Furthermore, (6) it does not provide means to describe Lenses or Validations . While a description of the classes and properties exhibited in Fig 1 is made available in the table below, limitations of the VoID vocabulary and possible solutions are further discussed in the next section. Vocabulary Description void:Dataset A set of RDF triples that are published, maintained or aggregated by a single provider. void:Linkset A collection of RDF links between two void:Datasets. void:subset has subset. Domain : void:Dataset Range : void:Dataset void:classPartition A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only entities of an explicitly defined rdfs:Class. Domain : void:Dataset Range : void:Dataset void:propertyPartition A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only triples composed with an explicitly defined rdf:Property. Domain : void:Dataset Range : void:Dataset void:class A relation that assigns to a void:Dataset a rdfs:Class that is the rdf:type of all entities (subject) in a class-based partition. Domain : void:Dataset Range : rdfs:Class ( exactly 1 ) void:property A relation that assigns to a void:Dataset a rdf:Property that is the predicate of all triples in a property-based partition. Domain : void:Dataset Range : rdfs:Property ( exactly 1 ) void:target A relation that assigns one of the two datasets linked by the Linkset. Domain : void:Linkset Range : void:Dataset ( exactly 2 ) void:subjectsTarget A relation that assigns the dataset describing the subjects of triples contained in the Linkset. Domain : void:Linkset Range : void:Dataset ( exactly 1 ) void:objectsTarget A relation that assigns the dataset describing the objects of the triples contained in the Linkset. Domain : void:Linkset Range : void:Dataset ( exactly 1 )","title":"1. Motivation to extend VoID"},{"location":"03.Ontology/#2-void-main-elements","text":"This section presents the main elements proposed as extension for VoID . It provides a simplified overview (Fig. 2) of VoID+ by (i) only describing the classes and properties that are of importance in this work and (ii) omitting hierarchical relations among classes. While the complete and detailed description of the VoID+ extension is provided in the next section, the complete ontology overview (figure) and the documentation extracted from the owl file are available in section 3.4. Fig 2: The Lenticular Lens Ontology The partial model of the simplified oververview depicted in Fig. 2 highlights in yellow the use of [ VoID ] terms and in blue the new VoID+ terminology . In order to describe the proposed ontology, we dissect Fig 2 into four parts. First, a Resource Selection is elucidated. Second, we go about describing a LinksetFormulation and show how it connects with a Resource Selection . The third step highlights that the description of a Linkset metadata involves specifying the Resource Selections used at the source and target positions of an entity matching process, the Linkset Formulation , eventual validations plus statistics and authority information. The fourth and final step focuses on the annotation of a Lens by describing the combination of one or more Linksets and/or Lenses.","title":"2. VoID+ Main Elements"},{"location":"03.Ontology/#21-resource-selection","text":"This step concerns the selection of the resources under scrutiny , that can potentially end up co-referent entities across or within datasources during an entity matching process. To therefore perform a matching, one first needs not only to select datasource(s) but also restrict which resources will undergo the matching. The first way of doing so is by applying a type (class) restriction. This is mandatory in the Lenticular Lens process as matching algorithms are not fully automated. Down this line, further restrictions can be applied by forcing the value of a number of properties to lie within a certain range. A Resource Selection is thereby, the annotation of such process. In the ontology excerpt depicted in Figure 3 we propose the entity type Resource Selection , which is a void+:Partition based on a void:classPartition and/or void:propertyPartition . While a void:classPartition solely consists in specifying the type of entity under scrutiny, the void:propertyPartition entails a little more. It consists in specifying a property or property path and a restriction that the selected property should undergo for the selection of the right entities for the further down the road entity matching process. Those restrictions can be combined using a Formula Description given by void+:hasFormulaDescription . Fig 3: Selecting a matching resource Example 3 : Resource Selection In this example, the entity resource:ResourceSelection-2 is a void+:ResourceSelection (and a void:Dataset ) subset of resource:index_op_doopregister_raw_20190830 and also a collection (partition) of entities of type pnv:PersonName where each entity passed the filter test of (1) name in the English language which appears without trailing dots \"%...%\"@en and (2) birthdates within the interval [1600, 1699] . the entity resource:ResourceSelection-2 lists all three entities of type void+:PropertyConstraint and elaborates on the logic expression that binds all restrictions. For example, the property restriction described by resource:PropertyConstraint-PHce78383e3ff6e9dd73b6 documents that, applying the date function \"minimal_date\"@en over dates in the format the \"YYYY-MM-DD\"@en with the year restriction of 1600 makes sure that only persons born on 1600 onwards are admitted. Turtle Syntax When ever a literal in RDF syntax conatins quote or new line characters, the litreal should be in a three quote syntax ( \"...\"\"@en ). In the example below, we deliberately wrote the literal value of void+:hasFormulaDescription is in a single quote ( \"...\"\"@en ) instead of a triple quote ( \"\"\"%...%=\"\"\"@en ) as the syntax highliter is somewhat buggy. ### RESOURCE 2 resource: ResourceSelection-2 a void: dataset , void+ : ResourceSelection ; rdfs: label \"Baptisms in the 17th Century\" @ en ; void: subset resource: index_op_doopregister_raw_20190830 ; void: classPartition [ void: class pnv: PersonName ] ; void: propertyPartition resource: PropertyConstraint-PHea6802ef02f99a848859 ; void: propertyPartition resource: PropertyConstraint-PHce78383e3ff6e9dd73b6 ; void: propertyPartition resource: PropertyConstraint-PH2580641bbdd572759cb9 ; void+ : hasFormulaDescription \" resource: PropertyConstraint-PHce78383e3ff6e9dd73b6 AND resource: PropertyConstraint-PH2580641bbdd572759cb9 AND ( resource: PropertyConstraint-PHea6802ef02f99a848859 ) \"@en . resource: PropertyConstraint-PHea6802ef02f99a848859 a void+ : PropertyConstraint ; void: property [ a rdfs: Sequence ; rdf : _ 1 pnv: literalName ] ; void+ : hasFilterFunction \"not_ilike\" @ en ; void+ : hasValueFunction \"%...%\" @ en . resource: PropertyConstraint-PHce78383e3ff6e9dd73b6 a void+ : PropertyConstraint ; void: property [ a rdfs: Sequence ; rdf : _ 1 saa: isInRecord ; rdf : _ 2 saa: IndexOpDoopregisters ; rdf : _ 3 saa: birthDate ] ; void+ : hasFilterFunction \"minimal_date\" @ en ; void+ : hasValueFunction 1600 ; void+ : hasFormatFunction \"YYYY-MM-DD\" @ en . resource: PropertyConstraint-PH2580641bbdd572759cb9 a void+ : PropertyConstraint ; void: property [ a rdfs: Sequence ; rdf : _ 1 saa: isInRecord ; rdf : _ 2 saa: IndexOpDoopregisters ; rdf : _ 3 saa: birthDate ] ; void+ : hasFilterFunction \"maximum_date\" @ en ; void+ : hasValueFunction \"1699\" @ en ; void+ : hasFormatFunction \"YYYY-MM-DD\" @ en .","title":"2.1 Resource Selection"},{"location":"03.Ontology/#22-linkset-formulation","text":"For simple matching problems, finding co-referents can be done using a single matching algorithm (matcher). However, time and again the data reality often imposes the use of more than one matcher instead. In this latter scenario, clearly reporting on how these matchers work together for detecting co-referents is essential. A Linkset Formulation entity is a resource for just doing the aforementioned, as depicted in Figure 4. Once resources of type Resource Restriction are created, one can go ahead and used them for specifying the restricted collections to be used in a particular Matching Method , which also specifies the Matching Algorithm and its arguments such as threshold, range and operator. In the end, all Matching Methods used in a matching process are documented in the Linkset Formulation resource as well as how they bind together in a logic expression given by the predicate void+:hasFormulaDescription . Fig 4: Specifying the way in which methods are logically combined Example 4 : Linkset Logic Expression In Example 2.8, the resource:PHb99da2ecd91ad533af65 is a void+:MatchingFormulation listing eight void+:MatchingMethods used for creating a linkset. They their logic combination is described as void+:hasFormulaDescription . Among the void+:MatchingMethods , the resource:TIME_DELTA-PHfdc744f6bd0ced4e283a is the only method detailes in this example, documenting the four void+:ResourceSelections involved, as well as the chosen void+:MatchingAlgorithm , namely resource:TIME_DELTA , besides the threshold (20), threshold-unit (\u201cYear\u201d@en), threshold-operator (>=) and threshold-range \u201c\u2115\u201d of the matching method. Turtle Syntax When ever a literal in RDF syntax conatins quote or new line characters, the litreal should be in a three quote syntax ( \"...\"\"@en ). In the example below, we deliberately wrote the literal value of void+:hasFormulaDescription is in a single quote ( \"...\"\"@en ) instead of a triple quote ( \"\"\"%...%=\"\"\"@en ) as the syntax highliter is somewhat buggy. ###################################################### # LINKSET LOGIC EXPRESSION # ###################################################### resource: PHb99da2ecd91ad533af65 a void+ : MatchingFormulation ; void+ : hasMethod resource: TIME_DELTA-PHfdc744f6bd0ced4e283a ; void+ : hasMethod resource: Exact-PH6491d1db6855098a70be ; void+ : hasMethod resource: LL_SOUNDEX-PH0ad3ad579d7a29347753 ; void+ : hasMethod resource: BLOOTHOOFT_REDUCT-PH10433274b57dafdd1335 ; void+ : hasMethod resource: Exact-PH4d4187a08c3ba4c1cf0d ; void+ : hasMethod resource: TIME_DELTA-PHe40547b9d3b6381347b4 ; void+ : hasMethod resource: LEVENSHTEIN_APPROX-PH10f4c17bbf933cae647f ; void+ : hasMethod resource: BLOOTHOOFT_REDUCT-PH98a9575087817b951447 ; void+ : hasFormulaDescription \" resource: TIME_DELTA-PHe40547b9d3b6381347b4 AND resource: TIME_DELTA-PHfdc744f6bd0ced4e283a AND ( resource: Exact-PH4d4187a08c3ba4c1cf0d OR ( resource: BLOOTHOOFT_REDUCT-PH98a9575087817b951447 AND resource: BLOOTHOOFT_REDUCT-PH10433274b57dafdd1335 ) OR ( resource: Exact-PH6491d1db6855098a70be AND ( resource: LL_SOUNDEX-PH0ad3ad579d7a29347753 AND resource: LEVENSHTEIN_APPROX-PH10f4c17bbf933cae647f ) ) ) \"@en . ###################################################### # METHOD SIGNATURES # ###################################################### ### METHOD SPECIFICATIONS TIME_DELTA resource: TIME_DELTA-PHe40547b9d3b6381347b4 a void+ : MatchingMethod ; void+ : hasAlgorithm resource: TIME_DELTA ; void+ : hasThresholdRange \"\u2115\" ; ### SOURCE PREDICATE CONFIGURATION void+ : hasSubjResourceSelection resource: ResourceSelection-PHbe38976fdf884b6c4a8e ; void+ : hasSubjResourceSelection resource: ResourceSelection-PHe8fa664d04ad00aaa697 ; ### TARGET PREDICATE CONFIGURATION void+ : hasObjResourceSelection resource: ResourceSelection-PH71818c17d54a8fbec22b ; void+ : hasObjResourceSelection resource: ResourceSelection-PHc8a3c6e494d230b79a6b . \u2022\u2022\u2022","title":"2.2 Linkset Formulation"},{"location":"03.Ontology/#23-linkset","text":"This step documents a linkset metadata including WHAT - HOW - WHEN - WHO and other processes explaining the aboutness of links. The Linkset Formulation specifies HOW entities are matched and Resource Selection specifies WHAT to match as subject and object targets. Also some statistic on the matching results can be reported such as the number of links found, the numbers of entities linked, WHO created the linkset and WHEN . Finally, a Validation entity can also be specified, comprising metadata with statitics and auhtority information on the validation process. Observe that when one or more validations are provided, statistics on this matter can be included in the linkset metadata, including eventual contradictions if one validation says a link is correct while another says it is not. As discussed earlier in this section, according to the [ VoID ] documentation, the void:Linkset definition expects as datasources exactly one source and one target, different from each other. This means it is more restrictive than the void+:Linkset here proposed, since the latter also expects a linkset to contain links within a datasource or across more than two. Therefore, we do not directly reuse that concept (and its correspoding properties void:subjects/objectsTarget). Naturally, one could still use void:Linkset for other purposes, but at the risk of abusing the VoID vocabulary if its instances do not really fit the required restrictions. Moreover, a void:Linkset (i.e. a resource representing a linkset\u2019s metadata) is also not an instance of void+:Linkset since the later requires the description of the processes underlying the creation of the links, which is not the case for the first. Fig 5: Specifying the linkset\u2019s context Example 5: A linkset annotation ### LINKSET 15 linkset : 15 a void+ : Linkset ; void: feature format: Turtle ; cc: attributionName \"LenticularLens\" @ en ; cc: license <http://purl.org/NET/rdflicense/W3C1.0> ; dcterms: created \"2020-09-26T09:37:23.933624\" ^^ <http://www.w3.org/2001/XMLSchema#dateTime> ; dcterms: creator \"AL IDRISSOU\" ; dcterms: creator \"GoldenAgents\" ; void: linkPredicate owl: sameAs ; rdfs: label \"Linkset 9\" @ en ; dcterms: description \"LINSET-15-9: Test Baptism against Marriage and burial with several nested methods \" @ en ; ### VOID LINKSET STATS void: entities 12580 ; ### LENTICULAR LENS LINKSET STATS ### SOURCE ENTITY TYPE SELECTION(S) void+ : subjectsTarget resource: ResourceSelection-2 ; ### TARGET ENTITY TYPE SELECTION(S) void+ : objectsTarget resource: ResourceSelection-1 ; void+ : objectsTarget resource: ResourceSelection-3 ; ### THE LOGIC FORMULA void+ : hasLogicFormulation resource: PHb6e5e320dc08d7d9dd98 .","title":"2.3 Linkset"},{"location":"03.Ontology/#24-lens","text":"Another process relevant to document is the creation of Lenses. In short, a lens is the result of a set-like operation over one or more Linkset and or Lens. Therefore, the entity Lens documents them as void+:hasTarget Fig 6: Specifying the linkset\u2019s context Example 6: A lens annotation","title":"2.4 Lens"},{"location":"03.Ontology/#3-void-ontology","text":"VoID provides interesting means to annotate datasets in the broad sense \u2013 such as subset, class and property partitions, and many more. However, it does not offer a variety of dataset-types to deal with. To alleviate this limitation, VoID+ proposes a range of dataset subtypes of importance for annotation clarity , along with their relations enriched with (more precise) cardinality, domain and range restrictions that supports communicating the intended meaning of those vocabularies. Hopefully, this helps avoiding miss-usage and allows for reasoning so that possible miss-usage that lead to inconsistencies can be detected. To this end, VoID+ offers: Partition : for enabling better clarity on how a dataset is derived from another dataset based on one or more type of partitions. LinkDataset : for discriminating among sets of links depending on the (i) number of datasets involved, (ii) importance of link direction and (iii) how a link or set of links came to be. Validation : a separation of concern for clearly allowing multiple validations of the same link or set of links. IntegrationView : For expressing the result of one\u2019s view on data integration. The remainder of this section discuses the complete and detailed description of the VoID+ extension through these aforementioned discriminating types. It first addresses the void+:Partition of void:Dataset and then tackles the void+:LinkDataset which is considered a special sub-type of void:Dataset. Finally Validation and IntegrationView are discussed in the last two subsections. The documentation extracted from the owl file is presented in the next section.","title":"3. VoID+ Ontology"},{"location":"03.Ontology/#31-partitions-of-voiddataset","text":"The VoID vocabulary defines void:Dataset as a dataset superset but does make available a granularity of subsets for refined work. VoID+ extends the void:Dataset class by distinguishes between five subsets of void:Dataset . A void:Dataset that is defined as a subset of another one is called void+:Partition . Three types of void+:Partition are distinguished according to the type of partition that is used: (i) void+:ClassPartition : the dataset is the result of an rdfs:Class based-partition; (ii) void+:PropertyPartition : the dataset is the result of a rdf:Property or by a rdf:Seq \u2013 defining a sequence of two or more of properties \u2013 partition; (iii) void+:Language : the dataset is the result of a language partition given by a standardised ISO language code. Another special type of partition is the void+:ResourceSelection . It is defined by one or more partions of the type above described. Fig. 7 provides a straightforward understanding of how the class void:Dataset is extended. On its the left hand side, Fig. 7 presents a view on the classes, their hierarchy and how they are connected via object properties. On the one hand, the property void:subset and its sub-properties are extended such that they provide a directional non-ambiguous reading. On the other hand, void:class and void:property are grouped into a superclass void+:partionedBy which also includes void:language to allow for one more type of partition. In particular, void:property is extended with a more generic relation void+:property that allows for describing a sequence of properties like in a property path. Fig 7: VoID+ Extension on the void:Dataset","title":"3.1 Partitions of void:Dataset"},{"location":"03.Ontology/#32-void-link-datasets","text":"The void+:LinkDataset concept is a specific type of void:Dataset allowing for distinguishing among sets of links depending on the (i) number of datasets involved, (ii) importance of link direction and (iii) how a link or set of links came to be. A quick take on this is to say that a void+:LinkDataset is a Dataset that contains links that may have been established among one (deduplication), two or many datasets. Figure 8 illustrates on the left hand side how the class void:Linkset is extended upward \u2013 with superclasses less restrictive with respect to its targets \u2013 by showing a hierarchical view of the classes as well as how they connect via the available object properties. When the creation of a void+:LinkDataset is such that it imposes that subjects always belong to the same datasets as well as objects, this is called void+:DirectedLinkDataset . This is the case for both void:Linkset and for our definition of void+:Linkset . The former, however, requires exactly one dataset as target for its subjects and another as target for its objects. In addition, our definition of void+:Linkset . is also a void+:DocumentedLinkDataset , which requires more detailed documentation, including targets described as void+:ResourceSelection (all values from) and a void+:formulation . A void+:Lens is also a void+:DocumentedLinkDataset which has one void:LensFormulation that specifies which void:LinkDatasets are combined and how. The right hand side of Figure 8 presents a hierarchy of properties in order to provide an understanding of how void:target and its sub-properties are extended. They actually are extended with superclasses having as range not only a void:Linkset but its superclass void+:LinkDataset . Fig 8: VoID+ Extension on the void:Linkset","title":"3.2 VoID+ Link-datasets"},{"location":"03.Ontology/#33-validation","text":"coming soon","title":"3.3 Validation"},{"location":"03.Ontology/#34-integrationview","text":"","title":"3.4 IntegrationView"},{"location":"03.Ontology/#35-void-owl","text":"Here, we provide a turtle description of the ontology. VoID+ @prefix : <http://lenticularlens.org/voidPlus/> . @prefix cc: <https://creativecommons.org/ns#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix xml: <http://www.w3.org/XML/1998/namespace> . @prefix xsd: <http://www.w3.org/2001/XMLSchema#> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix void: <http://rdfs.org/ns/void#> . @prefix void+ : <http://lenticularlens.org/voidPlus/> . @prefix dcterms: <http://purl.org/dc/terms/> . @base <http://lenticularlens.org/voidPlus> . <http://lenticularlens.org/voidPlus> rdf: type owl: Ontology . ################################################################# # Object Properties ################################################################# ### http://lenticularlens.org/voidPlus/hasAlgorithm void+ : hasAlgorithm rdf: type owl: ObjectProperty , owl: FunctionalProperty ; rdfs: domain void+ : MatchingMethod ; rdfs: range void+ : MatchingAlgorithm ; rdfs: comment \"relates a void+:MatchingMethod to its void+:MatchingAlgorithm.\" @ en . ### http://lenticularlens.org/voidPlus/hasClassPartition void+ : hasClassPartition rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasSubset , void: classPartition ; rdfs: comment \"relates a void:Dataset to its subset void+:ClassPartition. In practice, the main difference wrt void:classPartition is the name meant to avoid miss-usage as it can be read as \\\"is classPartition of\\\" as well.\" @ en . ### http://lenticularlens.org/voidPlus/hasFormulation void+ : hasFormulation rdf: type owl: ObjectProperty ; rdfs: domain void: Dataset ; rdfs: range void+ : Formulation ; rdfs: comment \"relates a void+:Dataset to its void+:Formulation.\" @ en . ### http://lenticularlens.org/voidPlus/hasItem void+ : hasItem rdf: type owl: ObjectProperty ; rdfs: domain void+ : Formulation ; rdfs: comment \"relates a void+:Formulation to its formula-items.\" @ en . ### http://lenticularlens.org/voidPlus/hasLanguagePartition void+ : hasLanguagePartition rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasSubset ; rdfs: comment \"relates a void:Dataset to its subset void+:LanguagePartition.\" @ en . ### http://lenticularlens.org/voidPlus/hasObjectResourceSelection void+ : hasObjectResourceSelection rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasResourceSelection ; rdfs: comment \"a relation to assign a void+:ResourceSelection as the source for the objects of triples under scrutiny.\" @ en . ### http://lenticularlens.org/voidPlus/hasPropertyPartition void+ : hasPropertyPartition rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasSubset , void: propertyPartition ; rdfs: comment \"relates a void:Dataset to its subset void+:PropertyPartition. In practice, the main difference wrt void:propertyPartition is the name meant to avoid miss-usage as it can be read as \\\"is propertyPartition of\\\" as well.\" @ en . ### http://lenticularlens.org/voidPlus/hasResourceSelection void+ : hasResourceSelection rdf: type owl: ObjectProperty ; rdfs: range void+ : ResourceSelection ; rdfs: comment \"a relation to assign a void+:ResourceSelection as the source for entities under scrutiny.For example, the selection of a void+:ResourceSelection for a void+:MatchingMethod.\" @ en . ### http://lenticularlens.org/voidPlus/hasSubjectResourceSelection void+ : hasSubjectResourceSelection rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasResourceSelection ; rdfs: comment \"a relation to assign a void+:ResourceSelection as the source for the subjects of triples under scrutiny.\" @ en . ### http://lenticularlens.org/voidPlus/hasSubset void+ : hasSubset rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void: subset ; owl: inverseOf void+ : subsetOf ; rdfs: domain void: Dataset ; rdfs: range void+ : Partition ; rdfs: comment \"relates a void:Dataset that has as subset a void+:Partition (dataset). In practice, the main difference wrt void:subset is the name meant to avoid miss-usage as it can be read as \\\"is subset of\\\" as well. For a similar purpose, an inverse relation void+:subsetOf is also defined.\" @ en . ### http://lenticularlens.org/voidPlus/hasTarget void+ : hasTarget rdf: type owl: ObjectProperty ; rdfs: domain void+ : LinkDataset ; rdfs: range void: Dataset ; rdfs: comment \"A relation that assigns one or more void:Datasets linked via a void+:LinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/hasValidation void+ : hasValidation rdf: type owl: ObjectProperty ; rdfs: domain void+ : LinkDataset ; rdfs: range void+ : Validation ; rdfs: comment \"A relation that assigns one or more void+:Validations to a void+:LinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/language void+ : language rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : partitionedBy ; rdfs: range void+ : Language ; rdfs: comment \"A relation that assigns to a void:Dataset a void+:Language (for ISO standard language codes) that is the rdf:language of all literals (objects) qualifying entities (subject) in a language-based partition.\" @ en . ### http://lenticularlens.org/voidPlus/linkPredicate void+ : linkPredicate rdf: type owl: ObjectProperty ; rdfs: subPropertyOf owl: topObjectProperty ; rdfs: comment \"a link predicate that holds for a void+:LinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/objectsTarget void+ : objectsTarget rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasTarget ; rdfs: domain void+ : DirectedLinkDataset ; rdfs: range void: Dataset ; rdfs: comment \"A relation that assigns void:Datasets describing the object of triples contained in the void+:DirectedLinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/partitionedBy void+ : partitionedBy rdf: type owl: ObjectProperty ; rdfs: domain void: Dataset ; rdfs: comment \"A relation that assigns a void:Dataset to the conditions under which it is partitioned.\" @ en . ### http://lenticularlens.org/voidPlus/property void+ : property rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : partitionedBy ; rdfs: range [ rdf: type owl: Class ; owl: unionOf ( rdf: Property rdf: Seq ) ] ; rdfs: comment \"A relation that assigns to a void:Dataset a rdf:Property or a rdf:Seq of properties that expresses the pattern of all triples in a property-based partition.\" @ en . ### http://lenticularlens.org/voidPlus/subjectsTarget void+ : subjectsTarget rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasTarget ; rdfs: domain void+ : DirectedLinkDataset ; rdfs: range void: Dataset ; rdfs: comment \"A relation that assigns void:Datasets describing the subjects of triples contained in the void+:DirectedLinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/subsetOf void+ : subsetOf rdf: type owl: ObjectProperty ; rdfs: comment \"relates a void+:Partition to its void:Dataset superset. Defined as the inverse relation of void+:hasSubset.\" @ en . ### http://purl.org/dc/terms/creator dcterms: creator rdf: type owl: ObjectProperty ; rdfs: comment \"The range for dcterms:creator is dcterms:Agent. All values used with this property have to be instances of the class [dcterms:Agent] . <span style=\\\"color: red\\\">must not be used with literal values</span>. You may use it only with non-literal values.\" @ en . ### http://purl.org/dc/terms/publisher dcterms: publisher rdf: type owl: ObjectProperty ; rdfs: range dcterms: Agent ; rdfs: comment \"The range for dcterms:publisher is dcterms:Agent. All values used with this property have to be instances of the class [dcterms:Agent] . <span style=\\\"color: red\\\">must not be used with literal values</span>. You may use it only with non-literal values.\" @ en . ### http://rdfs.org/ns/void#linkPredicate void: linkPredicate rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : linkPredicate ; rdfs: domain void: Linkset ; rdfs: range rdf: Property ; rdfs: comment \"a link predicate that holds for a void:Linkset.\" @ en . ### http://rdfs.org/ns/void#class void: class rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : partitionedBy ; rdfs: range rdfs: Class ; rdfs: comment \"A relation that assigns to a void:Dataset a rdfs:Class that is the rdf:type of all entities (subject) in a class-based partition.\" @ en . ### http://rdfs.org/ns/void#classPartition void: classPartition rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void: subset ; rdfs: comment \"A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only entities of an explicitly defined rdfs:Class.\" @ en . ### http://rdfs.org/ns/void#feature void: feature rdf: type owl: ObjectProperty ; rdfs: domain void: Dataset ; rdfs: range void: TechnicalFeature ; rdfs: comment \"relates a void:TechnicalFeature supported by a void:Datset.\" @ en . ### http://rdfs.org/ns/void#objectsTarget void: objectsTarget rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : objectsTarget , void: target ; rdfs: domain void: Linkset ; rdfs: comment \"A relation that assigns the void:Dataset describing the objects of triples contained in the void:Linkset.\" @ en . ### http://rdfs.org/ns/void#property void: property rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : property ; rdfs: domain void: Dataset ; rdfs: range rdf: Property ; rdfs: comment \"A relation that assigns to a void:Dataset a rdf:Property that is the predicate of all triples in a property-based partition.\" @ en . ### http://rdfs.org/ns/void#propertyPartition void: propertyPartition rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void: subset ; rdfs: comment \"A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only triples composed with an explicitly defined rdf:Property.\" @ en . ### http://rdfs.org/ns/void#subjectsTarget void: subjectsTarget rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : subjectsTarget , void: target ; rdfs: domain void: Linkset ; rdfs: comment \"A relation that assigns the void:Dataset describing the subjects of triples contained in the void:Linkset.\" @ en . ### http://rdfs.org/ns/void#subset void: subset rdf: type owl: ObjectProperty ; rdfs: domain void: Dataset ; rdfs: range void: Dataset ; rdfs: comment \"relates a void:Dataset that has subset another void:Dataset.\" @ en . ### http://rdfs.org/ns/void#target void: target rdf: type owl: ObjectProperty ; rdfs: subPropertyOf void+ : hasTarget ; rdfs: domain void: Linkset ; rdfs: range void: Dataset ; rdfs: comment \"A relation that assigns one of the two void:Datasets linked by the void:Linkset.\" @ en . ### http://www.w3.org/2000/01/rdf-schema#member rdfs: member rdf: type owl: ObjectProperty ; rdfs: comment \"rdfs:member is an instance of rdf:Property that is a super-property of all the container membership properties. In particular, properties called rdf:_1, rdf:_2, rdf:_3... etc., used for rdf:Seq are sub-properties of rdfs:member.\" @ en . ################################################################# # Data properties ################################################################# ### http://creativecommons.org/ns#attributionName <http://creativecommons.org/ns#attributionName> rdf: type owl: DatatypeProperty ; rdfs: comment \"The name the creator of a Work would like used when attributing re-use.\" @ en . ### http://creativecommons.org/ns#license <http://creativecommons.org/ns#license> rdf: type owl: DatatypeProperty ; rdfs: comment \"A work has license a License.\" @ en . ### http://lenticularlens.org/voidPlus/hasAccepted void+ : hasAccepted rdf: type owl: DatatypeProperty ; rdfs: domain [ rdf: type owl: Class ; owl: unionOf ( void+ : LinkDataset void+ : Validation ) ] ; rdfs: range xsd: integer ; rdfs: comment \"The amount of accepted links for a void+:LinkDataset or void+:Validation.\" @ en . ### http://lenticularlens.org/voidPlus/hasClusters void+ : hasClusters rdf: type owl: DatatypeProperty ; rdfs: domain void+ : LinkDataset ; rdfs: range xsd: integer ; rdfs: comment \"The amount of clusters computed for a void+:LinkDataset.\" @ en . ### http://lenticularlens.org/voidPlus/hasContradictions void+ : hasContradictions rdf: type owl: DatatypeProperty ; rdfs: domain void+ : LinkDataset ; rdfs: range xsd: integer ; rdfs: comment \"The amount of links of a void+:LinkDataset for which contradicting validations are found.\" @ en . ### http://lenticularlens.org/voidPlus/hasFilterFunction void+ : hasFilterFunction rdf: type owl: DatatypeProperty ; rdfs: domain void+ : PropertyPartition ; rdfs: comment \"It relates a property partition to a filter function, such as \\\"contains\\\" or \\\"minimal_date\\\". When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \\\"function\\\", a \\\"value\\\" and a \\\"format\\\".\" @ en . ### http://lenticularlens.org/voidPlus/hasFormatFunction void+ : hasFormatFunction rdf: type owl: DatatypeProperty ; rdfs: domain void+ : PropertyPartition ; rdfs: comment \"It relates a property partition to a format restriction which is applicable to the value used by an explicitly defined filter function, such as \\\"YYYY-MM-DD\\\" for filter \\\"minimal_date\\\". When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \\\"function\\\", a \\\"value\\\" and a \\\"format\\\".\" @ en . ### http://lenticularlens.org/voidPlus/hasFormulaDescription void+ : hasFormulaDescription rdf: type owl: DatatypeProperty ; rdfs: domain void+ : Formulation ; rdfs: range xsd: string ; rdfs: comment \"Describes how items listed in a formulation are combined. For example, using logic or set-like operators.\" @ en . ### http://lenticularlens.org/voidPlus/hasRejected void+ : hasRejected rdf: type owl: DatatypeProperty ; rdfs: domain [ rdf: type owl: Class ; owl: unionOf ( void+ : LinkDataset void+ : Validation ) ] ; rdfs: range xsd: integer ; rdfs: comment \"The amount of rejected links for a void+:LinkDataset or void+:Validation.\" @ en . ### http://lenticularlens.org/voidPlus/hasThreshold void+ : hasThreshold rdf: type owl: DatatypeProperty ; rdfs: domain void+ : MatchingMethod ; rdfs: comment \"Relates a void+:MatchingMethod to a link-acceptance-threshold value. Upon the choice of a void+:MatchingAlgorithm, the user should provide the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \\\"Year\\\"), a threshold operator (e.g. \\\">=\\\") and a threshold range (e.g. \\\"]0,1]\\\" or \\\"\u2115\\\").\" @ en . ### http://lenticularlens.org/voidPlus/hasThresholdAcceptanceOperator void+ : hasThresholdAcceptanceOperator rdf: type owl: DatatypeProperty ; rdfs: domain void+ : MatchingMethod ; rdfs: comment \"Relates a void+:MatchingMethod to a link-acceptance-threshold operator. Upon the choice of a void+:MatchingAlgorithm, the user should inform the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \\\"Year\\\"), a threshold operator (e.g. \\\">=\\\") and a threshold range (e.g. \\\"]0,1]\\\" or \\\"\u2115\\\").\" @ en . ### http://lenticularlens.org/voidPlus/hasThresholdRange void+ : hasThresholdRange rdf: type owl: DatatypeProperty ; rdfs: domain void+ : MatchingMethod ; rdfs: comment \"Relates a void+:MatchingMethod to a link-acceptance-threshold range. Upon the choice of a void+:MatchingAlgorithm, the user should inform the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \\\"Year\\\"), a threshold operator (e.g. \\\">=\\\") and a threshold range (e.g. \\\"]0,1]\\\" or \\\"\u2115\\\").\" @ en . ### http://lenticularlens.org/voidPlus/hasValidations void+ : hasValidations rdf: type owl: DatatypeProperty ; rdfs: domain [ rdf: type owl: Class ; owl: unionOf ( void+ : LinkDataset void+ : Validation ) ] ; rdfs: range xsd: integer ; rdfs: comment \"The amount of validated links in a void+:LinkDataset or void+:Validation.\" @ en . ### http://lenticularlens.org/voidPlus/hasValueFunction void+ : hasValueFunction rdf: type owl: DatatypeProperty ; rdfs: domain void+ : PropertyPartition ; rdfs: comment \"It relates a property partition to a value for a filter function, such as \\\"%van%\\\" for filter \\\"contains\\\" or \\\"01/01/1600\\\" for filter \\\"minimal_date\\\". When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \\\"function\\\", a \\\"value\\\" and a \\\"format\\\".\" @ en . ### http://lenticularlens.org/voidPlus/toBeValidated void+ : toBeValidated rdf: type owl: DatatypeProperty ; rdfs: domain [ rdf: type owl: Class ; owl: unionOf ( void+ : LinkDataset void+ : Validation ) ] ; rdfs: range xsd: integer ; rdfs: comment \"The amount of links yet to be validated in a void+:LinkDataset or void+:Validation.\" @ en . ### http://purl.org/dc/terms/created dcterms: created rdf: type owl: DatatypeProperty ; rdfs: range rdfs: Literal ; rdfs: comment \"The range defined for dcterms:created is the class of rdfs:Literal. Values used with this property therefore have to be literal values.\" @ en . ### http://purl.org/dc/terms/description dcterms: description rdf: type owl: DatatypeProperty ; rdfs: comment \"There is no range defined for dcterms:description. Therefore you can use it with literal values.\" @ en . ### http://rdfs.org/ns/void#distinctObjects void: distinctObjects rdf: type owl: DatatypeProperty ; rdfs: domain void: Dataset ; rdfs: range xsd: integer ; rdfs: comment \"The total number of distinct objects in a void:Dataset. In other words, the number of distinct resources that occur in the object position of triples in the dataset. Literals are included in this count.\" . ### http://rdfs.org/ns/void#distinctSubjects void: distinctSubjects rdf: type owl: DatatypeProperty ; rdfs: domain void: Dataset ; rdfs: range xsd: integer ; rdfs: comment \"The total number of distinct subjects in a void:Dataset. In other words, the number of distinct resources that occur in the subject position of triples in the dataset.\" . ### http://rdfs.org/ns/void#entities void: entities rdf: type owl: DatatypeProperty ; rdfs: domain void: Dataset ; rdfs: range xsd: integer ; rdfs: comment \"The total number of entities that are described in a void:Dataset.\" . ### http://rdfs.org/ns/void#triples void: triples rdf: type owl: DatatypeProperty ; rdfs: domain void: Dataset ; rdfs: range xsd: integer ; rdfs: comment \"The total number of triples contained in a void:Dataset.\" @ en . ### http://www.w3.org/2000/01/rdf-schema#label rdfs: label rdf: type owl: DatatypeProperty . ################################################################# # Classes ################################################################# ### http://lenticularlens.org/voidPlus/ClassPartition void+ : ClassPartition rdf: type owl: Class ; owl: equivalentClass [ rdf: type owl: Restriction ; owl: onProperty [ owl: inverseOf void+ : hasClassPartition ] ; owl: someValuesFrom void: Dataset ] , [ rdf: type owl: Restriction ; owl: onProperty void: class ; owl: qualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onClass rdfs: Class ] ; rdfs: subClassOf void+ : Partition ; rdfs: comment \"A void+:Partition that is a collection of resources partitioned on the basis of an rdfs:Class.\" @ en . ### http://lenticularlens.org/voidPlus/DirectedLinkDataset void+ : DirectedLinkDataset rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( [ rdf: type owl: Restriction ; owl: onProperty void+ : objectsTarget ; owl: someValuesFrom void: Dataset ] [ rdf: type owl: Restriction ; owl: onProperty void+ : subjectsTarget ; owl: someValuesFrom void: Dataset ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void+ : LinkDataset , [ rdf: type owl: Restriction ; owl: onProperty void: distinctObjects ; owl: qualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onDataRange xsd: integer ] , [ rdf: type owl: Restriction ; owl: onProperty void: distinctSubjects ; owl: qualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onDataRange xsd: integer ] ; rdfs: comment \"A void+:LinkDataset that imposes that subjects always belong to the same datasets as well as objects.\" @ en . ### http://lenticularlens.org/voidPlus/DocumentedLinkDataset void+ : DocumentedLinkDataset rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( void+ : LinkDataset [ rdf: type owl: Restriction ; owl: onProperty void+ : hasFormulation ; owl: someValuesFrom void+ : Formulation ] [ rdf: type owl: Restriction ; owl: onProperty void+ : hasTarget ; owl: someValuesFrom void+ : ResourceSelection ] [ rdf: type owl: Restriction ; owl: onProperty void+ : hasTarget ; owl: allValuesFrom void+ : ResourceSelection ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void+ : LinkDataset ; rdfs: comment \"A void+:LinkDataset that requires detailed documentation, namely it imposes that targets are described as void+:ResourceSelection and requires a void+:Formulation.\" @ en . ### http://lenticularlens.org/voidPlus/Formulation void+ : Formulation rdf: type owl: Class ; rdfs: comment \"A (reusable) formulation that lists a number of items (e.g. matching methods or linksets) and describes how they are meant to be combined (e.g. using logic or set operators).\" @ en . ### http://lenticularlens.org/voidPlus/IntegrationView void+ : IntegrationView rdf: type owl: Class ; rdfs: subClassOf void: Dataset , [ owl: intersectionOf ( [ rdf: type owl: Restriction ; owl: onProperty void+ : hasSubset ; owl: minQualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onClass void+ : LinkDataset ] [ rdf: type owl: Restriction ; owl: onProperty void+ : hasSubset ; owl: minQualifiedCardinality \"2\" ^^ xsd:nonNegativeInteger ; owl: onClass void+ : ResourceSelection ] ) ; rdf: type owl: Class ] ; rdfs: comment \"A set of RDF triples that are stemmed from two or more void+:ResourceSelections that are linked by at least one void+:LinkDataset. One may add void+:PropertyPartitions for both filtering or for data visualisation and analysis.\" @ en . ### http://lenticularlens.org/voidPlus/Language void+ : Language rdf: type owl: Class ; rdfs: comment \"A set of standardised language (ISO 639 not 3166) codes. For example, use nl, nld or dut to indicate the Dutch or Flemish languages.\" @ en . ### http://lenticularlens.org/voidPlus/LanguagePartition void+ : LanguagePartition rdf: type owl: Class ; owl: equivalentClass [ rdf: type owl: Restriction ; owl: onProperty void+ : language ; owl: someValuesFrom void+ : Language ] , [ rdf: type owl: Restriction ; owl: onProperty [ owl: inverseOf void+ : hasLanguagePartition ] ; owl: someValuesFrom void: Dataset ] ; rdfs: subClassOf void+ : Partition ; rdfs: comment \"A void+:Partition that is a collection of resources based on a void+:Language restriction, i.e. an ISO 639 standardised language code.\" @ en . ### http://lenticularlens.org/voidPlus/Lens void+ : Lens rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( void+ : DocumentedLinkDataset [ rdf: type owl: Restriction ; owl: onProperty void+ : hasFormulation ; owl: someValuesFrom void+ : LensFormulation ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void+ : DocumentedLinkDataset ; rdfs: comment \"Similar to a void+:Linkset (in the sense that it is a collection of links sharing the same linktype) also involving one, two or more datasets, but not necessarilly directed. Moreover, context-wise, it differs from a linkset as it is generated using different processes (set-like link manipulation operators such as Union, Intersection, Difference or Transitivity) as compared to how a linkset comes about (matching algorithms).\" @ en . ### http://lenticularlens.org/voidPlus/LensFormulation void+ : LensFormulation rdf: type owl: Class ; rdfs: subClassOf void+ : Formulation , [ rdf: type owl: Restriction ; owl: onProperty void+ : hasItem ; owl: minQualifiedCardinality \"2\" ^^ xsd:nonNegativeInteger ; owl: onClass void+ : LinkDataset ] ; rdfs: comment \"A resource that makes explicit all void+:LinkDatasets involved in the creation of a void+:Lens and how the they are combined.\" @ en . ### http://lenticularlens.org/voidPlus/LinkDataset void+ : LinkDataset rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( [ rdf: type owl: Restriction ; owl: onProperty void+ : hasTarget ; owl: someValuesFrom void: Dataset ] [ rdf: type owl: Restriction ; owl: onProperty void+ : linkPredicate ; owl: someValuesFrom rdf: Property ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void: Dataset , [ rdf: type owl: Restriction ; owl: onProperty dcterms: description ; owl: someValuesFrom rdfs: Literal ] ; rdfs: comment \"A collection of RDF links between dataset(s), using the same link predicate (regardless of the link being an equality predicate or not). This predicate is called the linktype of the linkset. The links may have been stabilished among one (deduplication), two or many datasets.\" @ en . ### http://lenticularlens.org/voidPlus/Linkset void+ : Linkset rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( void+ : DirectedLinkDataset [ rdf: type owl: Restriction ; owl: onProperty void+ : hasFormulation ; owl: someValuesFrom void+ : LinksetFormulation ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void+ : DirectedLinkDataset , void+ : DocumentedLinkDataset ; rdfs: comment \"A collection of RDF links between <em>one</em>, <em>two</em> or <em>more</em> void:Datasets. Only, all links share the same link-predicate and all subjects stem from a set of void datasets (D<sub>1</sub>) while all objects stem from either the same D<sub>1</sub> or a different set of void datasets D<sub>2</sub>.\" @ en . ### http://lenticularlens.org/voidPlus/LinksetFormulation void+ : LinksetFormulation rdf: type owl: Class ; rdfs: subClassOf void+ : Formulation , [ rdf: type owl: Restriction ; owl: onProperty void+ : hasItem ; owl: someValuesFrom void+ : MatchingMethod ] ; rdfs: comment \"A resource that makes explicit all void+:MatchingMethods involved in the creation of a void+:Linkset and how the methods are logically joint.\" @ en . ### http://lenticularlens.org/voidPlus/MatchingAlgorithm void+ : MatchingAlgorithm rdf: type owl: Class ; rdfs: comment \"A set of rules followed by a computer for finding pairs of matching resources. An algorithm can be further categorised as automated or semi-automated depending on whether it requires user assistance or not. Most matching algorithms employed in the Lenticular Lens are semi-automated.\" @ en . ### http://lenticularlens.org/voidPlus/MatchingMethod void+ : MatchingMethod rdf: type owl: Class ; owl: equivalentClass [ rdf: type owl: Restriction ; owl: onProperty void+ : hasAlgorithm ; owl: someValuesFrom void+ : MatchingAlgorithm ] ; rdfs: comment \"A resource that makes explicit all parameters/pre-requisites (datasets, entity-type and property-value restrictions, matching properties...) of a matching algorithm including the conditions in which the algorithm is to accept a discovered link (threshold).\" @ en . ### http://lenticularlens.org/voidPlus/Partition void+ : Partition rdf: type owl: Class ; owl: equivalentClass [ rdf: type owl: Restriction ; owl: onProperty void: subset ; owl: someValuesFrom void: Dataset ] ; rdfs: subClassOf void: Dataset ; rdfs: comment \"A void:Daset that is subset of another void:Daset.\" @ en . ### http://lenticularlens.org/voidPlus/PartitionFormulation void+ : PartitionFormulation rdf: type owl: Class ; rdfs: subClassOf void+ : Formulation , [ rdf: type owl: Restriction ; owl: onProperty void+ : hasItem ; owl: someValuesFrom void+ : Partition ] . ### http://lenticularlens.org/voidPlus/PropertyPartition void+ : PropertyPartition rdf: type owl: Class ; owl: equivalentClass [ rdf: type owl: Restriction ; owl: onProperty [ owl: inverseOf void+ : hasPropertyPartition ] ; owl: someValuesFrom void: Dataset ] , [ rdf: type owl: Restriction ; owl: onProperty void+ : property ; owl: qualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onClass [ rdf: type owl: Class ; owl: unionOf ( rdf: Property rdf: Seq ) ] ] ; rdfs: subClassOf void+ : Partition ; rdfs: comment \"A void+:Partition that is is a collection of resources described using either a rdf:Property or sequence of them (void+:PropertySequence or void+:TypedPropertySequence).\" @ en . ### http://lenticularlens.org/voidPlus/PropertySequence void+ : PropertySequence rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( [ rdf: type owl: Restriction ; owl: onProperty rdfs: member ; owl: allValuesFrom rdf: Property ] [ rdf: type owl: Restriction ; owl: onProperty rdfs: member ; owl: minQualifiedCardinality \"2\" ^^ xsd:nonNegativeInteger ; owl: onClass rdf: Property ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf rdf: Seq ; rdfs: comment \"A resource that represents a sequence of properties, analogously to a property path.\" @ en . ### http://lenticularlens.org/voidPlus/ResourceSelection void+ : ResourceSelection rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( void+ : Partition [ rdf: type owl: Restriction ; owl: onProperty void+ : hasFormulation ; owl: someValuesFrom void+ : PartitionFormulation ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf void: Dataset , [ rdf: type owl: Restriction ; owl: onProperty void+ : hasSubset ; owl: someValuesFrom void+ : Partition ] , [ rdf: type owl: Restriction ; owl: onProperty void: subset ; owl: someValuesFrom void: Dataset ] , [ rdf: type owl: Restriction ; owl: onProperty dcterms: description ; owl: someValuesFrom rdfs: Literal ] ; rdfs: comment \"A collection of resources stemmed from the same void:Dataset. It is expected to have a void+:ClassPartition as partition, directly or indirectly (i.e. as a subset). It can also have void+:PropertyPartitions, where property-value(s) can be restricted, as well as void+:LanguagePartitions, where the language of the property-value(s) can be restricted.\" @ en . ### http://lenticularlens.org/voidPlus/TypedPropertySequence void+ : TypedPropertySequence rdf: type owl: Class ; owl: equivalentClass [ owl: intersectionOf ( [ rdf: type owl: Restriction ; owl: onProperty rdfs: member ; owl: allValuesFrom [ rdf: type owl: Class ; owl: unionOf ( rdf: Property rdfs: Class ) ] ] [ rdf: type owl: Restriction ; owl: onProperty rdfs: member ; owl: minQualifiedCardinality \"1\" ^^ xsd:nonNegativeInteger ; owl: onClass rdfs: Class ] [ rdf: type owl: Restriction ; owl: onProperty rdfs: member ; owl: minQualifiedCardinality \"2\" ^^ xsd:nonNegativeInteger ; owl: onClass rdf: Property ] ) ; rdf: type owl: Class ] ; rdfs: subClassOf rdf: Seq ; rdfs: comment \"\"\"A resource that represents a sequence of properties and classes, as in a different type of property path with class restriction in between: property -> class -> property -> class -> property\"\"\"@en . ### http://lenticularlens.org/voidPlus/ValidatedLinkDataset void+:ValidatedLinkDataset rdf:type owl:Class ; owl:equivalentClass [ owl:intersectionOf ( void+:LinkDataset [ rdf:type owl:Restriction ; owl:onProperty void+:hasValidation ; owl:someValuesFrom void+:Validation ] ) ; rdf:type owl:Class ] ; rdfs:subClassOf void+:LinkDataset , [ rdf:type owl:Restriction ; owl:onProperty void+:hasContradictions ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] , [ rdf:type owl:Restriction ; owl:onProperty void+:hasValidations ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] , [ rdf:type owl:Restriction ; owl:onProperty void+:toBeValidated ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] ; rdfs:comment \"A voidPlus:LinkDataset that has been validated.\"@en . ### http://lenticularlens.org/voidPlus/Validation void+:Validation rdf:type owl:Class ; owl:equivalentClass [ rdf:type owl:Restriction ; owl:onProperty [ owl:inverseOf void+:hasValidation ] ; owl:someValuesFrom void+:LinkDataset ] ; rdfs:subClassOf void:Dataset , [ rdf:type owl:Restriction ; owl:onProperty void+:hasAccepted ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] , [ rdf:type owl:Restriction ; owl:onProperty void+:hasRejected ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] , [ rdf:type owl:Restriction ; owl:onProperty void+:hasValidations ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] , [ rdf:type owl:Restriction ; owl:onProperty void+:toBeValidated ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onDataRange xsd:integer ] ; rdfs:comment \"A void:Dataset that contains triples describing the validaty of links stemmed from a void+:LinkDataset.\"@en . ### http://purl.org/dc/terms/Agent dcterms:Agent rdf:type owl:Class ; rdfs:comment \"A resource that acts or has the power to act.\"@en . ### http://rdfs.org/ns/void#Dataset void:Dataset rdf:type owl:Class ; rdfs:comment \"A set of RDF triples that are published, maintained or aggregated by a single provider.\"@en . ### http://rdfs.org/ns/void#Linkset void:Linkset rdf:type owl:Class ; rdfs:subClassOf void+:DirectedLinkDataset , [ owl:intersectionOf ( [ rdf:type owl:Restriction ; owl:onProperty void:objectsTarget ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onClass void:Dataset ] [ rdf:type owl:Restriction ; owl:onProperty void:subjectsTarget ; owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ; owl:onClass void:Dataset ] ) ; rdf:type owl:Class ] ; rdfs:comment \"A collection of RDF links between *two* void:Datasets.\"@en . ### http://rdfs.org/ns/void#TechnicalFeature void:TechnicalFeature rdf:type owl:Class ; rdfs:comment \"A technical feature of a void:Dataset, such as a supported RDF serialization format.\"@en . ### http://www.w3.org/1999/02/22-rdf-syntax-ns#Property rdf:Property rdf:type owl:Class ; rdfs:comment \"The class of RDF properties.\"@en . ### http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq rdf:Seq rdf:type owl:Class ; rdfs:comment \"The class of RDF 'Sequence' containers.\"@en . ### http://www.w3.org/2000/01/rdf-schema#Class rdfs:Class rdf:type owl:Class ; rdfs:comment \"This is the class of resources that are RDF classes.\"@en . ### Generated by the OWL API (version 4.5.9.2019-02-01T07:24:44Z) https://github.com/owlcs/owlapi","title":"3.5 VoID+ OWL"},{"location":"03.Ontology/#4-void-documentation","text":"Markdown documentation created by pyLODE 2.8.3 Ontology URI : http://lenticularlens.org/voidPlus Ontology RDF : download voidPlus.owl","title":"4. VoID+ Documentation"},{"location":"03.Ontology/#41-overview","text":"See Figure 7 and 8 above for an overview.","title":"4.1 Overview"},{"location":"03.Ontology/#namespaces","text":"BASE <http://lenticularlens.org/voidPlus/> PREFIX cc : <https://creativecommons.org/ns#> PREFIX dcterms : <http://purl.org/dc/terms/> PREFIX owl : <http://www.w3.org/2002/07/owl#> PREFIX prov : <http://www.w3.org/ns/prov#> PREFIX rdf : <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX rdfs : <http://www.w3.org/2000/01/rdf-schema#> PREFIX sdo : <https://schema.org/> PREFIX skos : <http://www.w3.org/2004/02/skos/core#> PREFIX void : <http://rdfs.org/ns/void#> PREFIX void+ : <http://lenticularlens.org/voidPlus/> PREFIX xsd : <http://www.w3.org/2001/XMLSchema#>","title":"Namespaces"},{"location":"03.Ontology/#42-classes","text":"","title":"4.2 Classes"},{"location":"03.Ontology/#voiddataset","text":"Property Value URI http://rdfs.org/ns/void#Dataset Description A set of RDF triples that are published, maintained or aggregated by a single provider. Sub-classes void+:ResourceSelection void+:LinkDataset void+:Validation void+:Partition void+:IntegrationView In domain of void:entities void:triples void:distinctSubjects void+:partitionedBy void+:hasSubset void:property void:feature void:distinctObjects void+:hasFormulation void:subset In range of void:target void+:subjectsTarget void+:hasTarget void:subset void+:objectsTarget","title":"void:Dataset"},{"location":"03.Ontology/#linkset","text":"Property Value URI http://lenticularlens.org/voidPlus/Linkset Description A collection of RDF links between one , two or more void:Datasets. Only, all links share the same link-predicate and all subjects stem from a set of void datasets (D 1 ) while all objects stem from either the same D 1 or a different set of void datasets D 2 . Super-classes void+:DocumentedLinkDataset void+:DirectedLinkDataset Equivalent to void+:DirectedLinkDataset and ( void+:hasFormulation some void+:LinksetFormulation )","title":"Linkset"},{"location":"03.Ontology/#voidlinkset","text":"Property Value URI http://rdfs.org/ns/void#Linkset Description A collection of RDF links between two void:Datasets. Super-classes void+:DirectedLinkDataset ( void:objectsTarget exactly 1 void:Dataset ) and ( void:subjectsTarget exactly 1 void:Dataset ) In domain of void:linkPredicate void:target void:objectsTarget void:subjectsTarget","title":"void:Linkset"},{"location":"03.Ontology/#lens","text":"Property Value URI http://lenticularlens.org/voidPlus/Lens Description Similar to a void+:Linkset (in the sense that it is a collection of links sharing the same linktype) also involving one, two or more datasets, but not necessarilly directed. Moreover, context-wise, it differs from a linkset as it is generated using different processes (set-like link manipulation operators such as Union, Intersection, Difference or Transitivity) as compared to how a linkset comes about (matching algorithms). Super-classes void+:DocumentedLinkDataset Equivalent to void+:DocumentedLinkDataset and ( void+:hasFormulation some void+:LensFormulation )","title":"Lens"},{"location":"03.Ontology/#partition","text":"Property Value URI http://lenticularlens.org/voidPlus/Partition Description A void:Daset that is subset of another void:Daset. Super-classes void:Dataset Equivalent to void:subset some void:Dataset Sub-classes void+:PropertyPartition void+:LanguagePartition void+:ClassPartition In range of void+:hasSubset","title":"Partition"},{"location":"03.Ontology/#classpartition","text":"Property Value URI http://lenticularlens.org/voidPlus/ClassPartition Description A void+:Partition that is a collection of resources partitioned on the basis of an rdfs:Class. Super-classes void+:Partition Equivalent to void:class exactly 1 rdfs:Class inverse ( void+:hasClassPartition ) some void:Dataset","title":"ClassPartition"},{"location":"03.Ontology/#propertypartition","text":"Property Value URI http://lenticularlens.org/voidPlus/PropertyPartition Description A void+:Partition that is is a collection of resources described using either a rdf:Property or sequence of them (void+:PropertySequence or void+:TypedPropertySequence). Super-classes void+:Partition Equivalent to void:property exactly 1 rdf:Property inverse ( void+:has Property Partition ) some void:Dataset In domain of void+:hasValueFunction void+:hasFilterFunction void+:hasFormatFunction","title":"PropertyPartition"},{"location":"03.Ontology/#languagepartition","text":"Property Value URI http://lenticularlens.org/voidPlus/LanguagePartition Description A void+:Partition that is a collection of resources based on a void+:Language restriction, i.e. an ISO 639 standardised language code. Super-classes void+:Partition Equivalent to void+:language some void+:Language inverse ( void+:hasLanguagePartition ) some void:Dataset","title":"LanguagePartition"},{"location":"03.Ontology/#resourceselection","text":"Property Value URI http://lenticularlens.org/voidPlus/ResourceSelection Description A collection of resources stemmed from the same void:Dataset. It is expected to have a void+:ClassPartition as partition, directly or indirectly (i.e. as a subset). It can also have void+:PropertyPartitions, where property-value(s) can be restricted, as well as void+:LanguagePartitions, where the language of the property-value(s) can be restricted. Super-classes void:Dataset void:subset some void:Dataset void+:hasSubset some void+:Partition dcterms:description some rdfs:Literal Equivalent to void+:Partition and ( void+:hasFormulation some void+:PartitionFormulation ) In range of void+:hasResourceSelection","title":"ResourceSelection"},{"location":"03.Ontology/#linkdataset","text":"Property Value URI http://lenticularlens.org/voidPlus/LinkDataset Description A collection of RDF links between dataset(s), using the same link predicate (regardless of the link being an equality predicate or not). This predicate is called the linktype of the linkset. The links may have been stabilished among one (deduplication), two or many datasets. Super-classes void:Dataset dcterms:description some rdfs:Literal Equivalent to ( void+:linkPredicate some rdf:Property ) and ( void+:hasTarget some void:Dataset ) Sub-classes void+:ValidatedLinkDataset void+:DocumentedLinkDataset void+:DirectedLinkDataset In domain of void+:hasValidation void+:hasClusters void+:hasContradictions void+:hasTarget","title":"LinkDataset"},{"location":"03.Ontology/#directedlinkdataset","text":"Property Value URI http://lenticularlens.org/voidPlus/DirectedLinkDataset Description A void+:LinkDataset that imposes that subjects always belong to the same datasets as well as objects. Super-classes void+:LinkDataset void:distinctObjects exactly 1 void:distinctSubjects exactly 1 Sub-classes void+:Linkset void:Linkset In domain of void+:objectsTarget void+:subjectsTarget","title":"DirectedLinkDataset"},{"location":"03.Ontology/#documentedlinkdataset","text":"Property Value URI http://lenticularlens.org/voidPlus/DocumentedLinkDataset Description A void+:LinkDataset that requires detailed documentation, namely it imposes that targets are described as void+:ResourceSelection and requires a void+:Formulation. Super-classes void+:LinkDataset Equivalent to void+:LinkDataset and ( void+:hasFormulation some void+:Formulation ) and ( void+:hasTarget some void+:ResourceSelection ) and ( void+:hasTarget only void+:ResourceSelection ) Sub-classes void+:Lens void+:Linkset","title":"DocumentedLinkDataset"},{"location":"03.Ontology/#validatedlinkdataset","text":"Property Value URI http://lenticularlens.org/voidPlus/ValidatedLinkDataset Description A void+:LinkDataset that has been validated. Super-classes void+:LinkDataset void+:hasContradictions exactly 1 void+:hasValidations exactly 1 void+:toBeValidated exactly 1 Equivalent to void+:LinkDataset and ( void+:hasValidation some void+:Validation )","title":"ValidatedLinkDataset"},{"location":"03.Ontology/#formulation","text":"Property Value URI http://lenticularlens.org/voidPlus/Formulation Description A (reusable) formulation that lists a number of items (e.g. matching methods or linksets) and describes how they are meant to be combined (e.g. using logic or set operators). Sub-classes void+:PartitionFormulation void+:LensFormulation void+:LinksetFormulation In domain of void+:hasItem void+:hasFormulaDescription In range of void+:hasFormulation","title":"Formulation"},{"location":"03.Ontology/#lensformulation","text":"Property Value URI http://lenticularlens.org/voidPlus/LensFormulation Description A resource that makes explicit all void+:LinkDatasets involved in the creation of a void+:Lens and how the they are combined. Super-classes void+:Formulation void+:hasItem min 2 void+:LinkDataset","title":"LensFormulation"},{"location":"03.Ontology/#linksetformulation","text":"Property Value URI http://lenticularlens.org/voidPlus/LinksetFormulation Description A resource that makes explicit all void+:MatchingMethods involved in the creation of a void+:Linkset and how the methods are logically joint. Super-classes void+:Formulation void+:hasItem some void+:MatchingMethod","title":"LinksetFormulation"},{"location":"03.Ontology/#partitionformulation","text":"Property Value URI http://lenticularlens.org/voidPlus/PartitionFormulation Super-classes void+:Formulation void+:hasItem some void+:Partition","title":"PartitionFormulation"},{"location":"03.Ontology/#integrationview","text":"Property Value URI http://lenticularlens.org/voidPlus/IntegrationView Description A set of RDF triples that are stemmed from two or more void+:ResourceSelections that are linked by at least one void+:LinkDataset. One may add void+:PropertyPartitions for both filtering or for data visualisation and analysis. Super-classes void:Dataset ( void+:hasSubset min 2 void+:ResourceSelection ) and ( void+:hasSubset min 1 void+:LinkDataset )","title":"IntegrationView"},{"location":"03.Ontology/#language","text":"Property Value URI http://lenticularlens.org/voidPlus/Language Description A set of standardised language (ISO 639 not 3166) codes. For example, use nl, nld or dut to indicate the Dutch or Flemish languages. In range of void+:language","title":"Language"},{"location":"03.Ontology/#matchingalgorithm","text":"Property Value URI http://lenticularlens.org/voidPlus/MatchingAlgorithm Description A set of rules followed by a computer for finding pairs of matching resources. An algorithm can be further categorised as automated or semi-automated depending on whether it requires user assistance or not. Most matching algorithms employed in the Lenticular Lens are semi-automated. In range of void+:hasAlgorithm","title":"MatchingAlgorithm"},{"location":"03.Ontology/#matchingmethod","text":"Property Value URI http://lenticularlens.org/voidPlus/MatchingMethod Description A resource that makes explicit all arguments/pre-requisites (datasets, entity-type and property-value restrictions, matching properties\u2026) of a matching algorithm including the conditions in which the algorithm is to accept a discovered link (threshold). Equivalent to void+:hasAlgorithm some void+:MatchingAlgorithm In domain of void+:hasThresholdAcceptanceOperator void+:hasAlgorithm void+:hasThreshold void+:hasThresholdRange","title":"MatchingMethod"},{"location":"03.Ontology/#propertysequence","text":"Property Value URI http://lenticularlens.org/voidPlus/PropertySequence Description A resource that represents a sequence of properties, analogously to a property path. Super-classes rdf:Seq Equivalent to rdfs:member only ( rdf:Property or rdfs:Class ) and rdfs:member min 2 rdf:Property and rdfs:member min 1 rdfs:Class","title":"PropertySequence"},{"location":"03.Ontology/#typedpropertysequence","text":"Property Value URI http://lenticularlens.org/voidPlus/TypedPropertySequence Description A resource that represents a sequence of properties and classes, as in a different type of property path with class restriction in between: property -> class -> property -> class -> property Super-classes rdf:Seq Equivalent to rdfs:member only rdf:Property and rdfs:member min 2 rdf:Property","title":"TypedPropertySequence"},{"location":"03.Ontology/#validation","text":"Property Value URI http://lenticularlens.org/voidPlus/Validation Description A void:Dataset that contains triples describing the validaty of links stemmed from a void+:LinkDataset. Super-classes void:Dataset void+:toBeValidated exactly 1 void+:hasAccepted exactly 1 void+:hasRejected exactly 1 void+:hasValidations exactly 1 Equivalent to inverse ( void+:hasValidation ) some void+:LinkDataset In range of void+:hasValidation","title":"Validation"},{"location":"03.Ontology/#dctermsagent","text":"Property Value URI http://purl.org/dc/terms/Agent Description A resource that acts or has the power to act. In range of dcterms:publisher","title":"dcterms:Agent"},{"location":"03.Ontology/#voidtechnicalfeature","text":"Property Value URI http://rdfs.org/ns/void#TechnicalFeature Description A technical feature of a void:Dataset, such as a supported RDF serialization format. In range of void:feature","title":"void:TechnicalFeature"},{"location":"03.Ontology/#rdfproperty","text":"Property Value URI http://www.w3.org/1999/02/22-rdf-syntax-ns#Property Description The class of RDF properties. In range of void:linkPredicate void:property","title":"rdf:Property"},{"location":"03.Ontology/#rdfseq","text":"Property Value URI http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq Description The class of RDF \u2018Sequence\u2019 containers. Sub-classes void+:TypedPropertySequence void+:PropertySequence","title":"rdf:Seq"},{"location":"03.Ontology/#rdfsclass","text":"Property Value URI http://www.w3.org/2000/01/rdf-schema#Class Description This is the class of resources that are RDF classes. In range of void:class","title":"rdfs:Class"},{"location":"03.Ontology/#43-object-properties","text":"","title":"4.3 Object Properties"},{"location":"03.Ontology/#hasalgorithm","text":"Property Value URI http://lenticularlens.org/voidPlus/hasAlgorithm Description relates a void+:MatchingMethod to its void+:MatchingAlgorithm. Domain(s) void+:MatchingMethod Range(s) void+:MatchingAlgorithm","title":"hasAlgorithm"},{"location":"03.Ontology/#hasclasspartition","text":"Property Value URI http://lenticularlens.org/voidPlus/hasClassPartition Description relates a void:Dataset to its subset void+:ClassPartition. In practice, the main difference wrt void:classPartition is the name meant to avoid miss-usage as it can be read as \u201cis classPartition of\u201d as well. Super-properties void:classPartition void+:hasSubset","title":"hasClassPartition"},{"location":"03.Ontology/#haslanguagepartition","text":"Property Value URI http://lenticularlens.org/voidPlus/hasLanguagePartition Description relates a void:Dataset to its subset void+:LanguagePartition. Super-properties void+:hasSubset","title":"hasLanguagePartition"},{"location":"03.Ontology/#haspropertypartition","text":"Property Value URI http://lenticularlens.org/voidPlus/hasPropertyPartition Description relates a void:Dataset to its subset void+:PropertyPartition. In practice, the main difference wrt void:propertyPartition is the name meant to avoid miss-usage as it can be read as \u201cis propertyPartition of\u201d as well. Super-properties void:propertyPartition void+:hasSubset","title":"hasPropertyPartition"},{"location":"03.Ontology/#hasformulation","text":"Property Value URI http://lenticularlens.org/voidPlus/hasFormulation Description relates a void+:Dataset to its void+:Formulation. Domain(s) void:Dataset Range(s) void+:Formulation","title":"hasFormulation"},{"location":"03.Ontology/#hasitem","text":"Property Value URI http://lenticularlens.org/voidPlus/hasItem Description relates a void+:Formulation to its formula-items. Domain(s) void+:Formulation","title":"hasItem"},{"location":"03.Ontology/#hasobjectresourceselection","text":"Property Value URI http://lenticularlens.org/voidPlus/hasObjectResourceSelection Description a relation to assign a void+:ResourceSelection as the source for the objects of triples under scrutiny. Super-properties void+:hasResourceSelection","title":"hasObjectResourceSelection"},{"location":"03.Ontology/#hassubjectresourceselection","text":"Property Value URI http://lenticularlens.org/voidPlus/hasSubjectResourceSelection Description a relation to assign a void+:ResourceSelection as the source for the subjects of triples under scrutiny. Super-properties void+:hasResourceSelection","title":"hasSubjectResourceSelection"},{"location":"03.Ontology/#hasresourceselection","text":"Property Value URI http://lenticularlens.org/voidPlus/hasResourceSelection Description a relation to assign a void+:ResourceSelection as the source for entities under scrutiny.For example, the selection of a void+:ResourceSelection for a void+:MatchingMethod. Range(s) void+:ResourceSelection","title":"hasResourceSelection"},{"location":"03.Ontology/#hassubset","text":"Property Value URI http://lenticularlens.org/voidPlus/hasSubset Description relates a void:Dataset that has as subset a void+:Partition (dataset). In practice, the main difference wrt void:subset is the name meant to avoid miss-usage as it can be read as \u201cis subset of\u201d as well. For a similar purpose, an inverse relation void+:subsetOf is also defined. Super-properties void:subset Domain(s) void:Dataset Range(s) void+:Partition","title":"hasSubset"},{"location":"03.Ontology/#hastarget","text":"Property Value URI http://lenticularlens.org/voidPlus/hasTarget Description A relation that assigns one or more void:Datasets linked via a void+:LinkDataset. Domain(s) void+:LinkDataset Range(s) void:Dataset","title":"hasTarget"},{"location":"03.Ontology/#hasvalidation","text":"Property Value URI http://lenticularlens.org/voidPlus/hasValidation Description A relation that assigns one or more void+:Validations to a void+:LinkDataset. Domain(s) void+:LinkDataset Range(s) void+:Validation","title":"hasValidation"},{"location":"03.Ontology/#language_1","text":"Property Value URI http://lenticularlens.org/voidPlus/language Description A relation that assigns to a void:Dataset a void+:Language (for ISO standard language codes) that is the rdf:language of all literals (objects) qualifying entities (subject) in a language-based partition. Super-properties void+:partitionedBy Range(s) void+:Language","title":"language"},{"location":"03.Ontology/#linkpredicate","text":"Property Value URI http://lenticularlens.org/voidPlus/linkPredicate Description a link predicate that holds for a void+:LinkDataset. Super-properties owl:topObjectProperty","title":"linkPredicate"},{"location":"03.Ontology/#objectstarget","text":"Property Value URI http://lenticularlens.org/voidPlus/objectsTarget Description A relation that assigns void:Datasets describing the object of triples contained in the void+:DirectedLinkDataset. Super-properties void+:hasTarget Domain(s) void+:DirectedLinkDataset Range(s) void:Dataset","title":"objectsTarget"},{"location":"03.Ontology/#subjectstarget","text":"Property Value URI http://lenticularlens.org/voidPlus/subjectsTarget Description A relation that assigns void:Datasets describing the subjects of triples contained in the void+:DirectedLinkDataset. Super-properties void+:hasTarget Domain(s) void+:DirectedLinkDataset Range(s) void:Dataset v","title":"subjectsTarget"},{"location":"03.Ontology/#partitionedby","text":"Property Value URI http://lenticularlens.org/voidPlus/partitionedBy Description A relation that assigns a void:Dataset to the conditions under which it is partitioned. Domain(s) void:Dataset","title":"partitionedBy"},{"location":"03.Ontology/#property","text":"Property Value URI http://lenticularlens.org/voidPlus/property Description A relation that assigns to a void:Dataset a rdf:Property or a rdf:Seq of properties that expresses the pattern of all triples in a property-based partition. Super-properties void+:partitionedBy Range(s) rdf:Property rdf:Seq","title":"property"},{"location":"03.Ontology/#subsetof","text":"Property Value URI http://lenticularlens.org/voidPlus/subsetOf Description relates a void+:Partition to its void:Dataset superset. Defined as the inverse relation of void+:hasSubset.","title":"subsetOf"},{"location":"03.Ontology/#dctermscreator","text":"Property Value URI http://purl.org/dc/terms/creator Description The range for dcterms:creator is dcterms:Agent. All values used with this property have to be instances of the class [dcterms:Agent] . must not be used with literal values . You may use it only with non-literal values.","title":"dcterms:creator"},{"location":"03.Ontology/#dctermspublisher","text":"Property Value URI http://purl.org/dc/terms/publisher Description The range for dcterms:publisher is dcterms:Agent. All values used with this property have to be instances of the class [dcterms:Agent] . must not be used with literal values . You may use it only with non-literal values. Range(s) dcterms:Agent","title":"dcterms:publisher"},{"location":"03.Ontology/#voidlinkpredicate","text":"Property Value URI http://rdfs.org/ns/void#linkPredicate Description a link predicate that holds for a void:Linkset. Super-properties void+:linkPredicate Domain(s) void:Linkset Range(s) rdf:Property","title":"void:linkPredicate"},{"location":"03.Ontology/#voidobjectstarget","text":"Property Value URI http://rdfs.org/ns/void#objectsTarget Description A relation that assigns the void:Dataset describing the objects of triples contained in the void:Linkset. Super-properties void:target void+:objectsTarget Domain(s) void:Linkset","title":"void:objectsTarget"},{"location":"03.Ontology/#voidsubjectstarget","text":"Property Value URI http://rdfs.org/ns/void#subjectsTarget Description A relation that assigns the void:Dataset describing the subjects of triples contained in the void:Linkset. Super-properties void:target void+:subjectsTarget Domain(s) void:Linkset","title":"void:subjectsTarget"},{"location":"03.Ontology/#voidproperty","text":"Property Value URI http://rdfs.org/ns/void#property Description A relation that assigns to a void:Dataset a rdf:Property that is the predicate of all triples in a property-based partition. Super-properties void+:property Domain(s) void:Dataset Range(s) rdf:Property","title":"void:property"},{"location":"03.Ontology/#voidclass","text":"Property Value URI http://rdfs.org/ns/void#class Description A relation that assigns to a void:Dataset a rdfs:Class that is the rdf:type of all entities (subject) in a class-based partition. Super-properties void+:partitionedBy Range(s) rdfs:Class","title":"void:class"},{"location":"03.Ontology/#voidclasspartition","text":"Property Value URI http://rdfs.org/ns/void#classPartition Description A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only entities of an explicitly defined rdfs:Class. Super-properties void:subset","title":"void:classPartition"},{"location":"03.Ontology/#voidpropertypartition","text":"Property Value URI http://rdfs.org/ns/void#propertyPartition Description A relation between a void:Dataset and its Partition, which is a subset of a void:Dataset that contains only triples composed with an explicitly defined rdf:Property. Super-properties void:subset","title":"void:propertyPartition"},{"location":"03.Ontology/#voidfeature","text":"Property Value URI http://rdfs.org/ns/void#feature Description relates a void:TechnicalFeature supported by a void:Datset. Domain(s) void:Dataset Range(s) void:TechnicalFeature","title":"void:feature"},{"location":"03.Ontology/#voidsubset","text":"Property Value URI http://rdfs.org/ns/void#subset Description relates a void:Dataset that has subset another void:Dataset. Domain(s) void:Dataset Range(s) void:Dataset","title":"void:subset"},{"location":"03.Ontology/#voidtarget","text":"Property Value URI http://rdfs.org/ns/void#target Description A relation that assigns one of the two void:Datasets linked by the void:Linkset. Super-properties void+:hasTarget Domain(s) void:Linkset Range(s) void:Dataset","title":"void:target"},{"location":"03.Ontology/#rdfsmember","text":"Property Value URI http://www.w3.org/2000/01/rdf-schema#member Description rdfs:member is an instance of rdf:Property that is a super-property of all the container membership properties. In particular, properties called rdf:_1, rdf:_2, rdf:_3\u2026 etc., used for rdf:Seq are sub-properties of rdfs:member.","title":"rdfs:member"},{"location":"03.Ontology/#44-datatype-properties","text":"","title":"4.4 Datatype Properties"},{"location":"03.Ontology/#ccattributionname","text":"Property Value URI http://creativecommons.org/ns#attributionName Description The name the creator of a Work would like used when attributing re-use.","title":"cc:attributionName"},{"location":"03.Ontology/#cclicense","text":"Property Value URI http://creativecommons.org/ns#license Description A work has license a License.","title":"cc:license"},{"location":"03.Ontology/#hasaccepted","text":"Property Value URI http://lenticularlens.org/voidPlus/hasAccepted Description The amount of accepted links for a void+:LinkDataset or void+:Validation. Domain(s) ( void+:LinkDataset or void+:Validation ) Range(s) xsd:integer","title":"hasAccepted"},{"location":"03.Ontology/#hasclusters","text":"Property Value URI http://lenticularlens.org/voidPlus/hasClusters Description The amount of clusters computed for a void+:LinkDataset. Domain(s) void+:LinkDataset Range(s) xsd:integer","title":"hasClusters"},{"location":"03.Ontology/#hascontradictions","text":"Property Value URI http://lenticularlens.org/voidPlus/hasContradictions Description The amount of links of a void+:LinkDataset for which contradicting validations are found. Domain(s) void+:LinkDataset Range(s) xsd:integer","title":"hasContradictions"},{"location":"03.Ontology/#hasfilterfunction","text":"Property Value URI http://lenticularlens.org/voidPlus/hasFilterFunction Description It relates a property partition to a filter function, such as \u201ccontains\u201d or \u201cminimal_date\u201d. When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \u201cfunction\u201d, a \u201cvalue\u201d and a \u201cformat\u201d. Domain(s) void+:PropertyPartition","title":"hasFilterFunction"},{"location":"03.Ontology/#hasformatfunction","text":"Property Value URI http://lenticularlens.org/voidPlus/hasFormatFunction Description It relates a property partition to a format restriction which is applicable to the value used by an explicitly defined filter function, such as \u201cYYYY-MM-DD\u201d for filter \u201cminimal_date\u201d. When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \u201cfunction\u201d, a \u201cvalue\u201d and a \u201cformat\u201d. Domain(s) void+:PropertyPartition","title":"hasFormatFunction"},{"location":"03.Ontology/#hasformuladescription","text":"Property Value URI http://lenticularlens.org/voidPlus/hasFormulaDescription Description Describes how items listed in a formulation are combined. For example, using logic or set-like operators. Domain(s) void+:Formulation Range(s) xsd:string","title":"hasFormulaDescription"},{"location":"03.Ontology/#hasrejected","text":"Property Value URI http://lenticularlens.org/voidPlus/hasRejected Description The amount of rejected links for a void+:LinkDataset or void+:Validation. Domain(s) ( void+:LinkDataset or void+:Validation ) Range(s) xsd:integer","title":"hasRejected"},{"location":"03.Ontology/#hasthreshold","text":"Property Value URI http://lenticularlens.org/voidPlus/hasThreshold Description Relates a void+:MatchingMethod to a link-acceptance-threshold value. Upon the choice of a void+:MatchingAlgorithm, the user should provide the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \u201cYear\u201d), a threshold operator (e.g. \u201c>=\u201d) and a threshold range (e.g. \u201c]0,1]\u201d or \u201c\u2115\u201d). Domain(s) void+:MatchingMethod","title":"hasThreshold"},{"location":"03.Ontology/#hasthresholdacceptanceoperator","text":"Property Value URI http://lenticularlens.org/voidPlus/hasThresholdAcceptanceOperator Description Relates a void+:MatchingMethod to a link-acceptance-threshold operator. Upon the choice of a void+:MatchingAlgorithm, the user should inform the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \u201cYear\u201d), a threshold operator (e.g. \u201c>=\u201d) and a threshold range (e.g. \u201c]0,1]\u201d or \u201c\u2115\u201d). Domain(s) void+:MatchingMethod","title":"hasThresholdAcceptanceOperator"},{"location":"03.Ontology/#hasthresholdrange","text":"Property Value URI http://lenticularlens.org/voidPlus/hasThresholdRange Description Relates a void+:MatchingMethod to a link-acceptance-threshold range. Upon the choice of a void+:MatchingAlgorithm, the user should inform the parameters for guiding the acceptance of results. This may include a threshold value (e.g. 0,7 or 20), a threshold unit (e.g. \u201cYear\u201d), a threshold operator (e.g. \u201c>=\u201d) and a threshold range (e.g. \u201c]0,1]\u201d or \u201c\u2115\u201d). Domain(s) void+:MatchingMethod","title":"hasThresholdRange"},{"location":"03.Ontology/#hasvalidations","text":"Property Value URI http://lenticularlens.org/voidPlus/hasValidations Description The amount of validated links in a void+:LinkDataset or void+:Validation. Domain(s) ( void+:LinkDataset or void+:Validation ) Range(s) xsd:integer","title":"hasValidations"},{"location":"03.Ontology/#hasvaluefunction","text":"Property Value URI http://lenticularlens.org/voidPlus/hasValueFunction Description It relates a property partition to a value for a filter function, such as \u201c%van%\u201d for filter \u201ccontains\u201d or \u201c01/01/1600\u201d for filter \u201cminimal_date\u201d. When restricting a property partition, in addition to the property for which a value is expected to exist, a filter can also be applied to the value. This filter is composed of \u201cfunction\u201d, a \u201cvalue\u201d and a \u201cformat\u201d. Domain(s) void+:PropertyPartition","title":"hasValueFunction"},{"location":"03.Ontology/#tobevalidated","text":"Property Value URI http://lenticularlens.org/voidPlus/toBeValidated Description The amount of links yet to be validated in a void+:LinkDataset or void+:Validation. Domain(s) ( void+:LinkDataset or void+:Validation ) Range(s) xsd:integer","title":"toBeValidated"},{"location":"03.Ontology/#dctermscreated","text":"Property Value URI http://purl.org/dc/terms/created Description The range defined for dcterms:created is the class of rdfs:Literal. Values used with this property therefore have to be literal values. Range(s) rdfs:Literal","title":"dcterms:created"},{"location":"03.Ontology/#dctermsdescription","text":"Property Value URI http://purl.org/dc/terms/description Description There is no range defined for dcterms:description. Therefore you can use it with literal values.","title":"dcterms:description"},{"location":"03.Ontology/#voiddistinctobjects","text":"Property Value URI http://rdfs.org/ns/void#distinctObjects Description The total number of distinct objects in a void:Dataset. In other words, the number of distinct resources that occur in the object position of triples in the dataset. Literals are included in this count. Domain(s) void:Dataset Range(s) xsd:integer","title":"void:distinctObjects"},{"location":"03.Ontology/#voiddistinctsubjects","text":"Property Value URI http://rdfs.org/ns/void#distinctSubjects Description The total number of distinct subjects in a void:Dataset. In other words, the number of distinct resources that occur in the subject position of triples in the dataset. Domain(s) void:Dataset Range(s) xsd:integer","title":"void:distinctSubjects"},{"location":"03.Ontology/#voidentities","text":"Property Value URI http://rdfs.org/ns/void#entities Description The total number of entities that are described in a void:Dataset. Domain(s) void:Dataset Range(s) xsd:integer","title":"void:entities"},{"location":"03.Ontology/#voidtriples","text":"Property Value URI http://rdfs.org/ns/void#triples Description The total number of triples contained in a void:Dataset. Domain(s) void:Dataset Range(s) xsd:integer","title":"void:triples"},{"location":"04.Algorithms/","text":".katex img { display: block; position: absolute; width: 100%; height: inherit; } Matching Algorithms \u00b6 A set of rules followed by a computer for finding pairs of matching resources is defined here as a matching algorithm . It can be further categorised as automated or semi-automated depending on whether it requires user assistance or not. Most matching algorithms employed in the Lenticular Lens are semi-automated. This section presents the catalogue of algorithms available in the Lenticular Lens and the standard expected Inputs and Outputs. The system offers a variety of existing and newly developed algorithms from which the user can choose one or more in order to perform a matching. These algorithms apply for example, to strings, dates or URIs. Here by, we describe the ones currently offered by the system and how they are meant to be used: Exact search Exact Mapping Embedded Change / Edit Jaro Jaro-Winkler Levenshtein Bloothooft Phonetic Soundex Metaphone Double Metaphone NYIIS Hybrid Intermediate ( exact and mapping ) Soundex Approximation ( phonetic and changes/edits ) Approx over Same-Soundex ( phonetic and changes/edits ) Word Intersection ( in-exact and changes/edits ) TeAM:Text-Approximation-Match ( changes/edits and phonetic ) Numerical Numbers Time Delta Make use of above methods List Intersection 1. Discrete Strength \u00b6 Her we present algorithms for which the resulting strength of a matched pair of resources is assigned the value of 1 as 0 means a match could not be found. Exact \u00b6 This method is used to align source and target\u2019s IRIs whenever their respective user selected property values are identical. In Example-1, the linkset ex:ExactLinkset is generated using the exact method over the predicate rdfs:label . Although only sharing the exact same name, these two entities are clearly not the same. Example 1 : Generating a linkset using the Exact algorithm. institutes: grid .1019.9 rdfs: label \"Victoria University\" ; grid: wikipediaPage wiki: Victoria_University , _Australia ; grid: establishedYear \"1916\" ^^ <http://www.w3.org/2001/XMLSchema#gYear> ; gridC: abbreviation > \"VU\" ; skos: prefLabel \"Victoria University\" ; \u2022\u2022\u2022 institutes: grid .449929 . b rdfs: label \"Victoria University\" ; grid: wikipediaPage <https://en.wikipedia.org/wiki/Victoria_University_Uganda> ; grid: establishedYear \"2011\" ^^ <http://www.w3.org/2001/XMLSchema#gYear> ; gridC: abbreviation > \"VUU\" ; skos: prefLabel \"Victoria University\" ; \u2022\u2022\u2022 ex: ExactLinkset { << institutes: grid .1019.9 owl: sameAs institutes: grid .449929 . b>> ll: hasMatchingStrength 1 . } Embedded \u00b6 The Embedded method extracts an alignment already provided within the source dataset. The extraction relies on the value of the linking property, i.e. property of the source that holds the identifier of the target (IRI of the target). The inconvenience in generating a linkset in such way is that the real mechanism used to create the existing alignment is not explicitly provided by the source dataset. Example-2 shows a sample of the Grid dataset with embedded links. With such links, Grid connects its instances to external datasets such as Wikidata for example using the link predicate: grid:hasWikidataId illustrated in Example-2 with the linkset graph ex:EmbeddedLinkset . Example 2 : Generating a linkset using the Embedded algorithm. institutes: grid .1001.0 foaf: homepage <http://www.anu.edu.au/> ; rdfs: label \"Australian National University\" ; grid: isni \"0000 0001 2180 7477\" ; grid: hasWikidataId wikidata: Q127990 ; \u2022 \u2022 \u2022 institutes: grid .1002.3 foaf: homepage <http://www.monash.edu/> ; rdfs: label \"Monash University\" ; grid: isni \"0000 0004 1936 7857\" ; grid: hasWikidataId wikidata: Q598841 ; \u2022 \u2022 \u2022 ex: EmbeddedLinkset { << institutes: grid .1001.0 grid: hasWikidataId wikidata: Q127990 >> ll: hasMatchingStrength 1 . << institutes: grid .1002.3 grid: hasWikidataId wikidata: Q598841 >> ll: hasMatchingStrength 1 . } Intermediate \u00b6 The method aligns the source and the target\u2019s IRIs via an intermediate database by using properties that potentially present different descriptions of the same entity, such as country name and country code. This is possible by providing an intermediate dataset that binds the two alternative descriptions to the very same identifier. Example 3.1 : Generating a linkset using the Intermediate-Datatse algorithm. In the example below, it is possible to align the source and target country entities using the properties country and iso-3 via the intermediate dataset because it contains the information described at both, the Source and Target. dataset: Source-Dataset dataset: Intermediate-Dataset { { ex : 1 rdfs: label \"Benin\" . ex : 7 ex : 2 rdfs: label \"Cote d'Ivoire\" . ex: name \"Cote d'Ivoire\" ; ex : 3 rdfs: label \"Netherlands\" . ex: code \"CIV\" . } ex : 8 dataset: Target-Datas ex: name \"Benin\" ; { ex: code \"BEN\" . ex : 4 ex: iso-3 \"CIV\" . ex : 5 ex: iso-3 \"NLD\" . ex : 9 ex : 6 ex: iso-3 \"BEN\" . ex: name \"Netherlands\" ; } ex: code \"NLD\" . } # --- ALIGNMENT ----------------------------- # \u2022 If rdfs:label is aligned with ex:name # \u2022 AND ex:iso-3 is aligned with ex:code, # \u2022 We then get the following linkset: # ------------------------------------------- linkset: Match-Via-Intermediate { <<ex : 1 owl: sameAs ex : 6 >> ll: hasMatchingStrength 1 . <<ex : 2 owl: sameAs ex : 4 >> ll: hasMatchingStrength 1 . <<ex : 3 owl: sameAs ex : 5 >> ll: hasMatchingStrength 1 . } Example 3.2 : Generating a linkset using the Intermediate-Datatse algorithm. In this second example, it is also possible to align the source and target datasets using the the authoritative intermediate dataset as it contains information described at both, the Source and Target. dataset: Source-Dataset dataset: Intermediate-Dataset { { ex : 10 rdfs: label \"Rembrandt\" . ex : 70 ex : 20 rdfs: label \"van Gogh\" . ex: name \"Vincent Willem van Gogh\" ; ex : 30 rdfs: label \"Vermeer\" . ex: name \"Vincent van Gogh\" ; } ex: name \"van Gogh\" . ex : 80 dataset: Target-Datas ex: name \"Rembrandt\" ; { ex: name \"Rembrandt van Rijn\" . ex : 40 schema: name \"Rembrandt van Rijn\" . ex : 50 schema: name \"Vincent van Gogh\" . ex : 90 ex : 60 schema: name \"Johannes Vermeer\" . ex: name \"Johannes Vermeer\" ; } ex: name \"Vermeer\" . } # --- ALIGNMENT ------------------------------------ # \u2022 If rdfs:label is aligned with ex:name # \u2022 AND schema:name is also aligned with ex:name, # \u2022 we then get the following linkset: # -------------------------------------------------- linkset: Match-Via-Intermediate { <<ex : 10 owl: sameAs ex : 40 >> ll: hasMatchingStrength 1 . <<ex : 20 owl: sameAs ex : 50 >> ll: hasMatchingStrength 1 . <<ex : 30 owl: sameAs ex : 60 >> ll: hasMatchingStrength 1 . } Soundex \u00b6 Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. Example 4 : Generating a linkset using the Soundex algorithm The examples below shows the encoding of different names. Here, the size parameter indicates a degree of similarity through the length of the respective soundex codes. For example at size=3, Albert and Albertine have the same soundex representation while at size=5 their respective representations differ. In the Lenticular lens, the default size is set to 3. # ----------------------------- # INPUT SIZE 3 SIZE 5 # ----------------------------- # A. A000 A00000 # AL A400 A40000 # ALI A400 A40000 # ALBERT A416 A41630 # ALBERTINE A416 A41635 ex : 128.45 rdfs: label \"ALBERT\" ; \u2022\u2022\u2022 ex : 6852 : 28 rdfs: label \"ALBERTINE\" ; \u2022\u2022\u2022 ex: ExactLinkset { <<ex : 128.45 owl: sameAs ex : 6852 : 28 >> ll: hasMatchingStrength 1 . } Metaphone \u00b6 Designed by Lawrence Philips in 1990, Metaphone is a phonetic algorithm for a more accurate encoding of words by sound (as compared to SOUNDEX) as pronounced in English. In this algorithm as with SOUNDEX, similar-sounding words should share the same encoding key which is an approximate phonetic representation of the original word. For more accuracy, consider the use of Double Metaphone as it is an improvement of the Original Metaphone. In turn, for an even better improved accuracy, consider the use of Metaphone 3 . Double Metaphone \u00b6 Double Metaphone is a third generation phonetic algorithm improvement after Soundex and Metaphone for an accurate encoding of words by sound as pronounced in English. For more accuracy, consider the use of METAPHONE 3 as it is an improvement of the Double Metaphone algorithm. It is called \u201cDouble\u201d because it can return both a primary and a secondary code for a string; this accounts for some ambiguous cases as well as for multiple variants of surnames with common ancestry. For example, encoding the name \u201cSmith\u201d yields a primary code of SM0 and a secondary code of XMT, while the name \u201cSchmidt\u201d yields a primary code of XMT and a secondary code of SMT\u2014both have XMT in common. Double Metaphone tries to account for myriad irregularities in English of Slavic, Germanic, Celtic, Greek, French, Italian, Spanish, Chinese, and other origin. Thus it uses a much more complex ruleset for coding than its predecessor; for example, it tests for approximately 100 different contexts of the use of the letter C alone. See [ https://en.wikipedia.org/wiki/Metaphone ] for mor information. NYIIS \u00b6 TODO Example 7 : Numbers \u00b6 The method is used to align the source and the target IRIs whenever the delta difference between values (number/date) of the user selected properties is within a preset delta-threshold. Example 8 For example, if two entities have been aligned based on the similarity of their names but an extra check is to be investigated based on their respective year of birth, setting the delta-threshold to 1 year will ensure that the birth dates assigned to the two entities are no more than one year apart. Time Delta \u00b6 This function allows for finding co-referent entities on the basis of a minimum time difference between the times reported by the source and the target entities. For example, if the value zero is assigned to the time difference parameter, then, for a matched to be found, the time of the target and the one of the source are to be the exact same times. While accounting for margins of error, one may consider a pair of entities to be co-referent if the real entities are born lambda days, months or years apart among other-things (similar name, place..). Time Delta before, After or Between. Other ways of using time delta are also possible. Because Time Delta has no direction or sign (+ or -), it is not possible to require that the time documented at the source entity occurs before or after the one reported by the target\u2019s event. With the before / After options, this is possible. Using the Between option, we make it possible for the user to define the time interval in which both the source and target reported times can be viewed as acceptable. Example 9 : 2. Continuous Strength \u00b6 Her we present algorithms for which the resulting strength of a matched pair of resources is in the range ]0, 1] where 0 means a match could not be found while 1 means a perfect match is found. Jaro \u00b6 This \u200bmethod is used to align \u200b\u200bsource a\u200bnd \u200b\u200btarget\u2019s IRIs whenever the similarity score of their respective user selected property values are \u200b\u200babove a given threshold in the range ]0, 1]\u200b. Jaro distance is a measure of similarity between two strings. The higher the Jaro distance for two strings is, the more similar the strings are. The score is normalised such that 0 equates to no similarity and 1 is an exact match. Given two strings s 1 and s 2 , Find common characters x i from s 1 and y j from s 2 such that x i = y j where x i and y j are not more than d characters aways from each other and where the acceptable matching distance d is half the longest input string as expressed in the formula: d = m a x ( \u2223 s 1 \u2223 , \u2223 s 2 \u2223 2 d = \\dfrac{max(|s_1|, |s_2|} {2} d = 2 m a x ( \u2223 s 1 \u200b \u2223 , \u2223 s 2 \u200b \u2223 \u200b Let c be number of common characters shared by s 1 and s 2 . From the resulting common characters in s 1 and s 2 , let t be the number of transposition which is the number of characters that do not share the same position. Compute the strength using the formula: d j = { 0 if c = 0 1 3 ( c \u2223 s 1 \u2223 + c \u2223 s 2 \u2223 + c \u2212 t \u2223 s 1 \u2223 ) otherwise d_j = \\begin{cases} 0 &\\text{if } c = 0 \\\\ \\dfrac{1}{3} \\Bigg( \\dfrac{c}{|s_1|} + \\dfrac{c}{|s_2|} + \\dfrac{c - t}{|s_1|} \\Bigg) &\\text{otherwise} \\end{cases} d j \u200b = \u23a9 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a7 \u200b 0 3 1 \u200b ( \u2223 s 1 \u200b \u2223 c \u200b + \u2223 s 2 \u200b \u2223 c \u200b + \u2223 s 1 \u200b \u2223 c \u2212 t \u200b ) \u200b if c = 0 otherwise \u200b Example 10: Jaro results SOURCE : |s| = |source| = 6 TARGET : |t| = |target| = 6 MATCHING DISTANCE : d = max(6, 6) = 3 COMMON CHARACTERS : m = |re| = |re| = 2 TRANSPOSITIONS : t = 0 STRENGTH : s = 1/3 * ( 2/6 + 2/6 + (2 - 0/2)/2) = 0.55556 SOURCE : |s| = |jono| = 4 TARGET : |t| = |ojhono| = 6 MATCHING DISTANCE : d = max(4, 6) = 3 COMMON CHARACTERS : m = |jono| = |ojon| = 4 TRANSPOSITIONS : t = 4 STRENGTH : s = 1/3 * ( 4/4 + 4/6 + (4 - 4/2)/4) = 0.72222 SOURCE : |s| = |DUANE| = 5 TARGET : |t| = |DWAYNE| = 6 MATCHING DISTANCE : d = max(5, 6) = 3 COMMON CHARACTERS : m = |DANE| = |DANE| = 4 TRANSPOSITIONS : t = 0 STRENGTH : s = 1/3 * ( 4/5 + 4/6 + (4 - 0/2)/4) = 0.82222 SOURCE : |s| = |DIXON| = 5 TARGET : |t| = |DICKSONX| = 8 MATCHING DISTANCE : d = max(5, 8) = 4 COMMON CHARACTERS : m = |DION| = |DION| = 4 TRANSPOSITIONS : t = 0 STRENGTH : s = 1/3 * ( 4/5 + 4/8 + (4 - 0/2)/4) = 0.76667 SOURCE : |s| = |JELLYFISH| = 9 TARGET : |t| = |SMELLYFISH| = 10 MATCHING DISTANCE : d = max(9, 10) = 5 COMMON CHARACTERS : m = |ELLYFISH| = |ELLYFISH| = 8 TRANSPOSITIONS : t = 0 STRENGTH : s = 1/3 * ( 8/9 + 8/10 + (8 - 0/2)/8) = 0.8963 Jaro-Winkler \u00b6 Given two strings s 1 and s 2 , the Winkler similarity equation boosts up the Jaro algorithm\u2019s result d j by increasing it whenever the compared strings share a prefix of a maximum of four characters. In this shared prefix scenario, the boost is computed as: w b = P l \u2217 P w ( 1 \u2212 d j ) w_b = P_l * P_w ( 1 - d_j ) w b \u200b = P l \u200b \u2217 P w \u200b ( 1 \u2212 d j \u200b ) where P l is the length of the set of shared prefix and P w is a user dependent scaling factor for how much the score is adjusted upwards for having common prefixes. Because four is the maximum number of shared prefix to consider, the user\u2019s choice of P w lies between 0 and \u00bc. Setting P w to \u00bc implies a similarity of always 1 whenever the strings share the maximum of 4 prefixes, no matter the real dissimilarity between the strings. The Jaro-Winkler is computed as: d j w = d j + w b d_{jw}= d_j+ w_b d j w \u200b = d j \u200b + w b \u200b Example 11: Jaro vs Jaro-Winkler jaro ( \"alexander\" , \"alexandrine\" ) = 0.90236 jaro_winkler ( \"alexander\" , \"alexandrine\" , weight = 0.1 ) = 0.94141 Levenshtein \u00b6 This \u200bmethod is used to align \u200b\u200bsource a\u200bnd \u200b\u200btarget\u2019s IRIs whenever the similarity score of their respective user selected property values are \u200b\u200babove a given \u200bLevenshtein Distance threshold\u200b. Levenshtein (a.k.a. Edit Distance) is a way of quantifying how \u200bdissimilar two strings (e.g., words) are to one another by counting the minimum number of operations \u200b\u03b5 \u200b (\u200bremoval, insertion, or substitution of a character in the string)\u200b required to transform one string into the other. For example, \u200bthe \u200bLevenshtein distance between \u201ckitten\u201d and \u201csitting\u201d is \u200b\u03b5 \u200b= 3 as it requires two substitutions (\u201cs\u201d for \u201ck\u201d and \u201ci\u201d for \u201ce\u201d) and one insertion of \u201cg\u201d at the end. Normalisation \u200b\u03a9\u200b: In the Lenticular Lens , the \u200bsimilarity score \u200b\u03a9 \u200bof a matching pair of resources is quantified in the interval \u200b[0, 1]\u200b. For this, the\u200b dissimilarity score \u200b\u03b5 expressing the \u200bminimum number of operations to perform for \u200btwo strings to be the same \u200bis normalised as \u200b\u03a9 based on the length of the longest string. The \u200bdissimilarity score\u200b\u200b \u03b5\u200b = 3 \u200bbetween \u200b\u201dkitten\u201d and \u201csitting\u201d is then normalised to a \u200bsimilarity score \u03a9\u200b \u200b=\u200b \u200b1 - 3 / 7 = 0.57\u200b.\u200b Minimum threshold \u200b\u03c6: Using this algorithm, \u200ba \u200bminimum threshold value\u03c6\u200b \u200bmust be set in the interval \u200b[0,1], \u200bsuch that finding any matched pairs of IRIs based on the similarity of their respective property values depends on whether or not the computed \u200b\u03a9 is \u200bequal or above \u200b\u03c6\u200b. A threshold \u200b\u03c6 = 1 equates an exact match. In our previous example, if a \u200bminimum threshold of \u200b\u03c6 = \u200b0.7 is set, \u201ckitten\u201d and \u201csitting\u201d will not be matched. \u200bIn short, \u200b\u03c6 is the user defined threshold when the similarity score \u200b\u03a9 is selected for accepting or rejecting a match. Maximum character error threshold \u200b\ud835\udeff:\u200b In case the \u200boriginal edit distance score \u200b\u03b5 (minimum number of operations score) is preferred, the number of character errors \u200b\ud835\udeff \u200bof choice is used instead as threshold for deciding whether a match is accepted or rejected. However, for consistency purposes, the corresponding normalisation value \u200b\u03a9 is still computed for the minimum number of operations score computed \u200b\u03b5\u200b. For instance, in our previous example, if a \u200bmaximum \u200bcharacters errors i\u200bs set to \u200b\ud835\udeff = 3, \u201ckitten\u201d and \u201csitting\u201d will be matched but the computed strength will be \u200b\u03a9 = \u200b0.57 a\u200bnd \u200bnot \u200b\u03b5 = 3 as is it only serves the purpose of decision maker. I\u200bn short, \u200b\ud835\udeff i\u200bs the user defined threshold when the dissimilarity score \u03b5\u200b \u200bis selected for accepting or rejecting a match. Example 12 : The Levenshtein options 1. l_dist(Rembrand van Rijn, Rembrandt Harmensz van Rijn) = 10 2. Normalised_l_dist \u200b(Rembrand van Rijn, Rembrandt Harmensz van Rijn) = 0.63 >>> If \u200b\u03c6 (Minimum threshold) = 0.7 >>> t\u200bhen \u200b[\u200bRembrand van Rijn] owl:sameAs [Rembrandt Harmensz van Rijn] >>> is rejected\u200b >>> because \u03a9\u200b = 0.63 < \u03c6\u200b. >>> If \u200b\u03b4 (Maximum character error threshold) = 5 >>> t\u200bhen [\u200b\u200bRembrand van Rijn] and [Rembrandt Harmensz van Rijn] is \u200brejected >>> because \u03b5 \u200b= 10 >\u200b \u200b\u03b4. Soundex Approximation \u00b6 This option is performed in two phases. First, the string values of the user selected properties are normalised (converted to a soundex code). Then a user selected matcher (Levenshtein, Jaro, jaro-Winkler\u2026) is run over the normalised string (Soundex code). Example 13: Links found using soundex In the example below, given ex:E1 and ex:E2 , the matching strength using Edit Distance is 1 - \u215b = 0.875. When directly performing an Edit distance match or a Soundex, we respectively get 0.625 and 0 as strengths. # ------------------------------- # INPUT SIZE 3 SIZE 4 # ------------------------------- # Carretta C630 C6300 # Kareta K630 K6300 ex: E1 rdfs: label \"Carretta\" . ex: E2 rdfs: label \"Kareta\" . ex: ExactLinkset { << ex: E1 skos: closeMatch <ex:E2> > ll: hasMatchingStrength 0.875 . } Approx over Same-Soundex \u00b6 In the Lenticular Lens , Soundex can also be used as a first filtering step (string normalisation). This means that first we select all pairs sharing the same Soundex code. Then, for each of these pairs an extra string sequence matching algorithm such as (Levenshtein, Jaro, Jaro-Winkler\u2026) is applied to the original pair of matched strings (not the Soundex codes) to determine the strength of the match in the interval ]0, 1]. Example 14 : Generating a linkset using the Soundex algorithm. In the table below, the normalisation of both Louijs Rocourt and Lowis Ricourt becomes L200 R263 leading to an edit distance of 0 and a relative strength of 1. However, computing the same names using directly an edit distance results in an edit distance of 3 and a relative matching strength of 0.79. The example below shows the implementation of Soundex Distance in the Lenticular Lens and how it compares with Edit Distance over the original names ( no soundex-based normalisation ). ------------------------------------------------------------------------------------------------------------------------------------------------------ Source Target E . Dist Rel . distance Source soundex Target soundex Code E . Dist Code Rel . Dist ------------------------------------------------------------------------------------------------------------------------------------------------------ Jasper Cornelisz . Lodder Jaspar Cornelisz Lodder 2 0.92 J 216 C 654 L 360 J 216 C 654 L 360 0 1.0 Barent Teunis Barent Teunisz gen . Drent 12 0.52 B 653 T 520 B 653 T 520 G 500 D 653 10 0.47 Louijs Rocourt Louys Rocourt 2 0.86 L 200 R 263 L 200 R 263 0 1.0 Louijs Rocourt Lowis Ricourt 3 0.79 L 200 R 263 L 200 R 263 0 1.0 Louys Rocourt Lowis Ricourt 3 0.77 L 200 R 263 L 200 R 263 0 1.0 Cornelis Dircksz . Clapmus Cornelis Clapmuts 10 0.6 C 654 D 620 C 415 C 654 C 415 5 0.64 Geertruydt van den Breemde Geertruijd van den Bremde 4 0.85 G 636 V 500 D 500 B 653 G 636 V 500 D 500 B 653 0 1.0 Bloothooft \u00b6 This approximation method is specifically tailored for accessing the similarity between a pair of IRIs for which the user selected property values are Dutch names. The algorithm basically normalises the given names by removing or replacing specific characters\u2026. The resulting normalised names are then pairwise compared using the Approximated Levenshtein Distance (see the description of Approximated Levenshtein Distance). Example 15 : Word Intersection \u00b6 This approximation method is originally designed to find a subset of words within a larger text. However, it could also be used for any pair of strings regardless of the strings sizes. Several options are available: Whether or not the order in which the words are found is important. Whether or not the computed strength of each word should be approximated or identical. Whether or not abbreviation should be detected. Whether the default stopping character should be used, not used or modified. A threshold on the number of words not approximated/identical. An overall threshold for accepting or rejecting a match. Example 16 : Expectation For example, it can be used for aligning - [Rembrand van Rijn] and [van Rijn Rembrandt] - [Herdoopers anslagh op Amsterdam] and [Herdoopers anslagh op Amsterdam. Den x. may: 1535. Treur-spel.] regardless of the words\u2019 order. Example 1 With argument APPROX set to True, similarity can be found between Rembrand and Rembrandt (MODE-1). However, this is not possible in the inverse scenario (MODE-2). Furthermore, when the order is of no importance (MODE-1) the sequence in which the intersection of words appears is of no importance. SMALL : Rembrand (van) Rijn BIG : Rembrandt Harmensz. van Rijn MODE-1 : APPROX = True and ORDER = False FOUND : ['rijn', 'rembrandt'] STRENGTH : 0.97059 MODE-2 : APPROX = False and ORDER = True and FOUND : [] STRENGTH : 0.0 Example 2 Because the match is performed over exact occurrences (APPROX = False) and the sequence in which the matched words appear in the text is of importance (ORDER = True), elvervelt is the only intersection found in MODE-2 and thereby bringing the similarity strength to 0.333. SMALL : Elvervelt, Henrik van BIG : Henrik van Elvervelt MODE-1 : APPROX = True and ORDER = False FOUND : ['elvervelt', 'henrik', 'van'] STRENGTH : 1.0 MODE-2 : APPROX = False and ORDER = True and FOUND : ['elvervelt'] STRENGTH : 0.333 Example 3 In here all intersected words appear in the same sequence and without mismatch. SMALL : Herdoopers anslagh op Amsterdam BIG : Herdoopers anslagh op Amsterdam. Den x. may: 1535. Treur-spel. MODE-1 : APPROX = True and ORDER = False FOUND : ['herdoopers', 'anslagh', 'op', 'amsterdam'] STRENGTH : 1.0 MODE-2 : APPROX = False and ORDER = True and FOUND : ['herdoopers', 'anslagh', 'op', 'amsterdam'] STRENGTH : 1.0 List Intersection \u00b6 This method is better suited for matching events. It helps establishing a relationship between the source and target entities whenever a list of entities from the source dataset intersects another list of entities stemmed from the target dataset. For illustration, suppose that we have the two events in Example-17. (1) Event one, defined by the source event-entities, documents event-entities that lists entities representing persons about to get married. This event is the Intended-Marriage event. (2) Event two is the Marriage event. It lists (i) entities representing persons that got married and (ii) the guests who attended the event. Now that the two events are defined, let us imagine that we would like to find out which of the Intended-Marriage couple fulfilled its will to get married. For this to be true, we assume that a couple with the wish to be wed should end-up being present at a Marriage event, hopefully their own marriage. This means that, for a match to occur, a minimum of two elements from a list from the source must also belong to a list from the target. In other words, a match is to be generated whenever 100% (all) of the elements in a source\u2019s lists intersects a target\u2019s list. In this method, two thresholds are to be defined. The first threshold or similarity-threshold imposes a minimum accepted similarity score (generally in ]0, 1]) when elements stemmed from different lists are compared. Passing this threshold is interpreted as the occurrence of an intersection between elements of two lists. The second threshold, the intersection-threshold expressed in quantity or percentage, denotes the minimum number of intersections that must occur for a link to be created. In practice, looking at Example-17, only events ex: intended-1 and ex:married-2 are a match as the pairs ( Catharina Reminck , Catharina Remink ) and ( Mr. Jean van de Melie , J. van de Melie ) are respectively similar with a score of 0.94 and 0.82. Example 17 : List Intersection A source dataset documenting events as lists of couple with the intention of getting married and a target dataset with list of people at a wedding ceremony. Each of these latter lists is expected to includes the wed couple. --------------------------------------------------------------------------------------------- Source Dataset Target Dataset --------------------------------------------------------------------------------------------- ex: intended-1 ex: ceremony-1 ex: wife \"Catharina Reminck\" ; ex: participant \"Pieter Jas\" ; ex: husband \"Mr. Jean van de Melie\" . ex: participant \"Jacob Poppen\" ; ex: participant \"Gillis Graafland\" ; ex: intended-2 ex: participant \"Jacob Fransz. de Witt\" ; ex: wife \"Eva Oostrom\" ; ex: participant \"Elisabeth van Daaken\" ; ex: husband \"Pieter de Vriest\" . ex: participant \"Catharina Berewouts\" ; ex: participant \"Aafje Hendricx\" ; ex: intended-3 ex: participant \"Anthony van Paembergh\" ; ex: wife \"Eva Oostrom\" ; ex: participant \"Eva van Toorn\" ; ex: husband \"Wiggert van Wesick\" . ex: participant \"Maria Bor\" . ex: married-2 ex: participant \"J. van de Melie\" , \"Bernardus van Vijven\" . \"Margrita Schrik\" , \"Johannes de Bruijn\" , \"Maria Gosina Demol\" , \"Agneta Swartepaart\" , \"Hendrik de Lange\" , \"Catharina Remink\" . ex: intended-4 ex: ceremony-2 ex: wife \"Catharina Elisabet\" ; ex: participant \"Pieter Robid\u00e9\" , ex: husband \"Pieter Robid\u00e9\" . \"Jacob Fredrik\" , \"Petronella Hobbe\" , \"Catharina Maria Boom\" , \"Pieter Robie\" , \"Fredrik Arentse\" . With a Similarity-threshold set to 0.8 and an Intersection-threshold set to 2 or 100%, only events ex:intended-1 and ex:married-2 are a match as the pairs ( Catharina Reminck , Catharina Remink ) and ( Mr. Jean van de Melie , J. van de Melie ) are respectively similar with similarity scores of 0.94 and 0.82. TeAM \u00b6 The TeAM (Text Approximation Match) method allows for the approximation of the relevance of a document to a query. Such approximation can be done using lexical similarity (word level similarity), semantic similarity or hybrid similarities. In this method, the focus is rather on the lexical similarity. Although tailored to text, it has been adapted to also be applicable to name-based similarity. We now provide an overview of the motivation context supporting the design and implementation of the algorithm. The Amsterdam\u2019s city archives (SAA) possesses physical handwritten inventories records where a record may be for example an inventory of goods (paintings, prints, sculpture, furniture, porcelain, etc.) owned by an Amsterdamer and mentioned in a last will. Interested in documenting the ownership of paintings from the 17 th century, the Yale University Professor John Michael Montias compiled a database by transcribing 1280 physical handwritten inventories (scattered in the Netherlands) of goods. Now that a number of these physical inventories have been digitised using handwriting recognition, one of the goals of the Golden Agent project is to identify Montias\u2019 transcriptions of painting selections within the digitised inventories. This problem can be generically reformulated as, given a source-segment database (e.g. Montias DB) and a target-segment database (e.g. SAA) , find the best similar target segment for each source segment . Fig 4.9: Digitisation of inventory documents available at the Amsterdam\u2019s city archives. Check TeAM for a more detailed explanation.","title":"4. Matching Algorithms"},{"location":"04.Algorithms/#matching-algorithms","text":"A set of rules followed by a computer for finding pairs of matching resources is defined here as a matching algorithm . It can be further categorised as automated or semi-automated depending on whether it requires user assistance or not. Most matching algorithms employed in the Lenticular Lens are semi-automated. This section presents the catalogue of algorithms available in the Lenticular Lens and the standard expected Inputs and Outputs. The system offers a variety of existing and newly developed algorithms from which the user can choose one or more in order to perform a matching. These algorithms apply for example, to strings, dates or URIs. Here by, we describe the ones currently offered by the system and how they are meant to be used: Exact search Exact Mapping Embedded Change / Edit Jaro Jaro-Winkler Levenshtein Bloothooft Phonetic Soundex Metaphone Double Metaphone NYIIS Hybrid Intermediate ( exact and mapping ) Soundex Approximation ( phonetic and changes/edits ) Approx over Same-Soundex ( phonetic and changes/edits ) Word Intersection ( in-exact and changes/edits ) TeAM:Text-Approximation-Match ( changes/edits and phonetic ) Numerical Numbers Time Delta Make use of above methods List Intersection","title":"Matching Algorithms  "},{"location":"04.Algorithms/#1-discrete-strength","text":"Her we present algorithms for which the resulting strength of a matched pair of resources is assigned the value of 1 as 0 means a match could not be found.","title":"1. Discrete Strength"},{"location":"04.Algorithms/#exact","text":"This method is used to align source and target\u2019s IRIs whenever their respective user selected property values are identical. In Example-1, the linkset ex:ExactLinkset is generated using the exact method over the predicate rdfs:label . Although only sharing the exact same name, these two entities are clearly not the same. Example 1 : Generating a linkset using the Exact algorithm. institutes: grid .1019.9 rdfs: label \"Victoria University\" ; grid: wikipediaPage wiki: Victoria_University , _Australia ; grid: establishedYear \"1916\" ^^ <http://www.w3.org/2001/XMLSchema#gYear> ; gridC: abbreviation > \"VU\" ; skos: prefLabel \"Victoria University\" ; \u2022\u2022\u2022 institutes: grid .449929 . b rdfs: label \"Victoria University\" ; grid: wikipediaPage <https://en.wikipedia.org/wiki/Victoria_University_Uganda> ; grid: establishedYear \"2011\" ^^ <http://www.w3.org/2001/XMLSchema#gYear> ; gridC: abbreviation > \"VUU\" ; skos: prefLabel \"Victoria University\" ; \u2022\u2022\u2022 ex: ExactLinkset { << institutes: grid .1019.9 owl: sameAs institutes: grid .449929 . b>> ll: hasMatchingStrength 1 . }","title":"Exact"},{"location":"04.Algorithms/#embedded","text":"The Embedded method extracts an alignment already provided within the source dataset. The extraction relies on the value of the linking property, i.e. property of the source that holds the identifier of the target (IRI of the target). The inconvenience in generating a linkset in such way is that the real mechanism used to create the existing alignment is not explicitly provided by the source dataset. Example-2 shows a sample of the Grid dataset with embedded links. With such links, Grid connects its instances to external datasets such as Wikidata for example using the link predicate: grid:hasWikidataId illustrated in Example-2 with the linkset graph ex:EmbeddedLinkset . Example 2 : Generating a linkset using the Embedded algorithm. institutes: grid .1001.0 foaf: homepage <http://www.anu.edu.au/> ; rdfs: label \"Australian National University\" ; grid: isni \"0000 0001 2180 7477\" ; grid: hasWikidataId wikidata: Q127990 ; \u2022 \u2022 \u2022 institutes: grid .1002.3 foaf: homepage <http://www.monash.edu/> ; rdfs: label \"Monash University\" ; grid: isni \"0000 0004 1936 7857\" ; grid: hasWikidataId wikidata: Q598841 ; \u2022 \u2022 \u2022 ex: EmbeddedLinkset { << institutes: grid .1001.0 grid: hasWikidataId wikidata: Q127990 >> ll: hasMatchingStrength 1 . << institutes: grid .1002.3 grid: hasWikidataId wikidata: Q598841 >> ll: hasMatchingStrength 1 . }","title":"Embedded"},{"location":"04.Algorithms/#intermediate","text":"The method aligns the source and the target\u2019s IRIs via an intermediate database by using properties that potentially present different descriptions of the same entity, such as country name and country code. This is possible by providing an intermediate dataset that binds the two alternative descriptions to the very same identifier. Example 3.1 : Generating a linkset using the Intermediate-Datatse algorithm. In the example below, it is possible to align the source and target country entities using the properties country and iso-3 via the intermediate dataset because it contains the information described at both, the Source and Target. dataset: Source-Dataset dataset: Intermediate-Dataset { { ex : 1 rdfs: label \"Benin\" . ex : 7 ex : 2 rdfs: label \"Cote d'Ivoire\" . ex: name \"Cote d'Ivoire\" ; ex : 3 rdfs: label \"Netherlands\" . ex: code \"CIV\" . } ex : 8 dataset: Target-Datas ex: name \"Benin\" ; { ex: code \"BEN\" . ex : 4 ex: iso-3 \"CIV\" . ex : 5 ex: iso-3 \"NLD\" . ex : 9 ex : 6 ex: iso-3 \"BEN\" . ex: name \"Netherlands\" ; } ex: code \"NLD\" . } # --- ALIGNMENT ----------------------------- # \u2022 If rdfs:label is aligned with ex:name # \u2022 AND ex:iso-3 is aligned with ex:code, # \u2022 We then get the following linkset: # ------------------------------------------- linkset: Match-Via-Intermediate { <<ex : 1 owl: sameAs ex : 6 >> ll: hasMatchingStrength 1 . <<ex : 2 owl: sameAs ex : 4 >> ll: hasMatchingStrength 1 . <<ex : 3 owl: sameAs ex : 5 >> ll: hasMatchingStrength 1 . } Example 3.2 : Generating a linkset using the Intermediate-Datatse algorithm. In this second example, it is also possible to align the source and target datasets using the the authoritative intermediate dataset as it contains information described at both, the Source and Target. dataset: Source-Dataset dataset: Intermediate-Dataset { { ex : 10 rdfs: label \"Rembrandt\" . ex : 70 ex : 20 rdfs: label \"van Gogh\" . ex: name \"Vincent Willem van Gogh\" ; ex : 30 rdfs: label \"Vermeer\" . ex: name \"Vincent van Gogh\" ; } ex: name \"van Gogh\" . ex : 80 dataset: Target-Datas ex: name \"Rembrandt\" ; { ex: name \"Rembrandt van Rijn\" . ex : 40 schema: name \"Rembrandt van Rijn\" . ex : 50 schema: name \"Vincent van Gogh\" . ex : 90 ex : 60 schema: name \"Johannes Vermeer\" . ex: name \"Johannes Vermeer\" ; } ex: name \"Vermeer\" . } # --- ALIGNMENT ------------------------------------ # \u2022 If rdfs:label is aligned with ex:name # \u2022 AND schema:name is also aligned with ex:name, # \u2022 we then get the following linkset: # -------------------------------------------------- linkset: Match-Via-Intermediate { <<ex : 10 owl: sameAs ex : 40 >> ll: hasMatchingStrength 1 . <<ex : 20 owl: sameAs ex : 50 >> ll: hasMatchingStrength 1 . <<ex : 30 owl: sameAs ex : 60 >> ll: hasMatchingStrength 1 . }","title":"Intermediate"},{"location":"04.Algorithms/#soundex","text":"Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. Example 4 : Generating a linkset using the Soundex algorithm The examples below shows the encoding of different names. Here, the size parameter indicates a degree of similarity through the length of the respective soundex codes. For example at size=3, Albert and Albertine have the same soundex representation while at size=5 their respective representations differ. In the Lenticular lens, the default size is set to 3. # ----------------------------- # INPUT SIZE 3 SIZE 5 # ----------------------------- # A. A000 A00000 # AL A400 A40000 # ALI A400 A40000 # ALBERT A416 A41630 # ALBERTINE A416 A41635 ex : 128.45 rdfs: label \"ALBERT\" ; \u2022\u2022\u2022 ex : 6852 : 28 rdfs: label \"ALBERTINE\" ; \u2022\u2022\u2022 ex: ExactLinkset { <<ex : 128.45 owl: sameAs ex : 6852 : 28 >> ll: hasMatchingStrength 1 . }","title":"Soundex"},{"location":"04.Algorithms/#metaphone","text":"Designed by Lawrence Philips in 1990, Metaphone is a phonetic algorithm for a more accurate encoding of words by sound (as compared to SOUNDEX) as pronounced in English. In this algorithm as with SOUNDEX, similar-sounding words should share the same encoding key which is an approximate phonetic representation of the original word. For more accuracy, consider the use of Double Metaphone as it is an improvement of the Original Metaphone. In turn, for an even better improved accuracy, consider the use of Metaphone 3 .","title":"Metaphone"},{"location":"04.Algorithms/#double-metaphone","text":"Double Metaphone is a third generation phonetic algorithm improvement after Soundex and Metaphone for an accurate encoding of words by sound as pronounced in English. For more accuracy, consider the use of METAPHONE 3 as it is an improvement of the Double Metaphone algorithm. It is called \u201cDouble\u201d because it can return both a primary and a secondary code for a string; this accounts for some ambiguous cases as well as for multiple variants of surnames with common ancestry. For example, encoding the name \u201cSmith\u201d yields a primary code of SM0 and a secondary code of XMT, while the name \u201cSchmidt\u201d yields a primary code of XMT and a secondary code of SMT\u2014both have XMT in common. Double Metaphone tries to account for myriad irregularities in English of Slavic, Germanic, Celtic, Greek, French, Italian, Spanish, Chinese, and other origin. Thus it uses a much more complex ruleset for coding than its predecessor; for example, it tests for approximately 100 different contexts of the use of the letter C alone. See [ https://en.wikipedia.org/wiki/Metaphone ] for mor information.","title":"Double Metaphone"},{"location":"04.Algorithms/#nyiis","text":"TODO Example 7 :","title":"NYIIS"},{"location":"04.Algorithms/#numbers","text":"The method is used to align the source and the target IRIs whenever the delta difference between values (number/date) of the user selected properties is within a preset delta-threshold. Example 8 For example, if two entities have been aligned based on the similarity of their names but an extra check is to be investigated based on their respective year of birth, setting the delta-threshold to 1 year will ensure that the birth dates assigned to the two entities are no more than one year apart.","title":"Numbers"},{"location":"04.Algorithms/#time-delta","text":"This function allows for finding co-referent entities on the basis of a minimum time difference between the times reported by the source and the target entities. For example, if the value zero is assigned to the time difference parameter, then, for a matched to be found, the time of the target and the one of the source are to be the exact same times. While accounting for margins of error, one may consider a pair of entities to be co-referent if the real entities are born lambda days, months or years apart among other-things (similar name, place..). Time Delta before, After or Between. Other ways of using time delta are also possible. Because Time Delta has no direction or sign (+ or -), it is not possible to require that the time documented at the source entity occurs before or after the one reported by the target\u2019s event. With the before / After options, this is possible. Using the Between option, we make it possible for the user to define the time interval in which both the source and target reported times can be viewed as acceptable. Example 9 :","title":"Time Delta"},{"location":"04.Algorithms/#2-continuous-strength","text":"Her we present algorithms for which the resulting strength of a matched pair of resources is in the range ]0, 1] where 0 means a match could not be found while 1 means a perfect match is found.","title":"2.  Continuous Strength"},{"location":"04.Algorithms/#jaro","text":"This \u200bmethod is used to align \u200b\u200bsource a\u200bnd \u200b\u200btarget\u2019s IRIs whenever the similarity score of their respective user selected property values are \u200b\u200babove a given threshold in the range ]0, 1]\u200b. Jaro distance is a measure of similarity between two strings. The higher the Jaro distance for two strings is, the more similar the strings are. The score is normalised such that 0 equates to no similarity and 1 is an exact match. Given two strings s 1 and s 2 , Find common characters x i from s 1 and y j from s 2 such that x i = y j where x i and y j are not more than d characters aways from each other and where the acceptable matching distance d is half the longest input string as expressed in the formula: d = m a x ( \u2223 s 1 \u2223 , \u2223 s 2 \u2223 2 d = \\dfrac{max(|s_1|, |s_2|} {2} d = 2 m a x ( \u2223 s 1 \u200b \u2223 , \u2223 s 2 \u200b \u2223 \u200b Let c be number of common characters shared by s 1 and s 2 . From the resulting common characters in s 1 and s 2 , let t be the number of transposition which is the number of characters that do not share the same position. Compute the strength using the formula: d j = { 0 if c = 0 1 3 ( c \u2223 s 1 \u2223 + c \u2223 s 2 \u2223 + c \u2212 t \u2223 s 1 \u2223 ) otherwise d_j = \\begin{cases} 0 &\\text{if } c = 0 \\\\ \\dfrac{1}{3} \\Bigg( \\dfrac{c}{|s_1|} + \\dfrac{c}{|s_2|} + \\dfrac{c - t}{|s_1|} \\Bigg) &\\text{otherwise} \\end{cases} d j \u200b = \u23a9 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a7 \u200b 0 3 1 \u200b ( \u2223 s 1 \u200b \u2223 c \u200b + \u2223 s 2 \u200b \u2223 c \u200b + \u2223 s 1 \u200b \u2223 c \u2212 t \u200b ) \u200b if c = 0 otherwise \u200b Example 10: Jaro results SOURCE : |s| = |source| = 6 TARGET : |t| = |target| = 6 MATCHING DISTANCE : d = max(6, 6) = 3 COMMON CHARACTERS : m = |re| = |re| = 2 TRANSPOSITIONS : t = 0 STRENGTH : s = 1/3 * ( 2/6 + 2/6 + (2 - 0/2)/2) = 0.55556 SOURCE : |s| = |jono| = 4 TARGET : |t| = |ojhono| = 6 MATCHING DISTANCE : d = max(4, 6) = 3 COMMON CHARACTERS : m = |jono| = |ojon| = 4 TRANSPOSITIONS : t = 4 STRENGTH : s = 1/3 * ( 4/4 + 4/6 + (4 - 4/2)/4) = 0.72222 SOURCE : |s| = |DUANE| = 5 TARGET : |t| = |DWAYNE| = 6 MATCHING DISTANCE : d = max(5, 6) = 3 COMMON CHARACTERS : m = |DANE| = |DANE| = 4 TRANSPOSITIONS : t = 0 STRENGTH : s = 1/3 * ( 4/5 + 4/6 + (4 - 0/2)/4) = 0.82222 SOURCE : |s| = |DIXON| = 5 TARGET : |t| = |DICKSONX| = 8 MATCHING DISTANCE : d = max(5, 8) = 4 COMMON CHARACTERS : m = |DION| = |DION| = 4 TRANSPOSITIONS : t = 0 STRENGTH : s = 1/3 * ( 4/5 + 4/8 + (4 - 0/2)/4) = 0.76667 SOURCE : |s| = |JELLYFISH| = 9 TARGET : |t| = |SMELLYFISH| = 10 MATCHING DISTANCE : d = max(9, 10) = 5 COMMON CHARACTERS : m = |ELLYFISH| = |ELLYFISH| = 8 TRANSPOSITIONS : t = 0 STRENGTH : s = 1/3 * ( 8/9 + 8/10 + (8 - 0/2)/8) = 0.8963","title":"Jaro"},{"location":"04.Algorithms/#jaro-winkler","text":"Given two strings s 1 and s 2 , the Winkler similarity equation boosts up the Jaro algorithm\u2019s result d j by increasing it whenever the compared strings share a prefix of a maximum of four characters. In this shared prefix scenario, the boost is computed as: w b = P l \u2217 P w ( 1 \u2212 d j ) w_b = P_l * P_w ( 1 - d_j ) w b \u200b = P l \u200b \u2217 P w \u200b ( 1 \u2212 d j \u200b ) where P l is the length of the set of shared prefix and P w is a user dependent scaling factor for how much the score is adjusted upwards for having common prefixes. Because four is the maximum number of shared prefix to consider, the user\u2019s choice of P w lies between 0 and \u00bc. Setting P w to \u00bc implies a similarity of always 1 whenever the strings share the maximum of 4 prefixes, no matter the real dissimilarity between the strings. The Jaro-Winkler is computed as: d j w = d j + w b d_{jw}= d_j+ w_b d j w \u200b = d j \u200b + w b \u200b Example 11: Jaro vs Jaro-Winkler jaro ( \"alexander\" , \"alexandrine\" ) = 0.90236 jaro_winkler ( \"alexander\" , \"alexandrine\" , weight = 0.1 ) = 0.94141","title":"Jaro-Winkler"},{"location":"04.Algorithms/#levenshtein","text":"This \u200bmethod is used to align \u200b\u200bsource a\u200bnd \u200b\u200btarget\u2019s IRIs whenever the similarity score of their respective user selected property values are \u200b\u200babove a given \u200bLevenshtein Distance threshold\u200b. Levenshtein (a.k.a. Edit Distance) is a way of quantifying how \u200bdissimilar two strings (e.g., words) are to one another by counting the minimum number of operations \u200b\u03b5 \u200b (\u200bremoval, insertion, or substitution of a character in the string)\u200b required to transform one string into the other. For example, \u200bthe \u200bLevenshtein distance between \u201ckitten\u201d and \u201csitting\u201d is \u200b\u03b5 \u200b= 3 as it requires two substitutions (\u201cs\u201d for \u201ck\u201d and \u201ci\u201d for \u201ce\u201d) and one insertion of \u201cg\u201d at the end. Normalisation \u200b\u03a9\u200b: In the Lenticular Lens , the \u200bsimilarity score \u200b\u03a9 \u200bof a matching pair of resources is quantified in the interval \u200b[0, 1]\u200b. For this, the\u200b dissimilarity score \u200b\u03b5 expressing the \u200bminimum number of operations to perform for \u200btwo strings to be the same \u200bis normalised as \u200b\u03a9 based on the length of the longest string. The \u200bdissimilarity score\u200b\u200b \u03b5\u200b = 3 \u200bbetween \u200b\u201dkitten\u201d and \u201csitting\u201d is then normalised to a \u200bsimilarity score \u03a9\u200b \u200b=\u200b \u200b1 - 3 / 7 = 0.57\u200b.\u200b Minimum threshold \u200b\u03c6: Using this algorithm, \u200ba \u200bminimum threshold value\u03c6\u200b \u200bmust be set in the interval \u200b[0,1], \u200bsuch that finding any matched pairs of IRIs based on the similarity of their respective property values depends on whether or not the computed \u200b\u03a9 is \u200bequal or above \u200b\u03c6\u200b. A threshold \u200b\u03c6 = 1 equates an exact match. In our previous example, if a \u200bminimum threshold of \u200b\u03c6 = \u200b0.7 is set, \u201ckitten\u201d and \u201csitting\u201d will not be matched. \u200bIn short, \u200b\u03c6 is the user defined threshold when the similarity score \u200b\u03a9 is selected for accepting or rejecting a match. Maximum character error threshold \u200b\ud835\udeff:\u200b In case the \u200boriginal edit distance score \u200b\u03b5 (minimum number of operations score) is preferred, the number of character errors \u200b\ud835\udeff \u200bof choice is used instead as threshold for deciding whether a match is accepted or rejected. However, for consistency purposes, the corresponding normalisation value \u200b\u03a9 is still computed for the minimum number of operations score computed \u200b\u03b5\u200b. For instance, in our previous example, if a \u200bmaximum \u200bcharacters errors i\u200bs set to \u200b\ud835\udeff = 3, \u201ckitten\u201d and \u201csitting\u201d will be matched but the computed strength will be \u200b\u03a9 = \u200b0.57 a\u200bnd \u200bnot \u200b\u03b5 = 3 as is it only serves the purpose of decision maker. I\u200bn short, \u200b\ud835\udeff i\u200bs the user defined threshold when the dissimilarity score \u03b5\u200b \u200bis selected for accepting or rejecting a match. Example 12 : The Levenshtein options 1. l_dist(Rembrand van Rijn, Rembrandt Harmensz van Rijn) = 10 2. Normalised_l_dist \u200b(Rembrand van Rijn, Rembrandt Harmensz van Rijn) = 0.63 >>> If \u200b\u03c6 (Minimum threshold) = 0.7 >>> t\u200bhen \u200b[\u200bRembrand van Rijn] owl:sameAs [Rembrandt Harmensz van Rijn] >>> is rejected\u200b >>> because \u03a9\u200b = 0.63 < \u03c6\u200b. >>> If \u200b\u03b4 (Maximum character error threshold) = 5 >>> t\u200bhen [\u200b\u200bRembrand van Rijn] and [Rembrandt Harmensz van Rijn] is \u200brejected >>> because \u03b5 \u200b= 10 >\u200b \u200b\u03b4.","title":"Levenshtein"},{"location":"04.Algorithms/#soundex-approximation","text":"This option is performed in two phases. First, the string values of the user selected properties are normalised (converted to a soundex code). Then a user selected matcher (Levenshtein, Jaro, jaro-Winkler\u2026) is run over the normalised string (Soundex code). Example 13: Links found using soundex In the example below, given ex:E1 and ex:E2 , the matching strength using Edit Distance is 1 - \u215b = 0.875. When directly performing an Edit distance match or a Soundex, we respectively get 0.625 and 0 as strengths. # ------------------------------- # INPUT SIZE 3 SIZE 4 # ------------------------------- # Carretta C630 C6300 # Kareta K630 K6300 ex: E1 rdfs: label \"Carretta\" . ex: E2 rdfs: label \"Kareta\" . ex: ExactLinkset { << ex: E1 skos: closeMatch <ex:E2> > ll: hasMatchingStrength 0.875 . }","title":"Soundex Approximation"},{"location":"04.Algorithms/#approx-over-same-soundex","text":"In the Lenticular Lens , Soundex can also be used as a first filtering step (string normalisation). This means that first we select all pairs sharing the same Soundex code. Then, for each of these pairs an extra string sequence matching algorithm such as (Levenshtein, Jaro, Jaro-Winkler\u2026) is applied to the original pair of matched strings (not the Soundex codes) to determine the strength of the match in the interval ]0, 1]. Example 14 : Generating a linkset using the Soundex algorithm. In the table below, the normalisation of both Louijs Rocourt and Lowis Ricourt becomes L200 R263 leading to an edit distance of 0 and a relative strength of 1. However, computing the same names using directly an edit distance results in an edit distance of 3 and a relative matching strength of 0.79. The example below shows the implementation of Soundex Distance in the Lenticular Lens and how it compares with Edit Distance over the original names ( no soundex-based normalisation ). ------------------------------------------------------------------------------------------------------------------------------------------------------ Source Target E . Dist Rel . distance Source soundex Target soundex Code E . Dist Code Rel . Dist ------------------------------------------------------------------------------------------------------------------------------------------------------ Jasper Cornelisz . Lodder Jaspar Cornelisz Lodder 2 0.92 J 216 C 654 L 360 J 216 C 654 L 360 0 1.0 Barent Teunis Barent Teunisz gen . Drent 12 0.52 B 653 T 520 B 653 T 520 G 500 D 653 10 0.47 Louijs Rocourt Louys Rocourt 2 0.86 L 200 R 263 L 200 R 263 0 1.0 Louijs Rocourt Lowis Ricourt 3 0.79 L 200 R 263 L 200 R 263 0 1.0 Louys Rocourt Lowis Ricourt 3 0.77 L 200 R 263 L 200 R 263 0 1.0 Cornelis Dircksz . Clapmus Cornelis Clapmuts 10 0.6 C 654 D 620 C 415 C 654 C 415 5 0.64 Geertruydt van den Breemde Geertruijd van den Bremde 4 0.85 G 636 V 500 D 500 B 653 G 636 V 500 D 500 B 653 0 1.0","title":"Approx over Same-Soundex"},{"location":"04.Algorithms/#bloothooft","text":"This approximation method is specifically tailored for accessing the similarity between a pair of IRIs for which the user selected property values are Dutch names. The algorithm basically normalises the given names by removing or replacing specific characters\u2026. The resulting normalised names are then pairwise compared using the Approximated Levenshtein Distance (see the description of Approximated Levenshtein Distance). Example 15 :","title":"Bloothooft"},{"location":"04.Algorithms/#word-intersection","text":"This approximation method is originally designed to find a subset of words within a larger text. However, it could also be used for any pair of strings regardless of the strings sizes. Several options are available: Whether or not the order in which the words are found is important. Whether or not the computed strength of each word should be approximated or identical. Whether or not abbreviation should be detected. Whether the default stopping character should be used, not used or modified. A threshold on the number of words not approximated/identical. An overall threshold for accepting or rejecting a match. Example 16 : Expectation For example, it can be used for aligning - [Rembrand van Rijn] and [van Rijn Rembrandt] - [Herdoopers anslagh op Amsterdam] and [Herdoopers anslagh op Amsterdam. Den x. may: 1535. Treur-spel.] regardless of the words\u2019 order. Example 1 With argument APPROX set to True, similarity can be found between Rembrand and Rembrandt (MODE-1). However, this is not possible in the inverse scenario (MODE-2). Furthermore, when the order is of no importance (MODE-1) the sequence in which the intersection of words appears is of no importance. SMALL : Rembrand (van) Rijn BIG : Rembrandt Harmensz. van Rijn MODE-1 : APPROX = True and ORDER = False FOUND : ['rijn', 'rembrandt'] STRENGTH : 0.97059 MODE-2 : APPROX = False and ORDER = True and FOUND : [] STRENGTH : 0.0 Example 2 Because the match is performed over exact occurrences (APPROX = False) and the sequence in which the matched words appear in the text is of importance (ORDER = True), elvervelt is the only intersection found in MODE-2 and thereby bringing the similarity strength to 0.333. SMALL : Elvervelt, Henrik van BIG : Henrik van Elvervelt MODE-1 : APPROX = True and ORDER = False FOUND : ['elvervelt', 'henrik', 'van'] STRENGTH : 1.0 MODE-2 : APPROX = False and ORDER = True and FOUND : ['elvervelt'] STRENGTH : 0.333 Example 3 In here all intersected words appear in the same sequence and without mismatch. SMALL : Herdoopers anslagh op Amsterdam BIG : Herdoopers anslagh op Amsterdam. Den x. may: 1535. Treur-spel. MODE-1 : APPROX = True and ORDER = False FOUND : ['herdoopers', 'anslagh', 'op', 'amsterdam'] STRENGTH : 1.0 MODE-2 : APPROX = False and ORDER = True and FOUND : ['herdoopers', 'anslagh', 'op', 'amsterdam'] STRENGTH : 1.0","title":"Word Intersection"},{"location":"04.Algorithms/#list-intersection","text":"This method is better suited for matching events. It helps establishing a relationship between the source and target entities whenever a list of entities from the source dataset intersects another list of entities stemmed from the target dataset. For illustration, suppose that we have the two events in Example-17. (1) Event one, defined by the source event-entities, documents event-entities that lists entities representing persons about to get married. This event is the Intended-Marriage event. (2) Event two is the Marriage event. It lists (i) entities representing persons that got married and (ii) the guests who attended the event. Now that the two events are defined, let us imagine that we would like to find out which of the Intended-Marriage couple fulfilled its will to get married. For this to be true, we assume that a couple with the wish to be wed should end-up being present at a Marriage event, hopefully their own marriage. This means that, for a match to occur, a minimum of two elements from a list from the source must also belong to a list from the target. In other words, a match is to be generated whenever 100% (all) of the elements in a source\u2019s lists intersects a target\u2019s list. In this method, two thresholds are to be defined. The first threshold or similarity-threshold imposes a minimum accepted similarity score (generally in ]0, 1]) when elements stemmed from different lists are compared. Passing this threshold is interpreted as the occurrence of an intersection between elements of two lists. The second threshold, the intersection-threshold expressed in quantity or percentage, denotes the minimum number of intersections that must occur for a link to be created. In practice, looking at Example-17, only events ex: intended-1 and ex:married-2 are a match as the pairs ( Catharina Reminck , Catharina Remink ) and ( Mr. Jean van de Melie , J. van de Melie ) are respectively similar with a score of 0.94 and 0.82. Example 17 : List Intersection A source dataset documenting events as lists of couple with the intention of getting married and a target dataset with list of people at a wedding ceremony. Each of these latter lists is expected to includes the wed couple. --------------------------------------------------------------------------------------------- Source Dataset Target Dataset --------------------------------------------------------------------------------------------- ex: intended-1 ex: ceremony-1 ex: wife \"Catharina Reminck\" ; ex: participant \"Pieter Jas\" ; ex: husband \"Mr. Jean van de Melie\" . ex: participant \"Jacob Poppen\" ; ex: participant \"Gillis Graafland\" ; ex: intended-2 ex: participant \"Jacob Fransz. de Witt\" ; ex: wife \"Eva Oostrom\" ; ex: participant \"Elisabeth van Daaken\" ; ex: husband \"Pieter de Vriest\" . ex: participant \"Catharina Berewouts\" ; ex: participant \"Aafje Hendricx\" ; ex: intended-3 ex: participant \"Anthony van Paembergh\" ; ex: wife \"Eva Oostrom\" ; ex: participant \"Eva van Toorn\" ; ex: husband \"Wiggert van Wesick\" . ex: participant \"Maria Bor\" . ex: married-2 ex: participant \"J. van de Melie\" , \"Bernardus van Vijven\" . \"Margrita Schrik\" , \"Johannes de Bruijn\" , \"Maria Gosina Demol\" , \"Agneta Swartepaart\" , \"Hendrik de Lange\" , \"Catharina Remink\" . ex: intended-4 ex: ceremony-2 ex: wife \"Catharina Elisabet\" ; ex: participant \"Pieter Robid\u00e9\" , ex: husband \"Pieter Robid\u00e9\" . \"Jacob Fredrik\" , \"Petronella Hobbe\" , \"Catharina Maria Boom\" , \"Pieter Robie\" , \"Fredrik Arentse\" . With a Similarity-threshold set to 0.8 and an Intersection-threshold set to 2 or 100%, only events ex:intended-1 and ex:married-2 are a match as the pairs ( Catharina Reminck , Catharina Remink ) and ( Mr. Jean van de Melie , J. van de Melie ) are respectively similar with similarity scores of 0.94 and 0.82.","title":"List Intersection"},{"location":"04.Algorithms/#team","text":"The TeAM (Text Approximation Match) method allows for the approximation of the relevance of a document to a query. Such approximation can be done using lexical similarity (word level similarity), semantic similarity or hybrid similarities. In this method, the focus is rather on the lexical similarity. Although tailored to text, it has been adapted to also be applicable to name-based similarity. We now provide an overview of the motivation context supporting the design and implementation of the algorithm. The Amsterdam\u2019s city archives (SAA) possesses physical handwritten inventories records where a record may be for example an inventory of goods (paintings, prints, sculpture, furniture, porcelain, etc.) owned by an Amsterdamer and mentioned in a last will. Interested in documenting the ownership of paintings from the 17 th century, the Yale University Professor John Michael Montias compiled a database by transcribing 1280 physical handwritten inventories (scattered in the Netherlands) of goods. Now that a number of these physical inventories have been digitised using handwriting recognition, one of the goals of the Golden Agent project is to identify Montias\u2019 transcriptions of painting selections within the digitised inventories. This problem can be generically reformulated as, given a source-segment database (e.g. Montias DB) and a target-segment database (e.g. SAA) , find the best similar target segment for each source segment . Fig 4.9: Digitisation of inventory documents available at the Amsterdam\u2019s city archives. Check TeAM for a more detailed explanation.","title":"TeAM"},{"location":"04.ResultsCombination/","text":".katex img { display: block; position: absolute; width: 100%; height: inherit; } Understanding Matching Results \u00b6 Work in progress This section addresses the problem of understanding the results of Matching Algorithms and Matching Methods supported in the Lenticular Lens tool and also how to correctly combine them. The former corresponds to a set of rules followed by a computer for finding pairs of matching resources. The latter applies the former to generate matching results. One is expected to (i) choose a similarity algorithm, (ii) provide the required input (datasets, entity-type and property-value restrictions, matching properties\u2026) and (iii) provide the conditions (threshold) under which a matched pair is outputted along with its score/weight. Matching methods involving a single entity matching algorithm are distinguished from those involving more than one: simple versus complex method (not to be confused with the complexity of the underlying the matching algorithm). The latter, naturally, requires the results to be combined. Although combining matching results may seem relatively easy, deciding on the final score and the annotations of links with weights requires some extra thoughts. First of all, it requires deciding what a matching score is about: degree of truth or degree of confidence ? In order words, is this a problem of Vagueness or Uncertainty or both? (see Sect. 1). Answering the later question opens the doors to Section 2 where we unveil our take on \u201c how to combine scores? \u201d. In particular, we distinguish the problems into (i) \u201c how to combine degrees of truth? \u201d, (ii) \u201c how to combine degrees of confidence? \u201d and finally (iii) \u201c how/when to transition from degree of truth to degree of confidence? \u201d. 1. Vagueness vs. Uncertainty \u00b6 This section addresses the issue of \u201c how to interpret the various scores in the process of entity matching? \u201d by digging into \u201c What is vagueness and degrees of truth and how are they different from uncertainty and degrees of confidence? \u201d. Roughly, truth value or degrees of truth ( the tomato is ripe with degree of truth 0.7 ) is not to be confused with degrees of certainty / confidence ( I am 0.9 certain that the tomato is ripe with degree of truth 0.7 ) as the latter is not an assignment of truth value as opposed to the former, but rather an evaluation of a weighted proposition regardless of its truth value space ({0, 1} or [0, 1]). In the next subsections, the distinction between Vagueness and Uncertainty is outlined based on the interpretation we associate to the output scores of similarity algorithms and matching methods. 1.1 Scores of similarity algorithms \u00b6 Vagueness & Degrees of Truth Similarity algorithms (Levenshtein, Soundex, Cosine\u2026) meant for computing the overlap between a pair of input-arguments (property-value overlap) output values in the unit interval [0, 1] or values convertible in the unit interval (normalisation). These values are truth values / degrees of truth . For example, the input-arguments \u201cRembrand van Rijn\u201d and \u201cRembrandt Harmensz van Rijn\u201d have a similarity degree of truth of 0.63 using the Levenshtein algorithm and a similarity degree of truth of 0.74 using the Soundex algorithm. Transitioning from assigning boolean truth values {0, 1} to assigning continuous truth values in the unit interval [0, 1] to propositions (events) is clearly moving from classical logic to fuzzy logic which is in the modelling paradigm of many-valued Logics . The latter truth space (unit interval) is motivated by the presence of vague concepts in a proposition (the use of ripe in the proposition \u201c the tomato is ripe \u201d), making it hard or sometimes even impossible to establish whether the proposition is completely true or false [ Lukasiewicz2008 ]. 1.2 Scores of matching methods \u00b6 Uncertainty & Degree of confidence Whatever truth value is assigned to a proposition or event, sometimes, one may wonder about the likelihood that the event will occur or has occurred [ Kovalerchuk2017 ]. This reflects the uncertainty regarding the statement due to, for example, lack of information. In these cases, the use of theories such as Probabilistic or Possibilistic Logics can be considered for the evaluation of the likelihood of a proposition. In other words, the use of degree of (un)certainty, belief or confidence is to emphasise a confidence-evaluation on the assignment of a truth value to a proposition, which may or may not qualify as vague ( the tomato is ripe, the sky is blue, the bottle is full\u2026 ). For example, how certain are we in asserting that the proposition \u201cthe tomato is ripe\u201d is true ? As [Lukasiewicz2008] illustrates, asserting that \u201cJohn is a teacher\u201d and \u201cJohn is a student\u201d with respectively the values 0.3 and 0.7 as degrees of certainty is roughly saying that \u201cJohn is either a teacher or a student, but more likely a student\u201d . However, the vague statement \u201c John is tall\u201d with the assignment of 0.9 as degree of truth can be roughly translated as \u201cJohn is quite tall\u201d rather than as \u201cJohn is likely tall\u201d . Entity matching methods 1 output links optionally annotated with matching scores reflecting (i) the level of confidence of a method (the strength of the evidence) for assimilating a pair of resources to be co-referents and (ii) the lower boundary under which two resources can be viewed as co-referents. For example, given the resources e 1 e_1 e 1 \u200b and e 2 e_2 e 2 \u200b respectively labeled Rembrand van Rijn and Rembrandt Harmensz van Rijn , e 1 e_1 e 1 \u200b and e 2 e_2 e 2 \u200b can be linked with a matching score of 0.63 using the Levenshtein algorithm, provided that it is acceptable doing so at a matching score above 0.60 (more details on how this confidence score can be calculated is provided in Section 2.3 ). 2. Combination of the Scores \u00b6 Even though degree of truth (scores of similarity algorithms) and degree of confidence (scores of matching methods) are not be confused, they may be combined (multiple truth values or multiple degrees of confidence) or they me be subject to transition. In fact, it may be almost unthinkable to solve real-life-problems without doing so. Considering the famous abductive reasoning example of the duck test where something is probably is a duck if it (i) looks like a duck, (ii) swims like a duck, (iii) and quacks like a duck . This requires first a combination of truth values associated to propositions (i), (ii) and (iii) as conjunction (see Sect. 2.1), followed by a transition into a confidence value concluding that it is probably a duck (see Sect. 2.3). 2.1 Combining Truth Values \u00b6 One thing we know and agreed on is that similarity algorithms output boolean or fuzzy truth values in the range [0, 1]. This allows to make use of logic combination functions offers in conventional logic (Subsection 2.1.1) of fuzzy logic (Subsection 2.1.2) depending on whether we expect the solution space to be {0, 1} or [0, 1]. 2.1.1 Classic Logic \u00b6 The two standard logic operators or combination functions traditionally used are the classical Boolean Conjunction ( \u2227 \\land \u2227 ) and Disjunction ( \u2228 \\lor \u2228 ) . The first takes the minimum strength and the latter takes the maximum strength. This applies for both classic values (True -1- or False -0-) and fuzzy values ( between 0 and 1 ). Since the results from matching methods are assigned fuzzy values in the interval ]0,1], the table bellow illustrates the default behaviour of the Lenticular Lens when combining them. Example 18: Standard logic operations over conjunction (min) and disjunction (max). Source Target Levenshtein Soundex OR(max) AND(min) ------------------------------------------------------------------------------------------------------ Jasper Cornelisz. Lodder Jaspar Cornelisz Lodder 0.92 1.00 1.00 0.92 Rembrand van Rijn Rembrandt Harmensz van Rijn 0.63 0.74 0.74 0.63 Barent Teunis Barent Teunisz gen. Drent 0.52 0.47 0.52 0.47 2.1.2 Fuzzy Logic \u00b6 As provided in the next subsections, sophisticated combination functions such as T-norms ( \u2297 \\otimes \u2297 ) and S-norms ( \u2295 \\oplus \u2295 ) , developed by scholars like \u0141ukasiewicz, G\u00f6del, Goguen, Zadeh and others can also be used as alternatives for respectively Boolean Disjunction ( \u2228 \\lor \u2228 ) and Conjunction ( \u2227 \\land \u2227 ) . 2.1.2.1 T-norms \u00b6 A list of six different operations can be applied when dealing with methods combined by Conjunction . Here, we present them: Minimum t-norm \u22a4 m i n ( a , b ) = m i n ( a , b ) (1) \u22a4_{min} (a, b) = min(a, b) \\tag{1} \u22a4 m i n \u200b ( a , b ) = m i n ( a , b ) ( 1 ) Product t-norm \u22a4 p r o d ( a , b ) = a . b (2) \u22a4_{prod} (a, b) = a \\text{ . } b \\tag{2} \u22a4 p r o d \u200b ( a , b ) = a . b ( 2 ) \u0141ukasiewicz t-norm \u22a4 L u k ( a , b ) = m a x ( 0 , a + b \u2212 1 ) (3) \u22a4_{Luk} (a, b) = max(0, a + b - 1) \\tag{3} \u22a4 L u k \u200b ( a , b ) = m a x ( 0 , a + b \u2212 1 ) ( 3 ) Drastic \u22a4 D ( a , b ) = { b if a = 1 a if b = 1 0 otherwise (4) \u22a4_D(a, b) = \\begin{cases} b &\\text{if } a = 1 \\\\ a &\\text{if } b = 1 \\\\ 0 &\\text{otherwise} \\end{cases} \\tag{4} \u22a4 D \u200b ( a , b ) = \u23a9 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a7 \u200b b a 0 \u200b if a = 1 if b = 1 otherwise \u200b ( 4 ) Nilpotent minimum \u22a4 n M ( a , b ) = { m i n ( a , b ) if a + b > 1 0 otherwise (5) \u22a4_{nM}(a, b) = \\begin{cases} min(a, b) &\\text{if } a + b > 1 \\\\ 0 &\\text{otherwise} \\end{cases} \\tag{5} \u22a4 n M \u200b ( a , b ) = { m i n ( a , b ) 0 \u200b if a + b > 1 otherwise \u200b ( 5 ) Hamacher product \u22a4 H 0 ( a , b ) = { 0 if a = b = 0 a b a + b \u2212 a b otherwise (6) \u22a4_{H_0}(a, b) = \\begin{cases} 0 &\\text{if } a = b = 0 \\\\ \\dfrac{ab}{a + b - ab} &\\text{otherwise} \\end{cases} \\tag{6} \u22a4 H 0 \u200b \u200b ( a , b ) = \u23a9 \u23aa \u23a8 \u23aa \u23a7 \u200b 0 a + b \u2212 a b a b \u200b \u200b if a = b = 0 otherwise \u200b ( 6 ) The following table provides three case studies to illustrate the application of each of the aforementioned T-norm binary operations . They are presented in order from the less strict ( \u22a4 min ) to the most strict ( \u22a4 D ). Source, Target Levenshtein, Soundex \u22a4 min \u22a4 H 0 \u22a4 prod \u22a4 nM \u22a4 Luk \u22a4 D Src : Jasper Cornelisz. Lodder Trg : Jaspar Cornelisz Lodder 0.92, 1.00 0.920 0.920 0.920 0.920 0.920 0.920 Src : Rembrand van Rijn Trg : Rembrandt Harmensz van Rijn 0.63, 0.74 0.630 0.516 0.466 0.630 0.370 0.000 Src : Barent Teunis Trg : Barent Teunisz gen. Drent 0.52, 0.47 0.470 0.328 0.244 0.000 0.000 0.000 2.1.2.2 S-norms \u00b6 A list of six different operations can also be applied when dealing with methods combined by Disjunction . Here, we present them: Maximum S-norm \u22a5 m a x ( a , b ) = m a x ( a , b ) (7) \u22a5_{max} (a, b) = max(a, b) \\tag{7} \u22a5 m a x \u200b ( a , b ) = m a x ( a , b ) ( 7 ) Probabilistic sum \u22a5 s u m ( a , b ) = a + b \u2212 a . b (8) \u22a5_{sum} (a, b) = a + b - a \\text{ . } b \\tag{8} \u22a5 s u m \u200b ( a , b ) = a + b \u2212 a . b ( 8 ) Bounded sum \u22a5 L u k ( a , b ) = m i n ( a + b , 1 ) (9) \u22a5_{Luk} (a, b) = min(a + b, 1) \\tag{9} \u22a5 L u k \u200b ( a , b ) = m i n ( a + b , 1 ) ( 9 ) Drastic S-norm \u22a5 D ( a , b ) = { b if a = 0 a if b = 0 1 otherwise (10) \u22a5_D(a, b) = \\begin{cases} b &\\text{if } a = 0 \\\\ a &\\text{if } b = 0 \\\\ 1 &\\text{otherwise} \\end{cases} \\tag{10} \u22a5 D \u200b ( a , b ) = \u23a9 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a7 \u200b b a 1 \u200b if a = 0 if b = 0 otherwise \u200b ( 1 0 ) Nilpotent maximum \u22a5 n M ( a , b ) = { m a x ( a , b ) if a + b < 1 1 otherwise (11) \u22a5_{nM}(a, b) = \\begin{cases} max(a, b) &\\text{if } a + b < 1 \\\\ 1 &\\text{otherwise} \\end{cases} \\tag{11} \u22a5 n M \u200b ( a , b ) = { m a x ( a , b ) 1 \u200b if a + b < 1 otherwise \u200b ( 1 1 ) Einstein sum \u22a5 H 2 ( a , b ) = a + b 1 + a b (12) \u22a5_{H_2} (a, b) = \\dfrac{a + b} {1 + ab} \\tag{12} \u22a5 H 2 \u200b \u200b ( a , b ) = 1 + a b a + b \u200b ( 1 2 ) Source, Target Levenshtein, Soundex \u22a4 D \u22a4 Luk \u22a4 H 2 \u22a4 sum \u22a4 nM \u22a4 max Src : Jasper Cornelisz. Lodder Trg : Jaspar Cornelisz Lodder 0.92, 1.00 1.000 1.000 1.000 1.000 1.000 1.000 Src : Rembrand van Rijn Trg : Rembrandt Harmensz van Rijn 0.63, 0.74 1.000 1.000 0.934 0.904 1.000 0.740 Src : Barent Teunis Trg : Barent Teunisz gen. Drent 0.52, 0.47 1.000 0.990 0.796 0.746 0.520 0.520 2.1.3 Examples \u00b6 Suppose that, two data items E-1 and E-2 have the following information: E-1 Name : Titus Rembrandtsz. van Rijn Mother : Saskia Uylenburgh Father : Rembrand van Rijn Parent\u2019s Marriage date : 1644-06-22 E-2 Name : T. Rembrandtszoon van Rijn Mother : Saske van Uijlenburg Father : Rembrandt Harmensz van Rijn Baptism date :1641-09-22 To interpret E-1 and E-2 as representing co-referent persons, the following four tests are proposed. Test-1 OR Here, the names of E-1 and E-2 are to be compared using the Levenshtein and Soundex algorithms at a threshold of at least 0.7 . MATCHING RESULTS - Levenshtein(Titus Rembrandtsz van Rijn, T. Rembrandtszoon van Rijn) => 0.73 \u2705 - sdx_1 = Soundex(Titus Rembrandtsz van Rijn) = T320 R516 V500 R250 - sdx_2 = Soundex(T. Rembrandtszoon van Rijn) = T000 R516 V500 R250 - Levenshtein(sdx_1, sdx_2) => 0.89 \u2705 DISJUNCTIONS RESULTS - names similarity = S-norm(0.73, 0.88, 'MAXIMUM') => 0.89 \u2705 - names similarity = S-norm(0.73, 0.88, 'PROBABILISTIC') => 0.97 \u2705 Test-2 AND Names of the postulated mothers and fathers are to be similar at a threshold of at least 0.6 using the Levenshtein algorithm. MATCHING RESULTS - Levenshtein(Saskia Uylenburgh, Saske van Uijlenburg) => 0.65 \u2705 - Levenshtein(Rembrand van Rijn, Rembrandt Harmensz van Rijn) => 0.63 \u2705 CONJUNCTION RESULTS - Parent's names similarity = t_norm(0.65, 0.63, 'MINIMUM') => 0.63 \u2705 - Parent's names similarity = t_norm(0.65, 0.63, 'HAMACHER') => 0.47 \u274c Test-3 The period between the parent\u2019s marriage date on the one side and the child\u2019s baptism date on the other side are to be no more than 25 years apart . MATCHING RESULTS - Delta(1668-02-28, 1669-03-22, 25) => 1.00 \u2705 Test-4 AND Combining all above three tests considering a the conjunction fuzzy operator should result in a similarity score above or equal to 0.8. -------------------------------------------------------------------------------------- FINAL CONJUNCTIONS WITH A TRUTH VALUE LIST OF [0.850, 0.63, 1] -------------------------------------------------------------------------------------- - t_norm_list([0.850, 0.63, 1], 'MINIMIUM') => 0.63 \u274c - t_norm_list([0.850, 0.63, 1], 'HAMACHER') => 0.58 \u274c - t_norm_list([0.850, 0.63, 1], 'PRODUCT') => 0.56 \u274c - t_norm_list([0.850, 0.63, 1], 'NILPOTENT') => 0.63 \u274c - t_norm_list([0.850, 0.63, 1], '\u0141uk') => 0.52 \u274c - t_norm_list([0.850, 0.63, 1], 'DRASTIC') => 0.00 \u274c -------------------------------------------------------------------------------------- CONJUNCTIONS WITH A DIFFERENT LIST OF TRUTH VALUES [0.89, 0.82, 1] -------------------------------------------------------------------------------------- - t_norm_list([0.89, 0.82, 1], \"MINIMUM\") => 0.82 \u2705 - t_norm_list([0.89, 0.82, 1], \"HAMACHER\") => 0.74 \u274c - t_norm_list([0.89, 0.82, 1], \"PRODUCT\") => 0.73 \u274c - t_norm_list([0.89, 0.82, 1], \"NILPOTENYT\") => 0.82 \u2705 - t_norm_list([0.89, 0.82, 1], \"LUK\") => 0.71 \u274c - t_norm_list([0.89, 0.82, 1], \"DRASTIC\") => 0.0 \u274c -------------------------------------------------------------------------------------- EXAMPLE USING MORE THAN ONE FUZZY LOGIC OPERATOR -------------------------------------------------------------------------------------- - Ops.t_norm(Ops.t_norm(0.850, 0.63, 'HAMACHER'), 1, 'MINIMIUM') => 0.57 \u274c - Ops.t_norm(Ops.t_norm(0.850, 0.63, 'MINIMIUM'), 1, 'HAMACHER') => 0.63 \u274c Conclusion : Given the evidence provided for E-1 and E-2 and the rules described above, the interpretation resulting from the chosen fuzzy logic operations leads to the conclusion that there is no sufficient evidence to infer that the underlying data items are co-referent. This rejection is mainly due to the low similarity of the parents\u2019 names. If the resulting similarity were above 0.8, there would then be a better chance for the data items to be co-referent. Keep in mind that our conjectured rule asserts an identity relation for combination of scores only when above 0.8. A better data or more advanced algorithm could have helped. 2.2 Combining Confidence Values \u00b6 Understanding how to combine uncertain events starts by a better understanding of uncertainty itself. Sentz et al. provide two important distinctions of uncertainty: Aleatory (Objective Uncertainty originated from random behaviour) or Epistemic (Subjective Uncertainty originated from ignorance or lack of knowledge). Whereas traditional probability is clearly applicable to deal with Aleatory Uncertainty , researchers claim its inability to deal with Epistemic Uncertainty. In short this is because the latter neither implies knowing probability of all relevant events, nor its uniform distribution, not even the axiom of additivity (i.e. all probabilities summing up to 1). This leads to the emergence of more general representations of uncertainty as alternatives to the traditional probability theory, such as imprecise probabilities , possibility theory and evidence theory . Nonetheless, at present, there is no clear best representation of uncertainty [ Sentz2002 ]. This section introduces alternative representations of uncertainty that are planned to be implemented in the Lenticular Lens. Although their choice can be ultimately a choice of the user, we consider the problem of co-reference search by applying multiple matching methods to be a case of Epistemic Uncertainty nicely approached in evidence theory. 2.2.1 Probabilistic Logic \u00b6 Using probability for combining confidence values with the logic operators \u201cAND\u201d and \u201cOR\u201d can be done in the context of link manipulation in theory with the equations (13) and (14) respectively with the strong assumption that the events to be combined are independent ( the occurrence of one event has no effect on the probability of the occurrence of the other ). P ( A and B ) = P ( A ) \u2005 \u22c5 P ( B ) (13) P(\\text{A and B}) = P(A) \\: \\cdotp P(B) \\tag{13} P ( A and B ) = P ( A ) \u22c5 P ( B ) ( 1 3 ) P ( A or B ) = P ( A ) + P ( B ) \u2212 P ( A and B ) where P ( A and B ) = 0 if A and B are mutually exclusive events meaning that these events have no outcomes in common. (14) P(\\text{A or B}) = P(A) + P(B) - P(\\text{A and B}) \\tag{14} \\\\ \\footnotesize \\text{where } P(\\text{A and B}) = 0 \\text{ if A and B are mutually exclusive events} \\\\ \\text{ meaning that these events have no outcomes in common.} P ( A or B ) = P ( A ) + P ( B ) \u2212 P ( A and B ) where P ( A and B ) = 0 if A and B are mutually exclusive events meaning that these events have no outcomes in common. ( 1 4 ) On the one hand, assuming that events A and B are independent , equations (13) and (14) are somewhat similar to \u2297 \\otimes \u2297 ( \u22a4 p r o d ( a , b ) = a . b ) \\big(\u22a4_{prod}(a, b) = a \\text{ . } b\\big) ( \u22a4 p r o d \u200b ( a , b ) = a . b ) and \u2295 \\oplus \u2295 ( \u22a5 s u m ( a , b ) = a + b \u2212 a . b ) \\big(\u22a5_{sum}(a, b) = a + b - a \\text{ . } b\\big) ( \u22a5 s u m \u200b ( a , b ) = a + b \u2212 a . b ) of Product Logic hence straightforward to implement when in the need of manipulating links such as applying an intersection or a union operator to two sets of links for example. On the other hand, in the event that A and B are not independent , the value of P ( A and B ) P(\\text{A and B}) P ( A and B ) should be observable n ( A and B ) n ( S a m p l e ) \\frac{n(A\\text{ and } B)}{n(Sample)} n ( S a m p l e ) n ( A and B ) \u200b , provided or computed using conditional probability in Equation 15. \ud835\udc43 ( A and B ) = \ud835\udc43 ( \ud835\udc34 ) . \ud835\udc43 ( \ud835\udc35 \u2223 \ud835\udc34 ) (15) \ud835\udc43(\\text{A and B})=\ud835\udc43(\ud835\udc34)\\text{ . }\ud835\udc43(\ud835\udc35|\ud835\udc34) \\tag{15} P ( A and B ) = P ( A ) . P ( B \u2223 A ) ( 1 5 ) In the context of the Lenticular Lens, the computed confidence values are independent (the computation of a confidence value by method-1 has no effect on the computation of a confidence value by method-2). This being said, equation (15) is not applicable. 2.2.2 Possibilistic Logic \u00b6 Possibility 2 is compositional with respect to the union operator as the possibility of the union is deducible from the possibility of each component. Note however that it is not compositional with respect to the intersection operator. p o s ( A \u222a B ) = m a x ( p o s ( A ) , \u2005 p o s ( B ) ) \u2005 \u2005 \u2005 \u2005 for any subsets A and B (16) pos(A \\cup B) = max(pos(A), \\: pos(B)) \\:\\:\\:\\: \\text{\\footnotesize for any subsets A and B} \\tag{16} p o s ( A \u222a B ) = m a x ( p o s ( A ) , p o s ( B ) ) for any subsets A and B ( 1 6 ) p o s ( A \u2229 B ) \u2264 m i n ( p o s ( A ) , \u2005 p o s ( B ) ) \u2264 m a x ( p o s ( A ) , \u2005 p o s ( B ) ) (17) pos(A \\cap B) \\leq min(pos(A), \\: pos(B)) \\leq max(pos(A), \\: pos(B)) \\tag{17} p o s ( A \u2229 B ) \u2264 m i n ( p o s ( A ) , p o s ( B ) ) \u2264 m a x ( p o s ( A ) , p o s ( B ) ) ( 1 7 ) 2.2.3 Evidence Theory \u00b6 Evidence theory also provide different ways for combining uncertain scores. We present here two of them: starting from the first proposal, namely Dempster-Shafer , which is shown to have limitations, followed by a more accepted approach called average . 2.2.3.1 Dempster-Shafer \u00b6 Combining or aggregating confidence values associated with evidence is made possible with the Dempster-Shafer conjunctive combination rule as presented in Equation 18. Here too, the assumption of independence among sources providing supporting or conflicting assessments for the same frame of discernment [ Sentz2002 ]) is of key-importance and the basic assumption supporting the Dempster-Shafer combination rule. However, a crucial context dependent limitation of this rule as pointed out by [Zadeh, 1984], occurs in cases with significant conflicts because the denominator in the Dempster-Shafer combination rule has the effect of completely ignoring conflict while the numerator emphasis agreement , thereby yielding result inconsistency (unintuitive). m 12 ( A ) = ( m 1 \u2295 m 2 ) ( A ) = supporting evidence 1 \u2212 conflicting evidence = \u2211 B \u2229 C = A \u2260 \u2205 m 1 ( B ) m 2 ( C ) 1 \u2212 \u2211 B \u2229 C = \u2205 m 1 ( B ) m 2 ( C ) (18) m_{12}(A) = (m_1 \\oplus m_2)(A) = \\footnotesize{\\frac{\\text{supporting evidence}}{1 - \\text{conflicting evidence}}} = \\frac{ \\displaystyle\\sum_{B \\cap C = A \\ne \\emptyset} m_1(B) m_2(C) }{1 - \\displaystyle\\sum_{B \\cap C = \\emptyset} m_1(B) m_2(C) } \\tag{18} m 1 2 \u200b ( A ) = ( m 1 \u200b \u2295 m 2 \u200b ) ( A ) = 1 \u2212 conflicting evidence supporting evidence \u200b = 1 \u2212 B \u2229 C = \u2205 \u2211 \u200b m 1 \u200b ( B ) m 2 \u200b ( C ) B \u2229 C = A \ue020 = \u2205 \u2211 \u200b m 1 \u200b ( B ) m 2 \u200b ( C ) \u200b ( 1 8 ) where { m Basic probability assignment (bpa) function for the universal set P(X) to [0, 1] . m ( A ) The bpa value or the mass of a given set A but not for a particular subset of A . m 1 , m 2 Two given basic probability assignments . m 12 ( A ) The combination a.k.a the joint m 12 . m ( \u2205 ) = 0 The mass of the empty set is zero. \u2211 A \u2208 P ( X ) m ( A ) = 1 The masses of all the members of the power set add up to a total of 1. \\scriptsize \\text{where } \\begin{cases} m &\\text{Basic probability assignment (bpa) function for the universal set P(X) to [0, 1]}. \\\\ m(A) &\\text{The bpa value or the mass of a given set A but not for a particular subset of A}. \\\\ m_1 \\text{, } m_2 &\\text{Two given basic probability assignments}. \\\\ m_{12}(A) &\\text{The combination a.k.a the joint } m_{12}. \\\\ m(\\emptyset) = 0 &\\text{The mass of the empty set is zero.} \\\\ \\displaystyle\\sum_{A\\in P(X)} m(A) = 1 &\\text{The masses of all the members of the power set add up to a total of 1.} \\end{cases} where \u23a9 \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23a8 \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23a7 \u200b m m ( A ) m 1 \u200b , m 2 \u200b m 1 2 \u200b ( A ) m ( \u2205 ) = 0 A \u2208 P ( X ) \u2211 \u200b m ( A ) = 1 \u200b Basic probability assignment (bpa) function for the universal set P(X) to [0, 1] . The bpa value or the mass of a given set A but not for a particular subset of A . Two given basic probability assignments . The combination a.k.a the joint m 1 2 \u200b . The mass of the empty set is zero. The masses of all the members of the power set add up to a total of 1. \u200b This inconsistency is higgled in Fig. 1: Patient Diagnose (1) where the joint agreement on the patient\u2019s condition results to a 1 using Dempster\u2019s combination rule although the doctors agreed that the patient is less-likely to suffer from a brain tumour. Has it been the opposite scenario as in Fig. 1: Patient Diagnose (2) (Dr. Green and Dr. House assigning 0.99 as the basic probability for the patience suffering from a brain tumour), the result of m 12 ( b r a i n T u m o r ) = 1 \\scriptsize m_{12}(brainTumor)=1 m 1 2 \u200b ( b r a i n T u m o r ) = 1 would be consistent with our intuition. Fig. 1: Combining two doctors\u2019 diagnosis of a patient using the Dempster-Shafer combination rule. The results in (1) and (2) highlight the context dependent inconsistency of the implementation of the rule. 2.2.3.2 Averaging \u00b6 Many alternatives to Equation 18 have been proposed by scholars such as Yager (modified Dempster\u2019s rule), Inagaki (modified combination rule), Zhang (center combination rule), Dubois and Prade (disjunctive consensus rule), Ferson and Kreinovich (averaging), etc. One particular approach called Averaging (Equation 19) is considered to produced better outcomes [ Choi2009 ] and therefore more likely to be implemented in the Lenticular Lens for combining confidence values, such as lens operations union and intersection . It provides means to calculate an average of several ( n ) sources while also taking into account possible reliability weights attributed to each source. m 1... n ( A ) = 1 n \u2211 i = 1 n w i m i ( A ) (19) m_{1...n}(A) = \\frac{1}{n} \\sum_{i=1}^{n} w_im_i(A) \\tag{19} m 1 . . . n \u200b ( A ) = n 1 \u200b i = 1 \u2211 n \u200b w i \u200b m i \u200b ( A ) ( 1 9 ) Where { n Number of sources . w i Reliability weight of the source . m i Basic probability assignment of a body of evidence . \\scriptsize \\text{Where } \\begin{cases} n &\\text{Number of sources}. \\\\ w_i &\\text{Reliability weight of the source}. \\\\ m_i &\\text{Basic probability assignment of a body of evidence}. \\\\ \\end{cases} Where { n w i \u200b m i \u200b \u200b Number of sources . Reliability weight of the source . Basic probability assignment of a body of evidence . \u200b Applying Equation 19 to the two scenarios illustrated in Fig. 1 will yield the following results: Patient Diagnosis (1): m 1 , 2 ( b r a i n T u m o r ) = 0.01 + 0.01 2 = 0.01 \\small m_{1,2}(brainTumor) = \\frac{0.01 + 0.01}{2} = 0.01 m 1 , 2 \u200b ( b r a i n T u m o r ) = 2 0 . 0 1 + 0 . 0 1 \u200b = 0 . 0 1 Patient Diagnosis (2): m 1 , 2 ( b r a i n T u m o r ) = 0.99 + 0.99 2 = 0.99 \\small m_{1,2}(brainTumor) = \\frac{0.99 + 0.99}{2} = 0.99 m 1 , 2 \u200b ( b r a i n T u m o r ) = 2 0 . 9 9 + 0 . 9 9 \u200b = 0 . 9 9 2.3 From Truth to Confidence \u00b6 Similarly to the duck test, the modelling of ways to find supporting evidence for isolating potential entity matching candidates is crucial to inferring identity for a pair of resources. Section 2.1 already covers our take on how to combine truth values and Section 2.2 covers the combination of degrees of confidence. What now remains is to understand \u201c How to transition from truth value to uncertainty / degree of confidence? \u201d. For illustration purpose, this means for example, how to move from [looks like a duck (0.8), swims like a duck (1.0), and quacks like a duck (0.95)] to [it probably is a duck (???)]. Applying \u22a4 p r o d \u22a4_{prod} \u22a4 p r o d \u200b to the evidence truth values results in a truth value of 0.76. Assuming that a transition from the evidence truth value to the degree of confidence carries a weight of 1, we argue that it is now possible to extrapolate a confidence of 0.76 for asserting that the entity that looks like a duck (0.8), swims like a duck (1.0), and quacks like a duck (0.95) is probably (0.76) a duck. Similarly, if this transition weight is now reset to 0.9 for example, it is also reasonable that the degree of confidence computed for the entity being a duck drops to 0.684. 2.4 Implementation \u00b6 There are several operations in the Lenticular Lens in which one or more of the above discussed values and their combinations take place. We summarise them here, including also the possibilities of future improvements to allow for more control over the produced values. Link Construction Simple Macthing Method Transition from Truth to Confidence Degree Example : If the entities\u2019 names sound alike with degree of truth above 0.6, then the resources are probably the same with the sound-alike * 1 as degree of confidence. Complex Macthing Method Combination of truth values via logic-boxes followed by Transition from Truth to Confidence Degree Example : If the entities\u2019 names sound alike with a degree of truth above 0.6 OR look alike with a degree of truth above 0.7, AND the date of birth is the same, then final degree of truth using classical OR/AND combinations is min(max(sound-alike, look-alike), same-birth) and the resources are probably the same with the final degree of truth as degree of confidence. Currenlty a fix transition-weight of 1 applied, so that the final score (currenlty only one is outputted) reflects both degrees of truth and confidence. Improvements: Explicit output degrees of truth, confidence and transition-weight; Allow for the user to decide on the transition-weight applied, so that low-power identity criteria such as the example in simple method would not conclude that high name similarity means high confidence. Link Manipulation 1. Union Disjunction of the final degrees of truth, followed by the re-assignment of the confidence value given a transition-weight. Possible combinations: classic OR S-norms Disjunction of the final degrees confidence Possible combinations: Probabilistic OR Possibilitic Union Averaging 2. Intersection Conjunction of the final degrees of truth, followed by the re-assignment of the confidence value given a transition-weight Possible combinations: classic AND T-norms Conjunction of the final degrees confidence Possible combinations: Probabilistic AND Possibilitic Intersection Averaging is applicable provided that there is agreement between all sources, i.e. all the sources have produced a degree of confidence above 0 for the link under scrutiny. 3. Difference This operation does not require combination of values, but simply selects the matches that do not occur in another link-set. 4. Composition Transitivity over the final degress of truth ??? Composition of confidence attributed to independent events ??? 5. In Set This operation does not require combination of values, but simply selects the matches of which resources do occur in a given resource-set. Currently, only the combination of the truth values are implemented, with the transition-weight equal to 1. Improvements: allowing for the user to decide which values to combine and how, plus what transition-weight to (re)apply if needed would render the system more flexible. Link Validation On top of the automatically calculated dregre of confidence discussed so far, the manual validation allows for the user to attribute its own confidence. Currently, such manual attribution consists of simply accepting or rejecting the produced link (which means manual confidence of 1 or 0). Improvements: allowing for the user to attribute a confidence (increasing or decreasing the automatically calculated one) will render the system more flexible to handle matches that cannot be easily accepted or rejected, by allowing for example several experts\u2019 opinion to be registered and the final user to decide whether to take it as acceptable or not. This transition weight can easily be applied in the generation and manipulation of links. In a simplest setting where the transition weight is set to 1, discovered links can be annotated with an estimated degree of confidence by extrapolation of the evidence\u2019s truth value using an appropriate combination function. Things become a bit more complicated when dealing with the manipulation of links because we have options that range from classic or fuzzy logics to possibilistic or probabilistic logics or evidence theory. In the score combination examples illustrated below, ex:lens-1 is the result of the union of of ex:linkset-1 and ex:linkset-2 using fuzzy logic over the respective evidence\u2019s truth value of links being united while in ex:lens-2 and ex:lens-3 the degree of confidence of a link is computed using evidence theory and probabilistic logic. Combining Uncertainty in Identity ### Linkset meta-data ### ######################### ex: linkset-1 ex: combination-function ex: t-norm-product ; ex: transition-weight 1.0 . ex: linkset-2 ex: combination-function ex: s-norm-max ; ex: transition-weight 1.0 . ### Annotated Linkset ### ######################### ex: linkset-1 { << ex: e1 owl: sameAs ex: e2 >> ex: degree-of-truth 0.76 ; ex: degree-of-confidence 0.76 . } ex: linkset-2 { << ex: e1 owl: sameAs ex: e2 >> ex: degree-of-truth 0.9 ; ex: degree-of-confidence 0.9 . } ################################################################# ### lens-1: Obtaining a degree of confidence by combining #### ### scores with a UNION operation based on truth values #### ### using s-norm-sum fuzzy logic operator. #### ################################################################# ex: lens-1 ex: operator ex: UNION ; ex: target ex: linkset-1 , ex: linkset-2 ; ex: combination-function ex: s-norm-sum ; ex: transition-weight 1.0 . ex: lens-1 { << ex: e1 owl: sameAs ex: e2 >> ex: degree-of-truth 0.976 ; ex: degree-of-confidence 0.976 . } ################################################################# ### lens-2: Obtaining a degree of confidence by combining #### ### scores with a UNION operation based on confidence values #### ### using the event averaging operator. #### ################################################################# ex: lens-2 ex: operator ex: UNION ; ex: target ex: linkset-1 , ex: linkset-2 ; ex: combination-function ex: averaging . ex: lens-2 { << ex: e1 owl: sameAs ex: e2 >> ex: degree-of-confidence 0.83 . } ################################################################# ### lens-2: Obtaining a degree of confidence by combining #### ### scores with a UNION operation based on confidence values #### ### using probabilistic logic. #### ################################################################# ex: lens-3 ex: operator ex: UNION ; ex: combination-function ex: Probabilistic . ex: lens-3 { << ex: e1 owl: sameAs ex: e2 >> ex: degree-of-confidence 0.976 . } Alex Ben G 3. Conclusion \u00b6 We have shed light on the ambiguity surrounding the concepts of vagueness and uncertainty and their corresponding scores degree of truth and degree of confidence . This enables us to understand the nature of the computed scores and provide appropriate labels to scores obtained by matching algorithms as degree of truth (property value comparisons) and scores assigned to identity links generated by machines (matching methods or combination) or human (curation) as degree of confidence . As a consequence, different options for aggregating/combining these scores are presented, depending on whether one is dealing with degree of truth or degree of confidence . Matching methods make explicit all arguments/pre-requisites (datasets, entity-type and property-value restrictions, matching properties\u2026) of a matching algorithm including the conditions in which the algorithm is to accept a discovered link (threshold). \u21a9 For intellectual curiosity, see wikipedia for more information. \u21a9","title":"5. Understanding Matching Results"},{"location":"04.ResultsCombination/#understanding-matching-results","text":"Work in progress This section addresses the problem of understanding the results of Matching Algorithms and Matching Methods supported in the Lenticular Lens tool and also how to correctly combine them. The former corresponds to a set of rules followed by a computer for finding pairs of matching resources. The latter applies the former to generate matching results. One is expected to (i) choose a similarity algorithm, (ii) provide the required input (datasets, entity-type and property-value restrictions, matching properties\u2026) and (iii) provide the conditions (threshold) under which a matched pair is outputted along with its score/weight. Matching methods involving a single entity matching algorithm are distinguished from those involving more than one: simple versus complex method (not to be confused with the complexity of the underlying the matching algorithm). The latter, naturally, requires the results to be combined. Although combining matching results may seem relatively easy, deciding on the final score and the annotations of links with weights requires some extra thoughts. First of all, it requires deciding what a matching score is about: degree of truth or degree of confidence ? In order words, is this a problem of Vagueness or Uncertainty or both? (see Sect. 1). Answering the later question opens the doors to Section 2 where we unveil our take on \u201c how to combine scores? \u201d. In particular, we distinguish the problems into (i) \u201c how to combine degrees of truth? \u201d, (ii) \u201c how to combine degrees of confidence? \u201d and finally (iii) \u201c how/when to transition from degree of truth to degree of confidence? \u201d.","title":"Understanding Matching Results"},{"location":"04.ResultsCombination/#1-vagueness-vs-uncertainty","text":"This section addresses the issue of \u201c how to interpret the various scores in the process of entity matching? \u201d by digging into \u201c What is vagueness and degrees of truth and how are they different from uncertainty and degrees of confidence? \u201d. Roughly, truth value or degrees of truth ( the tomato is ripe with degree of truth 0.7 ) is not to be confused with degrees of certainty / confidence ( I am 0.9 certain that the tomato is ripe with degree of truth 0.7 ) as the latter is not an assignment of truth value as opposed to the former, but rather an evaluation of a weighted proposition regardless of its truth value space ({0, 1} or [0, 1]). In the next subsections, the distinction between Vagueness and Uncertainty is outlined based on the interpretation we associate to the output scores of similarity algorithms and matching methods.","title":"1. Vagueness vs. Uncertainty"},{"location":"04.ResultsCombination/#11-scores-of-similarity-algorithms","text":"Vagueness & Degrees of Truth Similarity algorithms (Levenshtein, Soundex, Cosine\u2026) meant for computing the overlap between a pair of input-arguments (property-value overlap) output values in the unit interval [0, 1] or values convertible in the unit interval (normalisation). These values are truth values / degrees of truth . For example, the input-arguments \u201cRembrand van Rijn\u201d and \u201cRembrandt Harmensz van Rijn\u201d have a similarity degree of truth of 0.63 using the Levenshtein algorithm and a similarity degree of truth of 0.74 using the Soundex algorithm. Transitioning from assigning boolean truth values {0, 1} to assigning continuous truth values in the unit interval [0, 1] to propositions (events) is clearly moving from classical logic to fuzzy logic which is in the modelling paradigm of many-valued Logics . The latter truth space (unit interval) is motivated by the presence of vague concepts in a proposition (the use of ripe in the proposition \u201c the tomato is ripe \u201d), making it hard or sometimes even impossible to establish whether the proposition is completely true or false [ Lukasiewicz2008 ].","title":"1.1 Scores of similarity algorithms"},{"location":"04.ResultsCombination/#12-scores-of-matching-methods","text":"Uncertainty & Degree of confidence Whatever truth value is assigned to a proposition or event, sometimes, one may wonder about the likelihood that the event will occur or has occurred [ Kovalerchuk2017 ]. This reflects the uncertainty regarding the statement due to, for example, lack of information. In these cases, the use of theories such as Probabilistic or Possibilistic Logics can be considered for the evaluation of the likelihood of a proposition. In other words, the use of degree of (un)certainty, belief or confidence is to emphasise a confidence-evaluation on the assignment of a truth value to a proposition, which may or may not qualify as vague ( the tomato is ripe, the sky is blue, the bottle is full\u2026 ). For example, how certain are we in asserting that the proposition \u201cthe tomato is ripe\u201d is true ? As [Lukasiewicz2008] illustrates, asserting that \u201cJohn is a teacher\u201d and \u201cJohn is a student\u201d with respectively the values 0.3 and 0.7 as degrees of certainty is roughly saying that \u201cJohn is either a teacher or a student, but more likely a student\u201d . However, the vague statement \u201c John is tall\u201d with the assignment of 0.9 as degree of truth can be roughly translated as \u201cJohn is quite tall\u201d rather than as \u201cJohn is likely tall\u201d . Entity matching methods 1 output links optionally annotated with matching scores reflecting (i) the level of confidence of a method (the strength of the evidence) for assimilating a pair of resources to be co-referents and (ii) the lower boundary under which two resources can be viewed as co-referents. For example, given the resources e 1 e_1 e 1 \u200b and e 2 e_2 e 2 \u200b respectively labeled Rembrand van Rijn and Rembrandt Harmensz van Rijn , e 1 e_1 e 1 \u200b and e 2 e_2 e 2 \u200b can be linked with a matching score of 0.63 using the Levenshtein algorithm, provided that it is acceptable doing so at a matching score above 0.60 (more details on how this confidence score can be calculated is provided in Section 2.3 ).","title":"1.2 Scores of matching methods"},{"location":"04.ResultsCombination/#2-combination-of-the-scores","text":"Even though degree of truth (scores of similarity algorithms) and degree of confidence (scores of matching methods) are not be confused, they may be combined (multiple truth values or multiple degrees of confidence) or they me be subject to transition. In fact, it may be almost unthinkable to solve real-life-problems without doing so. Considering the famous abductive reasoning example of the duck test where something is probably is a duck if it (i) looks like a duck, (ii) swims like a duck, (iii) and quacks like a duck . This requires first a combination of truth values associated to propositions (i), (ii) and (iii) as conjunction (see Sect. 2.1), followed by a transition into a confidence value concluding that it is probably a duck (see Sect. 2.3).","title":"2. Combination of the Scores"},{"location":"04.ResultsCombination/#21-combining-truth-values","text":"One thing we know and agreed on is that similarity algorithms output boolean or fuzzy truth values in the range [0, 1]. This allows to make use of logic combination functions offers in conventional logic (Subsection 2.1.1) of fuzzy logic (Subsection 2.1.2) depending on whether we expect the solution space to be {0, 1} or [0, 1].","title":"2.1 Combining Truth Values"},{"location":"04.ResultsCombination/#211-classic-logic","text":"The two standard logic operators or combination functions traditionally used are the classical Boolean Conjunction ( \u2227 \\land \u2227 ) and Disjunction ( \u2228 \\lor \u2228 ) . The first takes the minimum strength and the latter takes the maximum strength. This applies for both classic values (True -1- or False -0-) and fuzzy values ( between 0 and 1 ). Since the results from matching methods are assigned fuzzy values in the interval ]0,1], the table bellow illustrates the default behaviour of the Lenticular Lens when combining them. Example 18: Standard logic operations over conjunction (min) and disjunction (max). Source Target Levenshtein Soundex OR(max) AND(min) ------------------------------------------------------------------------------------------------------ Jasper Cornelisz. Lodder Jaspar Cornelisz Lodder 0.92 1.00 1.00 0.92 Rembrand van Rijn Rembrandt Harmensz van Rijn 0.63 0.74 0.74 0.63 Barent Teunis Barent Teunisz gen. Drent 0.52 0.47 0.52 0.47","title":"2.1.1 Classic Logic"},{"location":"04.ResultsCombination/#212-fuzzy-logic","text":"As provided in the next subsections, sophisticated combination functions such as T-norms ( \u2297 \\otimes \u2297 ) and S-norms ( \u2295 \\oplus \u2295 ) , developed by scholars like \u0141ukasiewicz, G\u00f6del, Goguen, Zadeh and others can also be used as alternatives for respectively Boolean Disjunction ( \u2228 \\lor \u2228 ) and Conjunction ( \u2227 \\land \u2227 ) .","title":"2.1.2 Fuzzy Logic"},{"location":"04.ResultsCombination/#2121-t-norms","text":"A list of six different operations can be applied when dealing with methods combined by Conjunction . Here, we present them: Minimum t-norm \u22a4 m i n ( a , b ) = m i n ( a , b ) (1) \u22a4_{min} (a, b) = min(a, b) \\tag{1} \u22a4 m i n \u200b ( a , b ) = m i n ( a , b ) ( 1 ) Product t-norm \u22a4 p r o d ( a , b ) = a . b (2) \u22a4_{prod} (a, b) = a \\text{ . } b \\tag{2} \u22a4 p r o d \u200b ( a , b ) = a . b ( 2 ) \u0141ukasiewicz t-norm \u22a4 L u k ( a , b ) = m a x ( 0 , a + b \u2212 1 ) (3) \u22a4_{Luk} (a, b) = max(0, a + b - 1) \\tag{3} \u22a4 L u k \u200b ( a , b ) = m a x ( 0 , a + b \u2212 1 ) ( 3 ) Drastic \u22a4 D ( a , b ) = { b if a = 1 a if b = 1 0 otherwise (4) \u22a4_D(a, b) = \\begin{cases} b &\\text{if } a = 1 \\\\ a &\\text{if } b = 1 \\\\ 0 &\\text{otherwise} \\end{cases} \\tag{4} \u22a4 D \u200b ( a , b ) = \u23a9 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a7 \u200b b a 0 \u200b if a = 1 if b = 1 otherwise \u200b ( 4 ) Nilpotent minimum \u22a4 n M ( a , b ) = { m i n ( a , b ) if a + b > 1 0 otherwise (5) \u22a4_{nM}(a, b) = \\begin{cases} min(a, b) &\\text{if } a + b > 1 \\\\ 0 &\\text{otherwise} \\end{cases} \\tag{5} \u22a4 n M \u200b ( a , b ) = { m i n ( a , b ) 0 \u200b if a + b > 1 otherwise \u200b ( 5 ) Hamacher product \u22a4 H 0 ( a , b ) = { 0 if a = b = 0 a b a + b \u2212 a b otherwise (6) \u22a4_{H_0}(a, b) = \\begin{cases} 0 &\\text{if } a = b = 0 \\\\ \\dfrac{ab}{a + b - ab} &\\text{otherwise} \\end{cases} \\tag{6} \u22a4 H 0 \u200b \u200b ( a , b ) = \u23a9 \u23aa \u23a8 \u23aa \u23a7 \u200b 0 a + b \u2212 a b a b \u200b \u200b if a = b = 0 otherwise \u200b ( 6 ) The following table provides three case studies to illustrate the application of each of the aforementioned T-norm binary operations . They are presented in order from the less strict ( \u22a4 min ) to the most strict ( \u22a4 D ). Source, Target Levenshtein, Soundex \u22a4 min \u22a4 H 0 \u22a4 prod \u22a4 nM \u22a4 Luk \u22a4 D Src : Jasper Cornelisz. Lodder Trg : Jaspar Cornelisz Lodder 0.92, 1.00 0.920 0.920 0.920 0.920 0.920 0.920 Src : Rembrand van Rijn Trg : Rembrandt Harmensz van Rijn 0.63, 0.74 0.630 0.516 0.466 0.630 0.370 0.000 Src : Barent Teunis Trg : Barent Teunisz gen. Drent 0.52, 0.47 0.470 0.328 0.244 0.000 0.000 0.000","title":"2.1.2.1 T-norms"},{"location":"04.ResultsCombination/#2122-s-norms","text":"A list of six different operations can also be applied when dealing with methods combined by Disjunction . Here, we present them: Maximum S-norm \u22a5 m a x ( a , b ) = m a x ( a , b ) (7) \u22a5_{max} (a, b) = max(a, b) \\tag{7} \u22a5 m a x \u200b ( a , b ) = m a x ( a , b ) ( 7 ) Probabilistic sum \u22a5 s u m ( a , b ) = a + b \u2212 a . b (8) \u22a5_{sum} (a, b) = a + b - a \\text{ . } b \\tag{8} \u22a5 s u m \u200b ( a , b ) = a + b \u2212 a . b ( 8 ) Bounded sum \u22a5 L u k ( a , b ) = m i n ( a + b , 1 ) (9) \u22a5_{Luk} (a, b) = min(a + b, 1) \\tag{9} \u22a5 L u k \u200b ( a , b ) = m i n ( a + b , 1 ) ( 9 ) Drastic S-norm \u22a5 D ( a , b ) = { b if a = 0 a if b = 0 1 otherwise (10) \u22a5_D(a, b) = \\begin{cases} b &\\text{if } a = 0 \\\\ a &\\text{if } b = 0 \\\\ 1 &\\text{otherwise} \\end{cases} \\tag{10} \u22a5 D \u200b ( a , b ) = \u23a9 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a7 \u200b b a 1 \u200b if a = 0 if b = 0 otherwise \u200b ( 1 0 ) Nilpotent maximum \u22a5 n M ( a , b ) = { m a x ( a , b ) if a + b < 1 1 otherwise (11) \u22a5_{nM}(a, b) = \\begin{cases} max(a, b) &\\text{if } a + b < 1 \\\\ 1 &\\text{otherwise} \\end{cases} \\tag{11} \u22a5 n M \u200b ( a , b ) = { m a x ( a , b ) 1 \u200b if a + b < 1 otherwise \u200b ( 1 1 ) Einstein sum \u22a5 H 2 ( a , b ) = a + b 1 + a b (12) \u22a5_{H_2} (a, b) = \\dfrac{a + b} {1 + ab} \\tag{12} \u22a5 H 2 \u200b \u200b ( a , b ) = 1 + a b a + b \u200b ( 1 2 ) Source, Target Levenshtein, Soundex \u22a4 D \u22a4 Luk \u22a4 H 2 \u22a4 sum \u22a4 nM \u22a4 max Src : Jasper Cornelisz. Lodder Trg : Jaspar Cornelisz Lodder 0.92, 1.00 1.000 1.000 1.000 1.000 1.000 1.000 Src : Rembrand van Rijn Trg : Rembrandt Harmensz van Rijn 0.63, 0.74 1.000 1.000 0.934 0.904 1.000 0.740 Src : Barent Teunis Trg : Barent Teunisz gen. Drent 0.52, 0.47 1.000 0.990 0.796 0.746 0.520 0.520","title":"2.1.2.2 S-norms"},{"location":"04.ResultsCombination/#213-examples","text":"Suppose that, two data items E-1 and E-2 have the following information: E-1 Name : Titus Rembrandtsz. van Rijn Mother : Saskia Uylenburgh Father : Rembrand van Rijn Parent\u2019s Marriage date : 1644-06-22 E-2 Name : T. Rembrandtszoon van Rijn Mother : Saske van Uijlenburg Father : Rembrandt Harmensz van Rijn Baptism date :1641-09-22 To interpret E-1 and E-2 as representing co-referent persons, the following four tests are proposed. Test-1 OR Here, the names of E-1 and E-2 are to be compared using the Levenshtein and Soundex algorithms at a threshold of at least 0.7 . MATCHING RESULTS - Levenshtein(Titus Rembrandtsz van Rijn, T. Rembrandtszoon van Rijn) => 0.73 \u2705 - sdx_1 = Soundex(Titus Rembrandtsz van Rijn) = T320 R516 V500 R250 - sdx_2 = Soundex(T. Rembrandtszoon van Rijn) = T000 R516 V500 R250 - Levenshtein(sdx_1, sdx_2) => 0.89 \u2705 DISJUNCTIONS RESULTS - names similarity = S-norm(0.73, 0.88, 'MAXIMUM') => 0.89 \u2705 - names similarity = S-norm(0.73, 0.88, 'PROBABILISTIC') => 0.97 \u2705 Test-2 AND Names of the postulated mothers and fathers are to be similar at a threshold of at least 0.6 using the Levenshtein algorithm. MATCHING RESULTS - Levenshtein(Saskia Uylenburgh, Saske van Uijlenburg) => 0.65 \u2705 - Levenshtein(Rembrand van Rijn, Rembrandt Harmensz van Rijn) => 0.63 \u2705 CONJUNCTION RESULTS - Parent's names similarity = t_norm(0.65, 0.63, 'MINIMUM') => 0.63 \u2705 - Parent's names similarity = t_norm(0.65, 0.63, 'HAMACHER') => 0.47 \u274c Test-3 The period between the parent\u2019s marriage date on the one side and the child\u2019s baptism date on the other side are to be no more than 25 years apart . MATCHING RESULTS - Delta(1668-02-28, 1669-03-22, 25) => 1.00 \u2705 Test-4 AND Combining all above three tests considering a the conjunction fuzzy operator should result in a similarity score above or equal to 0.8. -------------------------------------------------------------------------------------- FINAL CONJUNCTIONS WITH A TRUTH VALUE LIST OF [0.850, 0.63, 1] -------------------------------------------------------------------------------------- - t_norm_list([0.850, 0.63, 1], 'MINIMIUM') => 0.63 \u274c - t_norm_list([0.850, 0.63, 1], 'HAMACHER') => 0.58 \u274c - t_norm_list([0.850, 0.63, 1], 'PRODUCT') => 0.56 \u274c - t_norm_list([0.850, 0.63, 1], 'NILPOTENT') => 0.63 \u274c - t_norm_list([0.850, 0.63, 1], '\u0141uk') => 0.52 \u274c - t_norm_list([0.850, 0.63, 1], 'DRASTIC') => 0.00 \u274c -------------------------------------------------------------------------------------- CONJUNCTIONS WITH A DIFFERENT LIST OF TRUTH VALUES [0.89, 0.82, 1] -------------------------------------------------------------------------------------- - t_norm_list([0.89, 0.82, 1], \"MINIMUM\") => 0.82 \u2705 - t_norm_list([0.89, 0.82, 1], \"HAMACHER\") => 0.74 \u274c - t_norm_list([0.89, 0.82, 1], \"PRODUCT\") => 0.73 \u274c - t_norm_list([0.89, 0.82, 1], \"NILPOTENYT\") => 0.82 \u2705 - t_norm_list([0.89, 0.82, 1], \"LUK\") => 0.71 \u274c - t_norm_list([0.89, 0.82, 1], \"DRASTIC\") => 0.0 \u274c -------------------------------------------------------------------------------------- EXAMPLE USING MORE THAN ONE FUZZY LOGIC OPERATOR -------------------------------------------------------------------------------------- - Ops.t_norm(Ops.t_norm(0.850, 0.63, 'HAMACHER'), 1, 'MINIMIUM') => 0.57 \u274c - Ops.t_norm(Ops.t_norm(0.850, 0.63, 'MINIMIUM'), 1, 'HAMACHER') => 0.63 \u274c Conclusion : Given the evidence provided for E-1 and E-2 and the rules described above, the interpretation resulting from the chosen fuzzy logic operations leads to the conclusion that there is no sufficient evidence to infer that the underlying data items are co-referent. This rejection is mainly due to the low similarity of the parents\u2019 names. If the resulting similarity were above 0.8, there would then be a better chance for the data items to be co-referent. Keep in mind that our conjectured rule asserts an identity relation for combination of scores only when above 0.8. A better data or more advanced algorithm could have helped.","title":"2.1.3 Examples"},{"location":"04.ResultsCombination/#22-combining-confidence-values","text":"Understanding how to combine uncertain events starts by a better understanding of uncertainty itself. Sentz et al. provide two important distinctions of uncertainty: Aleatory (Objective Uncertainty originated from random behaviour) or Epistemic (Subjective Uncertainty originated from ignorance or lack of knowledge). Whereas traditional probability is clearly applicable to deal with Aleatory Uncertainty , researchers claim its inability to deal with Epistemic Uncertainty. In short this is because the latter neither implies knowing probability of all relevant events, nor its uniform distribution, not even the axiom of additivity (i.e. all probabilities summing up to 1). This leads to the emergence of more general representations of uncertainty as alternatives to the traditional probability theory, such as imprecise probabilities , possibility theory and evidence theory . Nonetheless, at present, there is no clear best representation of uncertainty [ Sentz2002 ]. This section introduces alternative representations of uncertainty that are planned to be implemented in the Lenticular Lens. Although their choice can be ultimately a choice of the user, we consider the problem of co-reference search by applying multiple matching methods to be a case of Epistemic Uncertainty nicely approached in evidence theory.","title":"2.2 Combining Confidence Values"},{"location":"04.ResultsCombination/#221-probabilistic-logic","text":"Using probability for combining confidence values with the logic operators \u201cAND\u201d and \u201cOR\u201d can be done in the context of link manipulation in theory with the equations (13) and (14) respectively with the strong assumption that the events to be combined are independent ( the occurrence of one event has no effect on the probability of the occurrence of the other ). P ( A and B ) = P ( A ) \u2005 \u22c5 P ( B ) (13) P(\\text{A and B}) = P(A) \\: \\cdotp P(B) \\tag{13} P ( A and B ) = P ( A ) \u22c5 P ( B ) ( 1 3 ) P ( A or B ) = P ( A ) + P ( B ) \u2212 P ( A and B ) where P ( A and B ) = 0 if A and B are mutually exclusive events meaning that these events have no outcomes in common. (14) P(\\text{A or B}) = P(A) + P(B) - P(\\text{A and B}) \\tag{14} \\\\ \\footnotesize \\text{where } P(\\text{A and B}) = 0 \\text{ if A and B are mutually exclusive events} \\\\ \\text{ meaning that these events have no outcomes in common.} P ( A or B ) = P ( A ) + P ( B ) \u2212 P ( A and B ) where P ( A and B ) = 0 if A and B are mutually exclusive events meaning that these events have no outcomes in common. ( 1 4 ) On the one hand, assuming that events A and B are independent , equations (13) and (14) are somewhat similar to \u2297 \\otimes \u2297 ( \u22a4 p r o d ( a , b ) = a . b ) \\big(\u22a4_{prod}(a, b) = a \\text{ . } b\\big) ( \u22a4 p r o d \u200b ( a , b ) = a . b ) and \u2295 \\oplus \u2295 ( \u22a5 s u m ( a , b ) = a + b \u2212 a . b ) \\big(\u22a5_{sum}(a, b) = a + b - a \\text{ . } b\\big) ( \u22a5 s u m \u200b ( a , b ) = a + b \u2212 a . b ) of Product Logic hence straightforward to implement when in the need of manipulating links such as applying an intersection or a union operator to two sets of links for example. On the other hand, in the event that A and B are not independent , the value of P ( A and B ) P(\\text{A and B}) P ( A and B ) should be observable n ( A and B ) n ( S a m p l e ) \\frac{n(A\\text{ and } B)}{n(Sample)} n ( S a m p l e ) n ( A and B ) \u200b , provided or computed using conditional probability in Equation 15. \ud835\udc43 ( A and B ) = \ud835\udc43 ( \ud835\udc34 ) . \ud835\udc43 ( \ud835\udc35 \u2223 \ud835\udc34 ) (15) \ud835\udc43(\\text{A and B})=\ud835\udc43(\ud835\udc34)\\text{ . }\ud835\udc43(\ud835\udc35|\ud835\udc34) \\tag{15} P ( A and B ) = P ( A ) . P ( B \u2223 A ) ( 1 5 ) In the context of the Lenticular Lens, the computed confidence values are independent (the computation of a confidence value by method-1 has no effect on the computation of a confidence value by method-2). This being said, equation (15) is not applicable.","title":"2.2.1 Probabilistic Logic"},{"location":"04.ResultsCombination/#222-possibilistic-logic","text":"Possibility 2 is compositional with respect to the union operator as the possibility of the union is deducible from the possibility of each component. Note however that it is not compositional with respect to the intersection operator. p o s ( A \u222a B ) = m a x ( p o s ( A ) , \u2005 p o s ( B ) ) \u2005 \u2005 \u2005 \u2005 for any subsets A and B (16) pos(A \\cup B) = max(pos(A), \\: pos(B)) \\:\\:\\:\\: \\text{\\footnotesize for any subsets A and B} \\tag{16} p o s ( A \u222a B ) = m a x ( p o s ( A ) , p o s ( B ) ) for any subsets A and B ( 1 6 ) p o s ( A \u2229 B ) \u2264 m i n ( p o s ( A ) , \u2005 p o s ( B ) ) \u2264 m a x ( p o s ( A ) , \u2005 p o s ( B ) ) (17) pos(A \\cap B) \\leq min(pos(A), \\: pos(B)) \\leq max(pos(A), \\: pos(B)) \\tag{17} p o s ( A \u2229 B ) \u2264 m i n ( p o s ( A ) , p o s ( B ) ) \u2264 m a x ( p o s ( A ) , p o s ( B ) ) ( 1 7 )","title":"2.2.2 Possibilistic Logic"},{"location":"04.ResultsCombination/#223-evidence-theory","text":"Evidence theory also provide different ways for combining uncertain scores. We present here two of them: starting from the first proposal, namely Dempster-Shafer , which is shown to have limitations, followed by a more accepted approach called average .","title":"2.2.3 Evidence Theory"},{"location":"04.ResultsCombination/#2231-dempster-shafer","text":"Combining or aggregating confidence values associated with evidence is made possible with the Dempster-Shafer conjunctive combination rule as presented in Equation 18. Here too, the assumption of independence among sources providing supporting or conflicting assessments for the same frame of discernment [ Sentz2002 ]) is of key-importance and the basic assumption supporting the Dempster-Shafer combination rule. However, a crucial context dependent limitation of this rule as pointed out by [Zadeh, 1984], occurs in cases with significant conflicts because the denominator in the Dempster-Shafer combination rule has the effect of completely ignoring conflict while the numerator emphasis agreement , thereby yielding result inconsistency (unintuitive). m 12 ( A ) = ( m 1 \u2295 m 2 ) ( A ) = supporting evidence 1 \u2212 conflicting evidence = \u2211 B \u2229 C = A \u2260 \u2205 m 1 ( B ) m 2 ( C ) 1 \u2212 \u2211 B \u2229 C = \u2205 m 1 ( B ) m 2 ( C ) (18) m_{12}(A) = (m_1 \\oplus m_2)(A) = \\footnotesize{\\frac{\\text{supporting evidence}}{1 - \\text{conflicting evidence}}} = \\frac{ \\displaystyle\\sum_{B \\cap C = A \\ne \\emptyset} m_1(B) m_2(C) }{1 - \\displaystyle\\sum_{B \\cap C = \\emptyset} m_1(B) m_2(C) } \\tag{18} m 1 2 \u200b ( A ) = ( m 1 \u200b \u2295 m 2 \u200b ) ( A ) = 1 \u2212 conflicting evidence supporting evidence \u200b = 1 \u2212 B \u2229 C = \u2205 \u2211 \u200b m 1 \u200b ( B ) m 2 \u200b ( C ) B \u2229 C = A \ue020 = \u2205 \u2211 \u200b m 1 \u200b ( B ) m 2 \u200b ( C ) \u200b ( 1 8 ) where { m Basic probability assignment (bpa) function for the universal set P(X) to [0, 1] . m ( A ) The bpa value or the mass of a given set A but not for a particular subset of A . m 1 , m 2 Two given basic probability assignments . m 12 ( A ) The combination a.k.a the joint m 12 . m ( \u2205 ) = 0 The mass of the empty set is zero. \u2211 A \u2208 P ( X ) m ( A ) = 1 The masses of all the members of the power set add up to a total of 1. \\scriptsize \\text{where } \\begin{cases} m &\\text{Basic probability assignment (bpa) function for the universal set P(X) to [0, 1]}. \\\\ m(A) &\\text{The bpa value or the mass of a given set A but not for a particular subset of A}. \\\\ m_1 \\text{, } m_2 &\\text{Two given basic probability assignments}. \\\\ m_{12}(A) &\\text{The combination a.k.a the joint } m_{12}. \\\\ m(\\emptyset) = 0 &\\text{The mass of the empty set is zero.} \\\\ \\displaystyle\\sum_{A\\in P(X)} m(A) = 1 &\\text{The masses of all the members of the power set add up to a total of 1.} \\end{cases} where \u23a9 \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23a8 \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23a7 \u200b m m ( A ) m 1 \u200b , m 2 \u200b m 1 2 \u200b ( A ) m ( \u2205 ) = 0 A \u2208 P ( X ) \u2211 \u200b m ( A ) = 1 \u200b Basic probability assignment (bpa) function for the universal set P(X) to [0, 1] . The bpa value or the mass of a given set A but not for a particular subset of A . Two given basic probability assignments . The combination a.k.a the joint m 1 2 \u200b . The mass of the empty set is zero. The masses of all the members of the power set add up to a total of 1. \u200b This inconsistency is higgled in Fig. 1: Patient Diagnose (1) where the joint agreement on the patient\u2019s condition results to a 1 using Dempster\u2019s combination rule although the doctors agreed that the patient is less-likely to suffer from a brain tumour. Has it been the opposite scenario as in Fig. 1: Patient Diagnose (2) (Dr. Green and Dr. House assigning 0.99 as the basic probability for the patience suffering from a brain tumour), the result of m 12 ( b r a i n T u m o r ) = 1 \\scriptsize m_{12}(brainTumor)=1 m 1 2 \u200b ( b r a i n T u m o r ) = 1 would be consistent with our intuition. Fig. 1: Combining two doctors\u2019 diagnosis of a patient using the Dempster-Shafer combination rule. The results in (1) and (2) highlight the context dependent inconsistency of the implementation of the rule.","title":"2.2.3.1 Dempster-Shafer"},{"location":"04.ResultsCombination/#2232-averaging","text":"Many alternatives to Equation 18 have been proposed by scholars such as Yager (modified Dempster\u2019s rule), Inagaki (modified combination rule), Zhang (center combination rule), Dubois and Prade (disjunctive consensus rule), Ferson and Kreinovich (averaging), etc. One particular approach called Averaging (Equation 19) is considered to produced better outcomes [ Choi2009 ] and therefore more likely to be implemented in the Lenticular Lens for combining confidence values, such as lens operations union and intersection . It provides means to calculate an average of several ( n ) sources while also taking into account possible reliability weights attributed to each source. m 1... n ( A ) = 1 n \u2211 i = 1 n w i m i ( A ) (19) m_{1...n}(A) = \\frac{1}{n} \\sum_{i=1}^{n} w_im_i(A) \\tag{19} m 1 . . . n \u200b ( A ) = n 1 \u200b i = 1 \u2211 n \u200b w i \u200b m i \u200b ( A ) ( 1 9 ) Where { n Number of sources . w i Reliability weight of the source . m i Basic probability assignment of a body of evidence . \\scriptsize \\text{Where } \\begin{cases} n &\\text{Number of sources}. \\\\ w_i &\\text{Reliability weight of the source}. \\\\ m_i &\\text{Basic probability assignment of a body of evidence}. \\\\ \\end{cases} Where { n w i \u200b m i \u200b \u200b Number of sources . Reliability weight of the source . Basic probability assignment of a body of evidence . \u200b Applying Equation 19 to the two scenarios illustrated in Fig. 1 will yield the following results: Patient Diagnosis (1): m 1 , 2 ( b r a i n T u m o r ) = 0.01 + 0.01 2 = 0.01 \\small m_{1,2}(brainTumor) = \\frac{0.01 + 0.01}{2} = 0.01 m 1 , 2 \u200b ( b r a i n T u m o r ) = 2 0 . 0 1 + 0 . 0 1 \u200b = 0 . 0 1 Patient Diagnosis (2): m 1 , 2 ( b r a i n T u m o r ) = 0.99 + 0.99 2 = 0.99 \\small m_{1,2}(brainTumor) = \\frac{0.99 + 0.99}{2} = 0.99 m 1 , 2 \u200b ( b r a i n T u m o r ) = 2 0 . 9 9 + 0 . 9 9 \u200b = 0 . 9 9","title":"2.2.3.2 Averaging"},{"location":"04.ResultsCombination/#23-from-truth-to-confidence","text":"Similarly to the duck test, the modelling of ways to find supporting evidence for isolating potential entity matching candidates is crucial to inferring identity for a pair of resources. Section 2.1 already covers our take on how to combine truth values and Section 2.2 covers the combination of degrees of confidence. What now remains is to understand \u201c How to transition from truth value to uncertainty / degree of confidence? \u201d. For illustration purpose, this means for example, how to move from [looks like a duck (0.8), swims like a duck (1.0), and quacks like a duck (0.95)] to [it probably is a duck (???)]. Applying \u22a4 p r o d \u22a4_{prod} \u22a4 p r o d \u200b to the evidence truth values results in a truth value of 0.76. Assuming that a transition from the evidence truth value to the degree of confidence carries a weight of 1, we argue that it is now possible to extrapolate a confidence of 0.76 for asserting that the entity that looks like a duck (0.8), swims like a duck (1.0), and quacks like a duck (0.95) is probably (0.76) a duck. Similarly, if this transition weight is now reset to 0.9 for example, it is also reasonable that the degree of confidence computed for the entity being a duck drops to 0.684.","title":"2.3 From Truth to Confidence"},{"location":"04.ResultsCombination/#24-implementation","text":"There are several operations in the Lenticular Lens in which one or more of the above discussed values and their combinations take place. We summarise them here, including also the possibilities of future improvements to allow for more control over the produced values. Link Construction Simple Macthing Method Transition from Truth to Confidence Degree Example : If the entities\u2019 names sound alike with degree of truth above 0.6, then the resources are probably the same with the sound-alike * 1 as degree of confidence. Complex Macthing Method Combination of truth values via logic-boxes followed by Transition from Truth to Confidence Degree Example : If the entities\u2019 names sound alike with a degree of truth above 0.6 OR look alike with a degree of truth above 0.7, AND the date of birth is the same, then final degree of truth using classical OR/AND combinations is min(max(sound-alike, look-alike), same-birth) and the resources are probably the same with the final degree of truth as degree of confidence. Currenlty a fix transition-weight of 1 applied, so that the final score (currenlty only one is outputted) reflects both degrees of truth and confidence. Improvements: Explicit output degrees of truth, confidence and transition-weight; Allow for the user to decide on the transition-weight applied, so that low-power identity criteria such as the example in simple method would not conclude that high name similarity means high confidence. Link Manipulation 1. Union Disjunction of the final degrees of truth, followed by the re-assignment of the confidence value given a transition-weight. Possible combinations: classic OR S-norms Disjunction of the final degrees confidence Possible combinations: Probabilistic OR Possibilitic Union Averaging 2. Intersection Conjunction of the final degrees of truth, followed by the re-assignment of the confidence value given a transition-weight Possible combinations: classic AND T-norms Conjunction of the final degrees confidence Possible combinations: Probabilistic AND Possibilitic Intersection Averaging is applicable provided that there is agreement between all sources, i.e. all the sources have produced a degree of confidence above 0 for the link under scrutiny. 3. Difference This operation does not require combination of values, but simply selects the matches that do not occur in another link-set. 4. Composition Transitivity over the final degress of truth ??? Composition of confidence attributed to independent events ??? 5. In Set This operation does not require combination of values, but simply selects the matches of which resources do occur in a given resource-set. Currently, only the combination of the truth values are implemented, with the transition-weight equal to 1. Improvements: allowing for the user to decide which values to combine and how, plus what transition-weight to (re)apply if needed would render the system more flexible. Link Validation On top of the automatically calculated dregre of confidence discussed so far, the manual validation allows for the user to attribute its own confidence. Currently, such manual attribution consists of simply accepting or rejecting the produced link (which means manual confidence of 1 or 0). Improvements: allowing for the user to attribute a confidence (increasing or decreasing the automatically calculated one) will render the system more flexible to handle matches that cannot be easily accepted or rejected, by allowing for example several experts\u2019 opinion to be registered and the final user to decide whether to take it as acceptable or not. This transition weight can easily be applied in the generation and manipulation of links. In a simplest setting where the transition weight is set to 1, discovered links can be annotated with an estimated degree of confidence by extrapolation of the evidence\u2019s truth value using an appropriate combination function. Things become a bit more complicated when dealing with the manipulation of links because we have options that range from classic or fuzzy logics to possibilistic or probabilistic logics or evidence theory. In the score combination examples illustrated below, ex:lens-1 is the result of the union of of ex:linkset-1 and ex:linkset-2 using fuzzy logic over the respective evidence\u2019s truth value of links being united while in ex:lens-2 and ex:lens-3 the degree of confidence of a link is computed using evidence theory and probabilistic logic. Combining Uncertainty in Identity ### Linkset meta-data ### ######################### ex: linkset-1 ex: combination-function ex: t-norm-product ; ex: transition-weight 1.0 . ex: linkset-2 ex: combination-function ex: s-norm-max ; ex: transition-weight 1.0 . ### Annotated Linkset ### ######################### ex: linkset-1 { << ex: e1 owl: sameAs ex: e2 >> ex: degree-of-truth 0.76 ; ex: degree-of-confidence 0.76 . } ex: linkset-2 { << ex: e1 owl: sameAs ex: e2 >> ex: degree-of-truth 0.9 ; ex: degree-of-confidence 0.9 . } ################################################################# ### lens-1: Obtaining a degree of confidence by combining #### ### scores with a UNION operation based on truth values #### ### using s-norm-sum fuzzy logic operator. #### ################################################################# ex: lens-1 ex: operator ex: UNION ; ex: target ex: linkset-1 , ex: linkset-2 ; ex: combination-function ex: s-norm-sum ; ex: transition-weight 1.0 . ex: lens-1 { << ex: e1 owl: sameAs ex: e2 >> ex: degree-of-truth 0.976 ; ex: degree-of-confidence 0.976 . } ################################################################# ### lens-2: Obtaining a degree of confidence by combining #### ### scores with a UNION operation based on confidence values #### ### using the event averaging operator. #### ################################################################# ex: lens-2 ex: operator ex: UNION ; ex: target ex: linkset-1 , ex: linkset-2 ; ex: combination-function ex: averaging . ex: lens-2 { << ex: e1 owl: sameAs ex: e2 >> ex: degree-of-confidence 0.83 . } ################################################################# ### lens-2: Obtaining a degree of confidence by combining #### ### scores with a UNION operation based on confidence values #### ### using probabilistic logic. #### ################################################################# ex: lens-3 ex: operator ex: UNION ; ex: combination-function ex: Probabilistic . ex: lens-3 { << ex: e1 owl: sameAs ex: e2 >> ex: degree-of-confidence 0.976 . } Alex Ben G","title":"2.4 Implementation"},{"location":"04.ResultsCombination/#3-conclusion","text":"We have shed light on the ambiguity surrounding the concepts of vagueness and uncertainty and their corresponding scores degree of truth and degree of confidence . This enables us to understand the nature of the computed scores and provide appropriate labels to scores obtained by matching algorithms as degree of truth (property value comparisons) and scores assigned to identity links generated by machines (matching methods or combination) or human (curation) as degree of confidence . As a consequence, different options for aggregating/combining these scores are presented, depending on whether one is dealing with degree of truth or degree of confidence . Matching methods make explicit all arguments/pre-requisites (datasets, entity-type and property-value restrictions, matching properties\u2026) of a matching algorithm including the conditions in which the algorithm is to accept a discovered link (threshold). \u21a9 For intellectual curiosity, see wikipedia for more information. \u21a9","title":"3. Conclusion"},{"location":"05.LinkConstruction/","text":"LINK CONSTRUCTION \u00b6 Linking co-referent entities across a variety of datasources is a pragmatic and fast way to seamlessly navigate across datasets without having to agree in a uniform vocabulary. This solution offered in the Semantic Web architecture appears attractive as the ultimate goal for the researcher executing this task is not the integration of data but the extraction of vital information for reaching valid conclusions about problems under scrutiny. This said, the Lenticular Lens offers means to reach that ultimate goal of the researcher while making sure that the steps taken by the researcher are documented such that other researchers can easily re-generate the data leading to specific conclusions if need be. Along the way of entity-based data integration and data extraction, the Lenticular Lens aims to document among others: The datasources to integrate; The reasons behind a specific integration; The entity types and restrictions that ensure correctness in bridging across datasources of interest; The matching methods and specifications justifying the existence of a set of links. The Lenticular Lens tool aims to provide generic methods that allows a broader audience suffering the same need for data integration. The first step in creating and documenting links using the Lenticular Lens is defining the scope in witch links are to be created and possibly validated. For that, the tool offers the RESEARCH menu followed by the SELECT and CONSTRUCT menus. We now go through each of these first three menus underlying the existence of links. 1. Scope \u00b6 The RESEARCH menu is the starting point in learning how to interact with the Lenticular Lens tool. In general, a research question somehow sets the scope in which link creations, manipulations or validation take place. This provides the first building block supporting the user with defining the context in which a particular alignment is generated. Using this menu, part of the context is made explicit by selecting the datasets and entity types necessary to continue the investigation. As an overview, the RESEARCH menu provides researchers with means to describe the research of interest in terms of: Research Question for inserting the main research question driving the integration. Hypothesis for pointing out the hypothesis in mind prior to the data extraction. Link and Citation to ensure that, if the results happen to be published, the researcher still has the facility to add a link to the publication and and a bibliographic reference for future reuse. Fig. 4.1 illustrates the different fields to be filled in by the researcher for a quick overview of what can happen in this research project and why. Once providing the information is done, the Save button at the bottom of the page can be clicked to save the provided information and exit the Lenticular Lens if the user which to continue with other tasks. Or, should the user choose to continue the alternative Save and next button can be used to save the project and move to the next window. Fig. 4.1: Describing the scope of he research question. 2. Data \u00b6 In the previous step or window, the researcher has defined the scope of the research for which data are to be extracted and analysed. In this second window labelled SELECT , the user is to describe and select the entity types involved in his research. For that, the location of the datasource needs to be provided and the datasets in which the respective entities of interest reside need to be selected. 2.1 Data Selection \u00b6 As the user activates the Saves and next button at the end of the previous page, she is presented with a new window with a single card labelled Entity-Type Selection 1 as presented in Fig. 4.2. The plus button at the right side of the picture enables the user to create new cards when needed while the arrow-head button at the left side of the card\u2019s label allows for the unveiling of the card as displayed in Fig 3. Fig. 4.2: The card view for data selection Describing the type of an entity can be done using the Description text box for each entity type. To provide the location of the data, the GraphQL Endpoint text box can be use to fill in the URL of any GraphQL end point. Once the endpoint is given and loaded, a dataset can be selected from the list of datasets available at the provided endpoint. The selection of a dataset will prompt a new dropdown text box as Entity type , providing the user with the facility to select the entity type of interest. After loading the provided URL of the default Golden Agent\u2019s endpoint, Fig. 4.3 shows the list of datasets available at that location to choose from. Fig. 4.3: List of datasets available at the default Golden Agent\u2019s GraphQL endpoint. 2.2 Data Restrictions \u00b6 If need be to filter entities based on specific conditions, this is also possible with the Filter card shown in Fig. 4.6. Fig. 4.6: The card for defining entity restrictions. Once the button is clicked, this card presents the user with a Filter-Logic box which enable the creation of a relatively complex and versatile entity restrictions. Fig. 4.7 for example show the list of available filtering options while Fig. 4.8 illustrates an example where the has minimum date and has maximum date filtering options are used to isolate entities of interest. These entities are now those between with a registration date between [1600, 1659] and having their respective literal name exempt of trailing dots (\u2026). Fig. 4.7: List of restriction options. Fig. 4.8: The card for defining entity restrictions. 2.2.1 Restriction options \u00b6 Equal to / Not Equal to. This option allows one to select entities that have the value of a certain property equal (or not) to a certain value. For example, all entities with property ex:workLocation equal (or not) to Amsterdam . Contains / Does not contain. This option is used to make sure that the property-value of the entities of interest contains or does not contain a specific sequence of characters. For example, %...% could be used for (i) excluding people whose names contain trailing dots or (ii) to select those entities to apply a particular modification onto their names, like adding the surname of the father for a baptised child whose surname is given as ... . Has property / Has no property. This option is used to select entities based on the existence (or not) of a certain property. Let assume, for example, that the user is interested in entities that are parents. This option allows one to filter all entities for which the a value exists for the property ex:parentOf for example. It also allows you to exclude all entities that are parents if the option Has no property is used instead. Has minimum / maximum value. This option allows for restricting entities to be within or outside a specified range given user\u2019s specified property-values of type number over which the restriction can be applied. To delimit both upper and lower bounds, the user can combine minimum and maximum using the logical box AND. Has minimum / maximum date. This option allows for restricting entities to be within or outside a specified range given user\u2019s specified property-values of type date over which the restriction can be applied. Within this option, a date format can be specified. The default format is YYYY-MM-DD . The values 10, 300 and 1990 for example will be considered as year while 10-1, 300-1 and 1990-1 will be considered as the first month of the respective year values. To delimit both upper and lower bounds, the user can combine minimum and maximum using the logical box AND. Has minimum / maximum appearances. This option allows for restricting entities for which a given property value occurs within a specified range . For example, to avoid excessive number of possible matches, one can delimit that only entities whose name value occur less than 5 times in the dataset will be included. To delimit both upper and lower bounds, the user can combine minimum and maximum using the logical box AND. In set. This option allows the filtering of a collection of resources of interest based on a set of resources. These set of resources is not manually provided but can be obtained through a list of existing linksets or lenses. The example below provides a detailed understanding of this filtering approach. Example 1: IN SET Two collections A and B to be matched via whatever method would create a se of links labelled linkset-AB. However, we are only interested in a subset of linkset-AB, such that it\u2019s resources (subject, object or both) are present in another given set, namely an input-linkset I. For efficiency purposes, linkset-AB does not need to be fully created to be filtered later on. This implies that the collections A and/or B need to be filtered such that A\u2019 = A \u2229 I and/or B\u2019 = B \u2229 I before executing the matching algorithm. ###################################################### # Linksets as named graphs # ###################################################### ex: input-linkset { A: Chiara owl: sameAs C: Latronico . A: Al owl: sameAs C: Al_Idrissou . A: Al owl: sameAs C: Al_Koudous . } ex: linkset-AB { A: Chiara owl: sameAs B: Chiara . A: Kerim owl: sameAs B: Kerim . } ###################################################### # In Resource Set # ###################################################### ### The set S of resources from input-linkset is: ### S = {A:Chiara, A:Al, C:Latronico, C:Al_Idrissou, C:Al_Koudous} ex: linkset-SubjectInSet { A: Chiara owl: sameAs B: Chiara . } 2.3 Data Exploration \u00b6 At this point, successfully providing the required information ( Dataset and Entity-Type ) triggers the appearance of the Explore Sample button at the right side of the card\u2019s label ( Entity-type selection 1 ) as displayed in Fig. 4.4. As illustrated in Fig. 4.5, with this button, users are now able to explore information of their choice about the entities of interest by selecting properties describing them. Keep in mind that this feature is only intended as exploration alternative to make sure of the choices (dataset, entity-type and restrictions) made. Fig. 4.4: The Explore sample button shows only after the entity type is selected. Fig. 4.5: Exploring the description of entities of choice, stemmed from the dataset of interest. 3. Matching in Practice \u00b6 Now that we have gone through available matching methods and how to combine them in the Lenticular Lens , we show their application in some case-studies aligning resources stemmed from various datasources of one\u2019s choice. We also provide example on the rdf export of the resulting linksets with metadata. For this purpose we choose as syntax the turtle format and RDFstar reification. 3.1 Simple Methods \u00b6 This case-study section aims to showcase matching problems involving a SINGLE matching method (Embedded, Exact, Intermediate, Levenshtein Distance, Soundex Distance, Gerrit Bloothooft, Word Intersection, List Intersection, Numbers and TeAM) run over one or multiple datasets. We call them Simple Methods as opposed to Complex Methods illustrated in the sequel. Keep in mind that the terms Simple and Complex refer to the use of single or combined methods and not to the algorithm complexity of the underlying the method(s). Case-1: Grid \u00b6 In this case study, displayed in Fig 4.10, the goal is to find out whether there exist duplicates Education Instances within the Grid\u2019s dataset. The dataset is composed of nine types of institutions including 27715 Companies , 19353 Educations , 12547 Nonprofit institutes, 12465 Healthcare institutes, 8499 Facility institutes, 5762 Government institutes, 2724 Archive institutes and 7823 institutes with no type specified. Although the dataset is of multiple types of entities, the case-study here aims only to deduplicate instances of type Education . This is depicted in Fig 4.10 where the Sources and Targets cards are GRID[Education] showing that the entity type Education has been selected within the GRID dataset. Case-1: Linkset Specifications Fig 4.10: An example showing how to deduplicate a dataset using edit distance with a user-defined threshold of 0.9. Also in the Matching Methods card, it can be seen that on both sides (source and target) two properties are selected for checking whether duplicates exist. This check relies on whether there exist entities that are documented within the GRID dataset with similar names using rdfs_label and skos_prefLabel . As the similarity score is measured in the interval 0 (not similar) to 1 (exactly similar), the threshold defined as 0.9 ensures that only paired entities with a high similarity (0.9 or above) are accepted. The same card shows the selected algorithm as Levenshtein Distance , which is run over the selected predicates generating 1,692 distinct links as shown in the statistics card (on the top). The latter card also provides statistics on: The number of entities at the Source and Target . In this particular case, over 19K educational institutes at both source and target as they are the same dataset. Such information provides hints on the maximum number of links to expect in the worst case scenario as well as an idea on how long the running algorithm could take. The number of entities matched at the subject and object positions. The number of clusters derived from the links found. Here, this provides a potentially better picture on the number of real entities, as co-referent are grouped together in clusters of various sizes. The Runtime durations informing on the elapsed time for (1) finding links and for (2) clustering them. In this Image 1, we deliberately choose two properties at both the Source and Target datasets for the deduplication. Choosing for more than one property either for the Source or Target triggers a combination of pairwise property-value matching joined with the logic operator OR . For example choosing properties x and y at the source while choosing only z at the target triggers the following pairwise combinations: ( x AND z ) OR ( y AND z ). In the current use-case, choosing for example rdfs_label and skos_prefLabel at both Source AND Target generates the following combination: rdfs_label AND rdfs_label OR rdfs_label AND skos_prefLabel OR skos_prefLabel AND skos_prefLabel . This explicit combination is implemented as an alternative complex method in the next section, where three executions of the `Levenshtein Distance algorithm is required, instead of one. Case-1: RDF Results. This section provides the complete metadata of the resulting Linkset for the specification above in Example 4.15, plus a sample of 9 links due to space limitation. From this metadata, a number of general statistical information on the linkset can be obtained, such as the number of distinct triples , entities or clusters , the number of links accepted or rejected and more. The metadata also presents a detailed description on the methods used to generate the links. For example, for each algorithm used, a uri and description is provided. This algorithm can be used in one or more methods, provided the link acceptance threshold , the vrange of the similarity score, the datasets , data-types and predicates uris used for link findings. Furthermore, a specific annotation is provided in an RDFstar format for each generated link. In this example, we have the strength of the link and whether the link has been validated ( accepted , rejected or not_validated ). Case-1: Turtle file sample ### PREDEFINED SHARED NAMESPACES ### @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix void: <http://rdfs.org/ns/void#> . @prefix dcterms: <http://purl.org/dc/terms/> . @prefix format: <http://www.w3.org/ns/formats/> . @prefix pav: <http://purl.org/ontology/similarity/> . @prefix cc: <http://creativecommons.org/ns#> . ### PREDEFINED SPECIFIC NAMESPACES ### @prefix ll: <http://data.goldenagents.org/ontology/> . @prefix ll_algo: <http://data.goldenagents.org/ontology/matching-method/> . @prefix ll_val: <http://data.goldenagents.org/ontology/validation/> . @prefix linkset: <http://data.goldenagents.org/resource/linkset/> . @prefix dataset: <http://data.goldenagents.org/resource/dataset/> . ### AUTOMATED NAMESPACES ### @prefix skos: <http://www.w3.org/2004/02/skos/core#> . @prefix institutes_S1: <http://www.grid.ac/institutes/> . ########################################### # GENERIC METADATA # ########################################### linkset: Grid a void: Linkset ; cc: attributionName \"LenticularLens\" ; void: feature format: Turtle ; cc: license <http://purl.org/NET/rdflicense/W3C1.0> ; ll: has-logic-formulation <http://data.goldenagents.org/resource/PHbb54a8dab0d2954> ; void: linkPredicate skos: exactMatch ; void: subjectsTarget <http://data.goldenagents.org/resource/dataset/Grid> ; void: objectsTarget <http://data.goldenagents.org/resource/dataset/Grid> ; dcterms: description \"Deduplication of entities of type Education in the GRID dataset\" @ en ; void: triples 1692 ; void: entities 1737 ; void: distinctSubjects 1737 ; void: distinctObjects 1737 ; ll: has-clusters 619 ; ll_val: has-validations 18 ; ll_val: has-accepted 3 ; ll_val: has-rejected 6 ; ll_val: has-remaining 1683 . ############################################# # LOGIC FORMULA PARTS # ############################################# <http://data.goldenagents.org/resource/PHbb54a8dab0d2954> a ll: LogicFormulation ; ll: has-method <http://data.goldenagents.org/resource/Normalised-EditDistance-H30d57e26e41bb04> ; ll: has-formula-description \"\"\"<http://data.goldenagents.org/resource/Normalised-EditDistance-H30d57e26e41bb04> \"\"\" . ############################################# # METHOD SIGNATURES # ############################################# ### ll_algo:Normalised-EditDistance ### <http://data.goldenagents.org/resource/Normalised-EditDistance-H30d57e26e41bb04> a ll:MatchingMethod ; ll:has-algorithm ll_algo:Normalised-EditDistance ; ll:has-threshold 0.9 ; ll:has-threshold-range \"]0, 1]\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Greater-than-or-equal-to> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> . ############################################# # METHOD DESCRIPTIONS # ############################################# ll_algo:Normalised-EditDistance a ll:MatchingAlgorithm ; dcterms:description \"\"\" This \u200bmethod is used to align \u200b\u200bsource a\u200bnd \u200b\u200btarget\u2019s IRIs whenever the similarity score of their respective user selected property values are \u200b\u200babove a given \u200bLevenshtein (edit) Distance threshold\u200b. Edit distance is a way of quantifying how \u200bdissimilar two strings (e.g., words) are to one another by counting the minimum number of operations \u200b\u03b5 \u200b(\u200bremoval, insertion, or substitution of a character in the string)\u200b required to transform one string into the other. For example, \u200bthe \u200bLevenshtein distance between kitten and sitting is \u200b\u03b5 \u200b= 3 as it requires a two substitutions (s for k and i for e) and one insertion of g at the end [https://en.wikipedia.org/wiki/Edit_distance]\u200b. \"\"\"@en . ############################################# # DATASET AND ENTITY SELECTIONS # ############################################# ### ENTITY SELECTION [SOURCE] N0: 1 ### <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> a ll:EntitySelection ; ll:has-dataset <http://data.goldenagents.org/resource/dataset/Grid> ; ll:has-entity-type <http://www.grid.ac/ontology/Education> . ############################################# # PREDICATE SELECTIONS # ############################################# ### PREDICATE SELECTED [SOURCE] N0: 1 ### <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> a ll:PropertySelection ; ll:has-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-predicate <http://www.w3.org/2000/01/rdf-schema#label> . ### PREDICATE SELECTED [SOURCE] N0: 2 ### <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> a ll:PropertySelection ; ll:has-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-predicate <http://www.w3.org/2004/02/skos/core#prefLabel> . ########################################### # ANNOTATED LINKSET # ########################################### linkset:Grid { <<institutes_S1:grid.1017.7 skos:exactMatch institutes_S1:grid.501980.5>> ll_val:has-validation \"rejected\" : ll:has-matching-strength 0.933 . <<institutes_S1:grid.1019.9 skos:exactMatch institutes_S1:grid.449929.b>> ll_val:has-validation \"accepted\" ; ll:has-matching-strength 1 . <<institutes_S1:grid.1020.3 skos:exactMatch institutes_S1:grid.266826.e>> ll_val:has-validation \"not_validated\" ; ll:has-matching-strength 1 . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.10347.31>> ll_val:has-validation \"rejected\" ; ll:has-matching-strength 0.950 . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.10595.38>> ll_val:has-validation \"rejected\" ; ll:has-matching-strength 0.900 . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.4462.4>> ll_val:has-validation \"rejected\" ; ll:has-matching-strength 0.900 . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.10595.38>> ll_val:has-validation \"rejected\" ; ll:has-matching-strength 0.900 . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.441173.4>> ll_val:has-validation \"rejected\" ; ll:has-matching-strength 0.900 . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.4462.4>> lll_val:has-validation \"accepted\" ; ll:has-matching-strength 0.900 . \u2022 \u2022 \u2022 } 3.2 Complex Methods \u00b6 Case-1: Alternative \u00b6 In Fig 4.11 is displayed an alternative where Case-1: Linkset Specifications Fig 4.11: An example showing how to deduplicate a dataset using an edit distance with threshold 0.9. Case-1: RDF Results Case-1: Turtle file sample ########################################### # NAMESPACES # ########################################### ### PREDEFINED SHARED NAMESPACES @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix void: <http://rdfs.org/ns/void#> . @prefix dcterms: <http://purl.org/dc/terms/> . @prefix format: <http://www.w3.org/ns/formats/> . @prefix pav: <http://purl.org/ontology/similarity/> . @prefix cc: <http://creativecommons.org/ns#> . ### PREDEFINED SPECIFIC NAMESPACES @prefix ll: <http://data.goldenagents.org/ontology/> . @prefix ll_algo: <http://data.goldenagents.org/ontology/matching-method/> . @prefix ll_val: <http://data.goldenagents.org/ontology/validation/> . @prefix linkset: <http://data.goldenagents.org/resource/linkset/> . @prefix dataset: <http://data.goldenagents.org/resource/dataset/> . ### AUTOMATED NAMESPACES @prefix skos: <http://www.w3.org/2004/02/skos/core#> . @prefix institutes_S1: <http://www.grid.ac/institutes/> . ############################################################################################################## # GENERIC METADATA # ############################################################################################################## linkset: Grid_2 a void: Linkset ; cc: attributionName \"LenticularLens\" ; void: feature format: Turtle ; cc: license <http://purl.org/NET/rdflicense/W3C1.0> ; ll: has-logic-formulation <http://data.goldenagents.org/resource/PH1ec0ee6f368dd62> ; void: linkPredicate skos: exactMatch ; void: subjectsTarget <http://data.goldenagents.org/resource/dataset/Grid> ; void: objectsTarget <http://data.goldenagents.org/resource/dataset/Grid> ; dcterms: description \"Deduplication of entities of type Education in the GRID dataset\" @ en ; void: triples 1692 ; void: entities 1737 ; void: distinctSubjects 1737 ; void: distinctObjects 1737 ; ll: has-clusters 619 ; ll_val: has-validations 18 ; ll_val: has-accepted 3 ; ll_val: has-rejected 6 ; ll_val: has-remaining 1683 . ################################################################################ # LOGIC FORMULA PARTS # ################################################################################ <http://data.goldenagents.org/resource/PH1ec0ee6f368dd62> a ll: LogicFormulation ; ll: has-method <http://data.goldenagents.org/resource/Normalised-EditDistance-H779a0ad1b5e5f93> ; ll: has-method <http://data.goldenagents.org/resource/Normalised-EditDistance-H3de4966a0b8aa01> ; ll: has-method <http://data.goldenagents.org/resource/Normalised-EditDistance-H11cbb0cc77c44a9> ; ll: has-formula-description \"\"\"<http://data.goldenagents.org/resource/Normalised-EditDistance-H779a0ad1b5e5f93> and (\u22a4min) <http://data.goldenagents.org/resource/Normalised-EditDistance-H3de4966a0b8aa01> and (\u22a4min) <http://data.goldenagents.org/resource/Normalised-EditDistance-H11cbb0cc77c44a9> \"\"\" . ################################################################################ # METHOD SIGNATURES # ################################################################################ ### ll_algo:Normalised-EditDistance <http://data.goldenagents.org/resource/Normalised-EditDistance-H779a0ad1b5e5f93> a ll:MatchingMethod ; ll:has-algorithm ll_algo:Normalised-EditDistance ; ll:has-threshold 0.9 ; ll:has-threshold-range \"]0, 1]\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Greater-than-or-equal-to> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> . ### ll_algo:Normalised-EditDistance <http://data.goldenagents.org/resource/Normalised-EditDistance-H3de4966a0b8aa01> a ll:MatchingMethod ; ll:has-algorithm ll_algo:Normalised-EditDistance ; ll:has-threshold 0.9 ; ll:has-threshold-range \"]0, 1]\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Greater-than-or-equal-to> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> . ### ll_algo:Normalised-EditDistance <http://data.goldenagents.org/resource/Normalised-EditDistance-H11cbb0cc77c44a9> a ll:MatchingMethod ; ll:has-algorithm ll_algo:Normalised-EditDistance ; ll:has-threshold 0.9 ; ll:has-threshold-range \"]0, 1]\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Greater-than-or-equal-to> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> . ################################################################################ # METHOD DESCRIPTIONS # ################################################################################ ll_algo:Normalised-EditDistance a ll:MatchingAlgorithm ; dcterms:description \"\"\" This \u200bmethod is used to align \u200b\u200bsource a\u200bnd \u200b\u200btarget\u2019s IRIs whenever the similarity score of their respective user selected property values are \u200b\u200babove a given \u200bLevenshtein (edit) Distance threshold\u200b. Edit distance is a way of quantifying how \u200bdissimilar two strings (e.g., words) are to one another by counting the minimum number of operations \u200b\u03b5 \u200b(\u200bremoval, insertion, or substitution of a character in the string)\u200b required to transform one string into the other. For example, \u200bthe \u200bLevenshtein distance between kitten and sitting is \u200b\u03b5 \u200b= 3 as it requires a two substitutions (s for k and i for e) and one insertion of g at the end [https://en.wikipedia.org/wiki/Edit_distance]\u200b. \"\"\"@en . ################################################################################ # DATASET AND ENTITY SELECTIONS # ################################################################################ ### ENTITY SELECTION [SOURCE] N0: 1 ### <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> a ll:EntitySelection ; ll:has-dataset <http://data.goldenagents.org/resource/dataset/Grid> ; ll:has-entity-type <http://www.grid.ac/ontology/Education> . ################################################################################ # PREDICATE SELECTIONS # ################################################################################ ### PREDICATE SELECTED [SOURCE] N0: 1 ### <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> a ll:PropertySelection ; ll:has-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-predicate <http://www.w3.org/2000/01/rdf-schema#label> . ### PREDICATE SELECTED [TARGET] N0: 2 ### <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> a ll:PropertySelection ; ll:has-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-predicate <http://www.w3.org/2004/02/skos/core#prefLabel> . ############################################################################################################## # ANNOTATED LINKSET # ############################################################################################################## linkset:Grid_2 { <<institutes_S1:grid.1017.7 skos:exactMatch institutes_S1:grid.501980.5>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.1019.9 skos:exactMatch institutes_S1:grid.449929.b>> ll_val:has-validation \"accepted\" . <<institutes_S1:grid.1020.3 skos:exactMatch institutes_S1:grid.266826.e>> ll_val:has-validation \"accepted\" . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.10347.31>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.10595.38>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.4462.4>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.10595.38>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.441173.4>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.4462.4>> ll_val:has-validation \"accepted\" . <<institutes_S1:grid.10373.36 skos:exactMatch institutes_S1:grid.266769.a>> ll_val:has-validation \"not_validated\" . \u2022 \u2022 \u2022 } Case-2: Getty \u00b6 In Fig 4.12 is displayed an alternative where Case-2: Linkset Specifications. Fig 4.12: An example showing how to deduplicate a dataset using an edit distance with threshold 0.9. Case-2: RDF Results. Case-2: Turtle file sample. NAMESPACES \u00b6 PREDEFINED SHARED NAMESPACES \u00b6 @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix void: <http://rdfs.org/ns/void#> . @prefix dcterms: <http://purl.org/dc/terms/> . @prefix format: <http://www.w3.org/ns/formats/> . @prefix pav: <http://purl.org/ontology/similarity/> . @prefix cc: <http://creativecommons.org/ns#> . PREDEFINED SPECIFIC NAMESPACES \u00b6 @prefix ll: <http://data.goldenagents.org/ontology/> . @prefix ll_algo: <http://data.goldenagents.org/ontology/matching-method/> . @prefix ll_val: <http://data.goldenagents.org/ontology/validation/> . @prefix linkset: <http://data.goldenagents.org/resource/linkset/> . @prefix dataset: <http://data.goldenagents.org/resource/dataset/> . AUTOMATED NAMESPACES \u00b6 @prefix skos: <http://www.w3.org/2004/02/skos/core#> . @prefix institutes_S1: <http://www.grid.ac/institutes/> . @prefix time: <http://www.w3.org/2006/time#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix Person_S1: <http://goldenagents.org/uva/SAA/Person/> . @prefix PersonName_T1: <https://data.goldenagents.org/datasets/SAA/PersonName/> . GENERIC METADATA \u00b6 linkset:Getty a void:Linkset ; cc:attributionName \"LenticularLens\" ; void:feature format:Turtle ; cc:license <http://purl.org/NET/rdflicense/W3C1.0> ; ll:has-logic-formulation <http://data.goldenagents.org/resource/PH6d47b550d1695d5> ; void:linkPredicate owl:sameAs ; void:subjectsTarget <https://data.goldenagents.org/datasets/ufab7d657a250e3461361c982ce9b38f3816e0c4b/frick_collection_montias_data_20200604> ; void:subjectsTarget <https://data.goldenagents.org/datasets/ufab7d657a250e3461361c982ce9b38f3816e0c4b/getty_provenance_index_montias_data_20200604> ; void:objectsTarget <https://data.goldenagents.org/datasets/ufab7d657a250e3461361c982ce9b38f3816e0c4b/index_op_notarieel_archief_enriched_20191202> ; dcterms:description \"Deduplication of entities of type Education in the GRID dataset\"@en ; void:triples 147 ; void:entities 261 ; void:distinctSubjects 135 ; void:distinctObjects 126 ; ll:has-clusters 117 ; ll_val:has-remaining 147 . ################################################################################ # LOGIC FORMULA PARTS # ################################################################################ http://data.goldenagents.org/resource/PH6d47b550d1695d5 a ll:LogicFormulation ; ll:has-method <http://data.goldenagents.org/resource/Normalised-Soundex-H4970fc2fe79ea5f> ; ll:has-method <http://data.goldenagents.org/resource/Exact-H918a02351d48ca9> ; ll:has-method <http://data.goldenagents.org/resource/Time-Delta-Hdcc5070996853e9> ; ll:has-formula-description \"\"\"<http://data.goldenagents.org/resource/Normalised-Soundex-H4970fc2fe79ea5f> and (\u22a4min) <http://data.goldenagents.org/resource/Exact-H918a02351d48ca9> and (\u22a4min) <http://data.goldenagents.org/resource/Time-Delta-Hdcc5070996853e9> \"\"\" . ################################################################################ # METHOD SIGNATURES # ################################################################################ ll_algo:Normalised-Soundex \u00b6 http://data.goldenagents.org/resource/Normalised-Soundex-H4970fc2fe79ea5f a ll:MatchingMethod ; ll:has-algorithm ll_algo:Normalised-Soundex ; ll:has-threshold 0.85 ; ll:has-threshold-range \"]0, 1]\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Greater-than-or-equal-to> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH914a94c6f3c93b4> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH10aaeebb6832fdf> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH7879419327d373d> . ll_algo:Exact \u00b6 http://data.goldenagents.org/resource/Exact-H918a02351d48ca9 a ll:MatchingMethod ; ll:has-algorithm ll_algo:Exact ; ll:has-threshold 1 ; ll:has-threshold-range \"1\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Equal> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH46d91d4f6e2209e> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0df0471cb5df515> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHe3cb3236c5b11b1> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHb2a681013fbb430> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH9f7bd41ea902bd0> . ll_algo:Time-Delta \u00b6 http://data.goldenagents.org/resource/Time-Delta-Hdcc5070996853e9 a ll:MatchingMethod ; ll:has-algorithm ll_algo:Time-Delta ; ll:has-threshold 0 ; ll:has-threshold-range \"\u2115\" ; time:unitType time:unitYear ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Equal> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHeb98e7f77b22fce> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHf741a098569afb1> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHee886bb3d021a6a> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH715d032180bd40c> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHe349eb18e5ba638> . ################################################################################ # METHOD DESCRIPTIONS # ################################################################################ ll_algo:Normalised-Soundex a ll:MatchingAlgorithm ; dcterms:description \u201c\u201d\u201d \u201cSoundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for ho- mophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first let- ter\u201d [ https://en.wikipedia.org/wiki/Soundex ]. In the Lenticular Lens, Soundex is used as a normaliser in the sense that an edit distance is run over the soundex code version of a name. For example, the in the table below, the normalisation of both Louijs Roc- ourt and `Lowis Ricourt becomes L200 R263 leading to an edit distance of 0 and a relative strength of 1. However, computing the same names using directly an edit distance results in an edit distance of 3 and a relative matching strength of 0. 79. -------------- -- Example -- THE USE OF SOUNDEX CODE FOR STRING APPROXIMATION -------------- The example below shows the implementation of Soundex Distance in the Lenticular Lens and how it compares with Edit Distance over the original names (no soundex-based normalisation). ------------------------------------------------------------------------------------------------------------------------------------------------------ Source Target E. Dist Rel. distance Source soundex Target soundex Code E. Dist Code Rel. Dist ------------------------------------------------------------------------------------------------------------------------------------------------------ Jasper Cornelisz. Lodder Jaspar Cornelisz Lodder 2 0.92 J216 C654 L360 J216 C654 L360 0 1.0 Barent Teunis Barent Teunisz gen. Drent 12 0.52 B653 T520 B653 T520 G500 D653 10 0.47 Louijs Rocourt Louys Rocourt 2 0.86 L200 R263 L200 R263 0 1.0 Louijs Rocourt Lowis Ricourt 3 0.79 L200 R263 L200 R263 0 1.0 Louys Rocourt Lowis Ricourt 3 0.77 L200 R263 L200 R263 0 1.0 Cornelis Dircksz. Clapmus Cornelis Clapmuts 10 0.6 C654 D620 C415 C654 C415 5 0.64 Geertruydt van den Breemde Geertruijd van den Bremde 4 0.85 G636 V500 D500 B653 G636 V500 D500 B653 \"\"\"@en . ll_algo:Exact a ll:MatchingAlgorithm ; dcterms:description \u201c\u201d\u201d Aligns source and target\u2019s IRIs whenever their respective user selected property values are identical.\u201d\u201c\u201d@en . ll_algo:Time-Delta a ll:MatchingAlgorithm ; dcterms:description \u201c\u201d\u201d 10.1 Time Delta. This function allows for finding co-referent entities on the basis of a minimum time dif- ference between the times reported by the source and the target entities. For example, if the value zero is assigned to the time difference parameter, then, for a matched to be found, the time of the target and the one of the source are to be the exact same times. While accounting for margins of error, one may consider a pair of entities to be co-referent if the real entities are born lambda days, months or years apart among other-things (similar name, place..). \u201c\u201d\u201c@en . ################################################################################ # DATASET AND ENTITY SELECTIONS # ################################################################################ ENTITY SELECTION [SOURCE] N0: 1 \u00b6 http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc a ll:EntitySelection ; ll:has-dataset https://data.goldenagents.org/datasets/ufab7d657a250e3461361c982ce9b38f3816e0c4b/frick_collection_montias_data_20200604 ; ll:has-entity-type https://data.goldenagents.org/datasets/SAA/ontology/Person . ENTITY SELECTION [SOURCE] N0: 2 \u00b6 http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36 a ll:EntitySelection ; ll:has-dataset https://data.goldenagents.org/datasets/ufab7d657a250e3461361c982ce9b38f3816e0c4b/getty_provenance_index_montias_data_20200604 ; ll:has-entity-type https://data.goldenagents.org/datasets/SAA/ontology/Person . ENTITY SELECTION [TARGET] N0: 3 \u00b6 http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 a ll:EntitySelection ; ll:has-dataset https://data.goldenagents.org/datasets/ufab7d657a250e3461361c982ce9b38f3816e0c4b/index_op_notarieel_archief_enriched_20191202 ; ll:has-entity-type https://w3id.org/pnv#PersonName . ################################################################################ # PREDICATE SELECTIONS # ################################################################################ PREDICATE SELECTED [SOURCE] N0: 1 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PH914a94c6f3c93b4 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc ; ll:has-predicate http://www.w3.org/2000/01/rdf-schema#label . PREDICATE SELECTED [SOURCE] N0: 2 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PH10aaeebb6832fdf a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36 ; ll:has-predicate http://www.w3.org/2000/01/rdf-schema#label . PREDICATE SELECTED [TARGET] N0: 3 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PH7879419327d373d a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://www.w3.org/2000/01/rdf-schema#label . PREDICATE SELECTED [SOURCE] N0: 4 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PH46d91d4f6e2209e a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36 ; ll:has-predicate http://data.goldenagents.org/resource/PH1df7dbfdf1d1eb8 . http://data.goldenagents.org/resource/PH1df7dbfdf1d1eb8 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Inventory ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/documentedIn ; rdf:_4 https://data.goldenagents.org/datasets/SAA/ontology/InventoryBook ; rdf:_5 https://data.goldenagents.org/datasets/SAA/ontology/inventoryNumber . PREDICATE SELECTED [SOURCE] N0: 5 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PH0df0471cb5df515 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc ; ll:has-predicate http://data.goldenagents.org/resource/PH1df7dbfdf1d1eb8 . PREDICATE SELECTED [TARGET] N0: 6 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PHe3cb3236c5b11b1 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PH7334cc09b832a17 . http://data.goldenagents.org/resource/PH7334cc09b832a17 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/HuwelijkseVoorwaarden ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/inventoryNumber . PREDICATE SELECTED [TARGET] N0: 7 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PHb2a681013fbb430 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PH00d91362d72928f . http://data.goldenagents.org/resource/PH00d91362d72928f a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Boedelinventaris ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/inventoryNumber . PREDICATE SELECTED [TARGET] N0: 8 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PH9f7bd41ea902bd0 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PH9f325d76d5fa623 . http://data.goldenagents.org/resource/PH9f325d76d5fa623 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Boedelscheiding ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/inventoryNumber . PREDICATE SELECTED [SOURCE] N0: 9 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PHeb98e7f77b22fce a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36 ; ll:has-predicate http://data.goldenagents.org/resource/PH22c48ebe6b24223 . http://data.goldenagents.org/resource/PH22c48ebe6b24223 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Inventory ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/registrationDate . PREDICATE SELECTED [SOURCE] N0: 10 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PHf741a098569afb1 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc ; ll:has-predicate http://data.goldenagents.org/resource/PH22c48ebe6b24223 . PREDICATE SELECTED [TARGET] N0: 11 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PHee886bb3d021a6a a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PHda68158d0b9f392 . http://data.goldenagents.org/resource/PHda68158d0b9f392 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/HuwelijkseVoorwaarden ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/registrationDate . PREDICATE SELECTED [TARGET] N0: 12 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PH715d032180bd40c a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PH956023596f37d1b . http://data.goldenagents.org/resource/PH956023596f37d1b a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Boedelinventaris ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/registrationDate . PREDICATE SELECTED [TARGET] N0: 13 \u00b6 http://data.goldenagents.org/resource/PredicateSelection-PHe349eb18e5ba638 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PH0ef342da86f3226 . http://data.goldenagents.org/resource/PH0ef342da86f3226 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Boedelscheiding ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/registrationDate . ANNOTATED LINKSET \u00b6 linkset:Getty { < > ll_val:has-validation \u201cnot_validated\u201d . \u2022 \u2022 \u2022 } ```","title":"6. Linkset - Link Construction"},{"location":"05.LinkConstruction/#link-construction","text":"Linking co-referent entities across a variety of datasources is a pragmatic and fast way to seamlessly navigate across datasets without having to agree in a uniform vocabulary. This solution offered in the Semantic Web architecture appears attractive as the ultimate goal for the researcher executing this task is not the integration of data but the extraction of vital information for reaching valid conclusions about problems under scrutiny. This said, the Lenticular Lens offers means to reach that ultimate goal of the researcher while making sure that the steps taken by the researcher are documented such that other researchers can easily re-generate the data leading to specific conclusions if need be. Along the way of entity-based data integration and data extraction, the Lenticular Lens aims to document among others: The datasources to integrate; The reasons behind a specific integration; The entity types and restrictions that ensure correctness in bridging across datasources of interest; The matching methods and specifications justifying the existence of a set of links. The Lenticular Lens tool aims to provide generic methods that allows a broader audience suffering the same need for data integration. The first step in creating and documenting links using the Lenticular Lens is defining the scope in witch links are to be created and possibly validated. For that, the tool offers the RESEARCH menu followed by the SELECT and CONSTRUCT menus. We now go through each of these first three menus underlying the existence of links.","title":"LINK CONSTRUCTION"},{"location":"05.LinkConstruction/#1-scope","text":"The RESEARCH menu is the starting point in learning how to interact with the Lenticular Lens tool. In general, a research question somehow sets the scope in which link creations, manipulations or validation take place. This provides the first building block supporting the user with defining the context in which a particular alignment is generated. Using this menu, part of the context is made explicit by selecting the datasets and entity types necessary to continue the investigation. As an overview, the RESEARCH menu provides researchers with means to describe the research of interest in terms of: Research Question for inserting the main research question driving the integration. Hypothesis for pointing out the hypothesis in mind prior to the data extraction. Link and Citation to ensure that, if the results happen to be published, the researcher still has the facility to add a link to the publication and and a bibliographic reference for future reuse. Fig. 4.1 illustrates the different fields to be filled in by the researcher for a quick overview of what can happen in this research project and why. Once providing the information is done, the Save button at the bottom of the page can be clicked to save the provided information and exit the Lenticular Lens if the user which to continue with other tasks. Or, should the user choose to continue the alternative Save and next button can be used to save the project and move to the next window. Fig. 4.1: Describing the scope of he research question.","title":"1. Scope"},{"location":"05.LinkConstruction/#2-data","text":"In the previous step or window, the researcher has defined the scope of the research for which data are to be extracted and analysed. In this second window labelled SELECT , the user is to describe and select the entity types involved in his research. For that, the location of the datasource needs to be provided and the datasets in which the respective entities of interest reside need to be selected.","title":"2. Data"},{"location":"05.LinkConstruction/#21-data-selection","text":"As the user activates the Saves and next button at the end of the previous page, she is presented with a new window with a single card labelled Entity-Type Selection 1 as presented in Fig. 4.2. The plus button at the right side of the picture enables the user to create new cards when needed while the arrow-head button at the left side of the card\u2019s label allows for the unveiling of the card as displayed in Fig 3. Fig. 4.2: The card view for data selection Describing the type of an entity can be done using the Description text box for each entity type. To provide the location of the data, the GraphQL Endpoint text box can be use to fill in the URL of any GraphQL end point. Once the endpoint is given and loaded, a dataset can be selected from the list of datasets available at the provided endpoint. The selection of a dataset will prompt a new dropdown text box as Entity type , providing the user with the facility to select the entity type of interest. After loading the provided URL of the default Golden Agent\u2019s endpoint, Fig. 4.3 shows the list of datasets available at that location to choose from. Fig. 4.3: List of datasets available at the default Golden Agent\u2019s GraphQL endpoint.","title":"2.1 Data Selection"},{"location":"05.LinkConstruction/#22-data-restrictions","text":"If need be to filter entities based on specific conditions, this is also possible with the Filter card shown in Fig. 4.6. Fig. 4.6: The card for defining entity restrictions. Once the button is clicked, this card presents the user with a Filter-Logic box which enable the creation of a relatively complex and versatile entity restrictions. Fig. 4.7 for example show the list of available filtering options while Fig. 4.8 illustrates an example where the has minimum date and has maximum date filtering options are used to isolate entities of interest. These entities are now those between with a registration date between [1600, 1659] and having their respective literal name exempt of trailing dots (\u2026). Fig. 4.7: List of restriction options. Fig. 4.8: The card for defining entity restrictions.","title":"2.2 Data Restrictions"},{"location":"05.LinkConstruction/#221-restriction-options","text":"Equal to / Not Equal to. This option allows one to select entities that have the value of a certain property equal (or not) to a certain value. For example, all entities with property ex:workLocation equal (or not) to Amsterdam . Contains / Does not contain. This option is used to make sure that the property-value of the entities of interest contains or does not contain a specific sequence of characters. For example, %...% could be used for (i) excluding people whose names contain trailing dots or (ii) to select those entities to apply a particular modification onto their names, like adding the surname of the father for a baptised child whose surname is given as ... . Has property / Has no property. This option is used to select entities based on the existence (or not) of a certain property. Let assume, for example, that the user is interested in entities that are parents. This option allows one to filter all entities for which the a value exists for the property ex:parentOf for example. It also allows you to exclude all entities that are parents if the option Has no property is used instead. Has minimum / maximum value. This option allows for restricting entities to be within or outside a specified range given user\u2019s specified property-values of type number over which the restriction can be applied. To delimit both upper and lower bounds, the user can combine minimum and maximum using the logical box AND. Has minimum / maximum date. This option allows for restricting entities to be within or outside a specified range given user\u2019s specified property-values of type date over which the restriction can be applied. Within this option, a date format can be specified. The default format is YYYY-MM-DD . The values 10, 300 and 1990 for example will be considered as year while 10-1, 300-1 and 1990-1 will be considered as the first month of the respective year values. To delimit both upper and lower bounds, the user can combine minimum and maximum using the logical box AND. Has minimum / maximum appearances. This option allows for restricting entities for which a given property value occurs within a specified range . For example, to avoid excessive number of possible matches, one can delimit that only entities whose name value occur less than 5 times in the dataset will be included. To delimit both upper and lower bounds, the user can combine minimum and maximum using the logical box AND. In set. This option allows the filtering of a collection of resources of interest based on a set of resources. These set of resources is not manually provided but can be obtained through a list of existing linksets or lenses. The example below provides a detailed understanding of this filtering approach. Example 1: IN SET Two collections A and B to be matched via whatever method would create a se of links labelled linkset-AB. However, we are only interested in a subset of linkset-AB, such that it\u2019s resources (subject, object or both) are present in another given set, namely an input-linkset I. For efficiency purposes, linkset-AB does not need to be fully created to be filtered later on. This implies that the collections A and/or B need to be filtered such that A\u2019 = A \u2229 I and/or B\u2019 = B \u2229 I before executing the matching algorithm. ###################################################### # Linksets as named graphs # ###################################################### ex: input-linkset { A: Chiara owl: sameAs C: Latronico . A: Al owl: sameAs C: Al_Idrissou . A: Al owl: sameAs C: Al_Koudous . } ex: linkset-AB { A: Chiara owl: sameAs B: Chiara . A: Kerim owl: sameAs B: Kerim . } ###################################################### # In Resource Set # ###################################################### ### The set S of resources from input-linkset is: ### S = {A:Chiara, A:Al, C:Latronico, C:Al_Idrissou, C:Al_Koudous} ex: linkset-SubjectInSet { A: Chiara owl: sameAs B: Chiara . }","title":"2.2.1 Restriction options"},{"location":"05.LinkConstruction/#23-data-exploration","text":"At this point, successfully providing the required information ( Dataset and Entity-Type ) triggers the appearance of the Explore Sample button at the right side of the card\u2019s label ( Entity-type selection 1 ) as displayed in Fig. 4.4. As illustrated in Fig. 4.5, with this button, users are now able to explore information of their choice about the entities of interest by selecting properties describing them. Keep in mind that this feature is only intended as exploration alternative to make sure of the choices (dataset, entity-type and restrictions) made. Fig. 4.4: The Explore sample button shows only after the entity type is selected. Fig. 4.5: Exploring the description of entities of choice, stemmed from the dataset of interest.","title":"2.3 Data Exploration"},{"location":"05.LinkConstruction/#3-matching-in-practice","text":"Now that we have gone through available matching methods and how to combine them in the Lenticular Lens , we show their application in some case-studies aligning resources stemmed from various datasources of one\u2019s choice. We also provide example on the rdf export of the resulting linksets with metadata. For this purpose we choose as syntax the turtle format and RDFstar reification.","title":"3. Matching in Practice"},{"location":"05.LinkConstruction/#31-simple-methods","text":"This case-study section aims to showcase matching problems involving a SINGLE matching method (Embedded, Exact, Intermediate, Levenshtein Distance, Soundex Distance, Gerrit Bloothooft, Word Intersection, List Intersection, Numbers and TeAM) run over one or multiple datasets. We call them Simple Methods as opposed to Complex Methods illustrated in the sequel. Keep in mind that the terms Simple and Complex refer to the use of single or combined methods and not to the algorithm complexity of the underlying the method(s).","title":"3.1 Simple Methods"},{"location":"05.LinkConstruction/#case-1-grid","text":"In this case study, displayed in Fig 4.10, the goal is to find out whether there exist duplicates Education Instances within the Grid\u2019s dataset. The dataset is composed of nine types of institutions including 27715 Companies , 19353 Educations , 12547 Nonprofit institutes, 12465 Healthcare institutes, 8499 Facility institutes, 5762 Government institutes, 2724 Archive institutes and 7823 institutes with no type specified. Although the dataset is of multiple types of entities, the case-study here aims only to deduplicate instances of type Education . This is depicted in Fig 4.10 where the Sources and Targets cards are GRID[Education] showing that the entity type Education has been selected within the GRID dataset. Case-1: Linkset Specifications Fig 4.10: An example showing how to deduplicate a dataset using edit distance with a user-defined threshold of 0.9. Also in the Matching Methods card, it can be seen that on both sides (source and target) two properties are selected for checking whether duplicates exist. This check relies on whether there exist entities that are documented within the GRID dataset with similar names using rdfs_label and skos_prefLabel . As the similarity score is measured in the interval 0 (not similar) to 1 (exactly similar), the threshold defined as 0.9 ensures that only paired entities with a high similarity (0.9 or above) are accepted. The same card shows the selected algorithm as Levenshtein Distance , which is run over the selected predicates generating 1,692 distinct links as shown in the statistics card (on the top). The latter card also provides statistics on: The number of entities at the Source and Target . In this particular case, over 19K educational institutes at both source and target as they are the same dataset. Such information provides hints on the maximum number of links to expect in the worst case scenario as well as an idea on how long the running algorithm could take. The number of entities matched at the subject and object positions. The number of clusters derived from the links found. Here, this provides a potentially better picture on the number of real entities, as co-referent are grouped together in clusters of various sizes. The Runtime durations informing on the elapsed time for (1) finding links and for (2) clustering them. In this Image 1, we deliberately choose two properties at both the Source and Target datasets for the deduplication. Choosing for more than one property either for the Source or Target triggers a combination of pairwise property-value matching joined with the logic operator OR . For example choosing properties x and y at the source while choosing only z at the target triggers the following pairwise combinations: ( x AND z ) OR ( y AND z ). In the current use-case, choosing for example rdfs_label and skos_prefLabel at both Source AND Target generates the following combination: rdfs_label AND rdfs_label OR rdfs_label AND skos_prefLabel OR skos_prefLabel AND skos_prefLabel . This explicit combination is implemented as an alternative complex method in the next section, where three executions of the `Levenshtein Distance algorithm is required, instead of one. Case-1: RDF Results. This section provides the complete metadata of the resulting Linkset for the specification above in Example 4.15, plus a sample of 9 links due to space limitation. From this metadata, a number of general statistical information on the linkset can be obtained, such as the number of distinct triples , entities or clusters , the number of links accepted or rejected and more. The metadata also presents a detailed description on the methods used to generate the links. For example, for each algorithm used, a uri and description is provided. This algorithm can be used in one or more methods, provided the link acceptance threshold , the vrange of the similarity score, the datasets , data-types and predicates uris used for link findings. Furthermore, a specific annotation is provided in an RDFstar format for each generated link. In this example, we have the strength of the link and whether the link has been validated ( accepted , rejected or not_validated ). Case-1: Turtle file sample ### PREDEFINED SHARED NAMESPACES ### @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix void: <http://rdfs.org/ns/void#> . @prefix dcterms: <http://purl.org/dc/terms/> . @prefix format: <http://www.w3.org/ns/formats/> . @prefix pav: <http://purl.org/ontology/similarity/> . @prefix cc: <http://creativecommons.org/ns#> . ### PREDEFINED SPECIFIC NAMESPACES ### @prefix ll: <http://data.goldenagents.org/ontology/> . @prefix ll_algo: <http://data.goldenagents.org/ontology/matching-method/> . @prefix ll_val: <http://data.goldenagents.org/ontology/validation/> . @prefix linkset: <http://data.goldenagents.org/resource/linkset/> . @prefix dataset: <http://data.goldenagents.org/resource/dataset/> . ### AUTOMATED NAMESPACES ### @prefix skos: <http://www.w3.org/2004/02/skos/core#> . @prefix institutes_S1: <http://www.grid.ac/institutes/> . ########################################### # GENERIC METADATA # ########################################### linkset: Grid a void: Linkset ; cc: attributionName \"LenticularLens\" ; void: feature format: Turtle ; cc: license <http://purl.org/NET/rdflicense/W3C1.0> ; ll: has-logic-formulation <http://data.goldenagents.org/resource/PHbb54a8dab0d2954> ; void: linkPredicate skos: exactMatch ; void: subjectsTarget <http://data.goldenagents.org/resource/dataset/Grid> ; void: objectsTarget <http://data.goldenagents.org/resource/dataset/Grid> ; dcterms: description \"Deduplication of entities of type Education in the GRID dataset\" @ en ; void: triples 1692 ; void: entities 1737 ; void: distinctSubjects 1737 ; void: distinctObjects 1737 ; ll: has-clusters 619 ; ll_val: has-validations 18 ; ll_val: has-accepted 3 ; ll_val: has-rejected 6 ; ll_val: has-remaining 1683 . ############################################# # LOGIC FORMULA PARTS # ############################################# <http://data.goldenagents.org/resource/PHbb54a8dab0d2954> a ll: LogicFormulation ; ll: has-method <http://data.goldenagents.org/resource/Normalised-EditDistance-H30d57e26e41bb04> ; ll: has-formula-description \"\"\"<http://data.goldenagents.org/resource/Normalised-EditDistance-H30d57e26e41bb04> \"\"\" . ############################################# # METHOD SIGNATURES # ############################################# ### ll_algo:Normalised-EditDistance ### <http://data.goldenagents.org/resource/Normalised-EditDistance-H30d57e26e41bb04> a ll:MatchingMethod ; ll:has-algorithm ll_algo:Normalised-EditDistance ; ll:has-threshold 0.9 ; ll:has-threshold-range \"]0, 1]\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Greater-than-or-equal-to> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> . ############################################# # METHOD DESCRIPTIONS # ############################################# ll_algo:Normalised-EditDistance a ll:MatchingAlgorithm ; dcterms:description \"\"\" This \u200bmethod is used to align \u200b\u200bsource a\u200bnd \u200b\u200btarget\u2019s IRIs whenever the similarity score of their respective user selected property values are \u200b\u200babove a given \u200bLevenshtein (edit) Distance threshold\u200b. Edit distance is a way of quantifying how \u200bdissimilar two strings (e.g., words) are to one another by counting the minimum number of operations \u200b\u03b5 \u200b(\u200bremoval, insertion, or substitution of a character in the string)\u200b required to transform one string into the other. For example, \u200bthe \u200bLevenshtein distance between kitten and sitting is \u200b\u03b5 \u200b= 3 as it requires a two substitutions (s for k and i for e) and one insertion of g at the end [https://en.wikipedia.org/wiki/Edit_distance]\u200b. \"\"\"@en . ############################################# # DATASET AND ENTITY SELECTIONS # ############################################# ### ENTITY SELECTION [SOURCE] N0: 1 ### <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> a ll:EntitySelection ; ll:has-dataset <http://data.goldenagents.org/resource/dataset/Grid> ; ll:has-entity-type <http://www.grid.ac/ontology/Education> . ############################################# # PREDICATE SELECTIONS # ############################################# ### PREDICATE SELECTED [SOURCE] N0: 1 ### <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> a ll:PropertySelection ; ll:has-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-predicate <http://www.w3.org/2000/01/rdf-schema#label> . ### PREDICATE SELECTED [SOURCE] N0: 2 ### <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> a ll:PropertySelection ; ll:has-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-predicate <http://www.w3.org/2004/02/skos/core#prefLabel> . ########################################### # ANNOTATED LINKSET # ########################################### linkset:Grid { <<institutes_S1:grid.1017.7 skos:exactMatch institutes_S1:grid.501980.5>> ll_val:has-validation \"rejected\" : ll:has-matching-strength 0.933 . <<institutes_S1:grid.1019.9 skos:exactMatch institutes_S1:grid.449929.b>> ll_val:has-validation \"accepted\" ; ll:has-matching-strength 1 . <<institutes_S1:grid.1020.3 skos:exactMatch institutes_S1:grid.266826.e>> ll_val:has-validation \"not_validated\" ; ll:has-matching-strength 1 . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.10347.31>> ll_val:has-validation \"rejected\" ; ll:has-matching-strength 0.950 . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.10595.38>> ll_val:has-validation \"rejected\" ; ll:has-matching-strength 0.900 . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.4462.4>> ll_val:has-validation \"rejected\" ; ll:has-matching-strength 0.900 . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.10595.38>> ll_val:has-validation \"rejected\" ; ll:has-matching-strength 0.900 . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.441173.4>> ll_val:has-validation \"rejected\" ; ll:has-matching-strength 0.900 . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.4462.4>> lll_val:has-validation \"accepted\" ; ll:has-matching-strength 0.900 . \u2022 \u2022 \u2022 }","title":"Case-1: Grid"},{"location":"05.LinkConstruction/#32-complex-methods","text":"","title":"3.2 Complex Methods"},{"location":"05.LinkConstruction/#case-1-alternative","text":"In Fig 4.11 is displayed an alternative where Case-1: Linkset Specifications Fig 4.11: An example showing how to deduplicate a dataset using an edit distance with threshold 0.9. Case-1: RDF Results Case-1: Turtle file sample ########################################### # NAMESPACES # ########################################### ### PREDEFINED SHARED NAMESPACES @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix void: <http://rdfs.org/ns/void#> . @prefix dcterms: <http://purl.org/dc/terms/> . @prefix format: <http://www.w3.org/ns/formats/> . @prefix pav: <http://purl.org/ontology/similarity/> . @prefix cc: <http://creativecommons.org/ns#> . ### PREDEFINED SPECIFIC NAMESPACES @prefix ll: <http://data.goldenagents.org/ontology/> . @prefix ll_algo: <http://data.goldenagents.org/ontology/matching-method/> . @prefix ll_val: <http://data.goldenagents.org/ontology/validation/> . @prefix linkset: <http://data.goldenagents.org/resource/linkset/> . @prefix dataset: <http://data.goldenagents.org/resource/dataset/> . ### AUTOMATED NAMESPACES @prefix skos: <http://www.w3.org/2004/02/skos/core#> . @prefix institutes_S1: <http://www.grid.ac/institutes/> . ############################################################################################################## # GENERIC METADATA # ############################################################################################################## linkset: Grid_2 a void: Linkset ; cc: attributionName \"LenticularLens\" ; void: feature format: Turtle ; cc: license <http://purl.org/NET/rdflicense/W3C1.0> ; ll: has-logic-formulation <http://data.goldenagents.org/resource/PH1ec0ee6f368dd62> ; void: linkPredicate skos: exactMatch ; void: subjectsTarget <http://data.goldenagents.org/resource/dataset/Grid> ; void: objectsTarget <http://data.goldenagents.org/resource/dataset/Grid> ; dcterms: description \"Deduplication of entities of type Education in the GRID dataset\" @ en ; void: triples 1692 ; void: entities 1737 ; void: distinctSubjects 1737 ; void: distinctObjects 1737 ; ll: has-clusters 619 ; ll_val: has-validations 18 ; ll_val: has-accepted 3 ; ll_val: has-rejected 6 ; ll_val: has-remaining 1683 . ################################################################################ # LOGIC FORMULA PARTS # ################################################################################ <http://data.goldenagents.org/resource/PH1ec0ee6f368dd62> a ll: LogicFormulation ; ll: has-method <http://data.goldenagents.org/resource/Normalised-EditDistance-H779a0ad1b5e5f93> ; ll: has-method <http://data.goldenagents.org/resource/Normalised-EditDistance-H3de4966a0b8aa01> ; ll: has-method <http://data.goldenagents.org/resource/Normalised-EditDistance-H11cbb0cc77c44a9> ; ll: has-formula-description \"\"\"<http://data.goldenagents.org/resource/Normalised-EditDistance-H779a0ad1b5e5f93> and (\u22a4min) <http://data.goldenagents.org/resource/Normalised-EditDistance-H3de4966a0b8aa01> and (\u22a4min) <http://data.goldenagents.org/resource/Normalised-EditDistance-H11cbb0cc77c44a9> \"\"\" . ################################################################################ # METHOD SIGNATURES # ################################################################################ ### ll_algo:Normalised-EditDistance <http://data.goldenagents.org/resource/Normalised-EditDistance-H779a0ad1b5e5f93> a ll:MatchingMethod ; ll:has-algorithm ll_algo:Normalised-EditDistance ; ll:has-threshold 0.9 ; ll:has-threshold-range \"]0, 1]\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Greater-than-or-equal-to> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> . ### ll_algo:Normalised-EditDistance <http://data.goldenagents.org/resource/Normalised-EditDistance-H3de4966a0b8aa01> a ll:MatchingMethod ; ll:has-algorithm ll_algo:Normalised-EditDistance ; ll:has-threshold 0.9 ; ll:has-threshold-range \"]0, 1]\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Greater-than-or-equal-to> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> . ### ll_algo:Normalised-EditDistance <http://data.goldenagents.org/resource/Normalised-EditDistance-H11cbb0cc77c44a9> a ll:MatchingMethod ; ll:has-algorithm ll_algo:Normalised-EditDistance ; ll:has-threshold 0.9 ; ll:has-threshold-range \"]0, 1]\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Greater-than-or-equal-to> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> . ################################################################################ # METHOD DESCRIPTIONS # ################################################################################ ll_algo:Normalised-EditDistance a ll:MatchingAlgorithm ; dcterms:description \"\"\" This \u200bmethod is used to align \u200b\u200bsource a\u200bnd \u200b\u200btarget\u2019s IRIs whenever the similarity score of their respective user selected property values are \u200b\u200babove a given \u200bLevenshtein (edit) Distance threshold\u200b. Edit distance is a way of quantifying how \u200bdissimilar two strings (e.g., words) are to one another by counting the minimum number of operations \u200b\u03b5 \u200b(\u200bremoval, insertion, or substitution of a character in the string)\u200b required to transform one string into the other. For example, \u200bthe \u200bLevenshtein distance between kitten and sitting is \u200b\u03b5 \u200b= 3 as it requires a two substitutions (s for k and i for e) and one insertion of g at the end [https://en.wikipedia.org/wiki/Edit_distance]\u200b. \"\"\"@en . ################################################################################ # DATASET AND ENTITY SELECTIONS # ################################################################################ ### ENTITY SELECTION [SOURCE] N0: 1 ### <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> a ll:EntitySelection ; ll:has-dataset <http://data.goldenagents.org/resource/dataset/Grid> ; ll:has-entity-type <http://www.grid.ac/ontology/Education> . ################################################################################ # PREDICATE SELECTIONS # ################################################################################ ### PREDICATE SELECTED [SOURCE] N0: 1 ### <http://data.goldenagents.org/resource/PredicateSelection-PHab504e102405ab0> a ll:PropertySelection ; ll:has-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-predicate <http://www.w3.org/2000/01/rdf-schema#label> . ### PREDICATE SELECTED [TARGET] N0: 2 ### <http://data.goldenagents.org/resource/PredicateSelection-PH0d712649af643f3> a ll:PropertySelection ; ll:has-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH61bd543e4ce34c2> ; ll:has-predicate <http://www.w3.org/2004/02/skos/core#prefLabel> . ############################################################################################################## # ANNOTATED LINKSET # ############################################################################################################## linkset:Grid_2 { <<institutes_S1:grid.1017.7 skos:exactMatch institutes_S1:grid.501980.5>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.1019.9 skos:exactMatch institutes_S1:grid.449929.b>> ll_val:has-validation \"accepted\" . <<institutes_S1:grid.1020.3 skos:exactMatch institutes_S1:grid.266826.e>> ll_val:has-validation \"accepted\" . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.10347.31>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.10595.38>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.10215.37 skos:exactMatch institutes_S1:grid.4462.4>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.10595.38>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.441173.4>> ll_val:has-validation \"rejected\" . <<institutes_S1:grid.10347.31 skos:exactMatch institutes_S1:grid.4462.4>> ll_val:has-validation \"accepted\" . <<institutes_S1:grid.10373.36 skos:exactMatch institutes_S1:grid.266769.a>> ll_val:has-validation \"not_validated\" . \u2022 \u2022 \u2022 }","title":"Case-1: Alternative"},{"location":"05.LinkConstruction/#case-2-getty","text":"In Fig 4.12 is displayed an alternative where Case-2: Linkset Specifications. Fig 4.12: An example showing how to deduplicate a dataset using an edit distance with threshold 0.9. Case-2: RDF Results. Case-2: Turtle file sample.","title":"Case-2: Getty"},{"location":"05.LinkConstruction/#namespaces","text":"","title":"NAMESPACES"},{"location":"05.LinkConstruction/#predefined-shared-namespaces","text":"@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> . @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix void: <http://rdfs.org/ns/void#> . @prefix dcterms: <http://purl.org/dc/terms/> . @prefix format: <http://www.w3.org/ns/formats/> . @prefix pav: <http://purl.org/ontology/similarity/> . @prefix cc: <http://creativecommons.org/ns#> .","title":"PREDEFINED SHARED NAMESPACES"},{"location":"05.LinkConstruction/#predefined-specific-namespaces","text":"@prefix ll: <http://data.goldenagents.org/ontology/> . @prefix ll_algo: <http://data.goldenagents.org/ontology/matching-method/> . @prefix ll_val: <http://data.goldenagents.org/ontology/validation/> . @prefix linkset: <http://data.goldenagents.org/resource/linkset/> . @prefix dataset: <http://data.goldenagents.org/resource/dataset/> .","title":"PREDEFINED SPECIFIC NAMESPACES"},{"location":"05.LinkConstruction/#automated-namespaces","text":"@prefix skos: <http://www.w3.org/2004/02/skos/core#> . @prefix institutes_S1: <http://www.grid.ac/institutes/> . @prefix time: <http://www.w3.org/2006/time#> . @prefix owl: <http://www.w3.org/2002/07/owl#> . @prefix Person_S1: <http://goldenagents.org/uva/SAA/Person/> . @prefix PersonName_T1: <https://data.goldenagents.org/datasets/SAA/PersonName/> .","title":"AUTOMATED NAMESPACES"},{"location":"05.LinkConstruction/#generic-metadata","text":"","title":"GENERIC METADATA"},{"location":"05.LinkConstruction/#ll_algonormalised-soundex","text":"http://data.goldenagents.org/resource/Normalised-Soundex-H4970fc2fe79ea5f a ll:MatchingMethod ; ll:has-algorithm ll_algo:Normalised-Soundex ; ll:has-threshold 0.85 ; ll:has-threshold-range \"]0, 1]\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Greater-than-or-equal-to> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH914a94c6f3c93b4> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH10aaeebb6832fdf> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH7879419327d373d> .","title":"ll_algo:Normalised-Soundex"},{"location":"05.LinkConstruction/#ll_algoexact","text":"http://data.goldenagents.org/resource/Exact-H918a02351d48ca9 a ll:MatchingMethod ; ll:has-algorithm ll_algo:Exact ; ll:has-threshold 1 ; ll:has-threshold-range \"1\" ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Equal> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH46d91d4f6e2209e> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH0df0471cb5df515> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHe3cb3236c5b11b1> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHb2a681013fbb430> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH9f7bd41ea902bd0> .","title":"ll_algo:Exact"},{"location":"05.LinkConstruction/#ll_algotime-delta","text":"http://data.goldenagents.org/resource/Time-Delta-Hdcc5070996853e9 a ll:MatchingMethod ; ll:has-algorithm ll_algo:Time-Delta ; ll:has-threshold 0 ; ll:has-threshold-range \"\u2115\" ; time:unitType time:unitYear ; ll:has-threshold-acceptance-operator <http://data.goldenagents.org/resource/Equal> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36> ; ll:has-subj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHeb98e7f77b22fce> ; ll:has-subj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHf741a098569afb1> ; ll:has-obj-entity-selection <http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHee886bb3d021a6a> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PH715d032180bd40c> ; ll:has-obj-predicate-selection <http://data.goldenagents.org/resource/PredicateSelection-PHe349eb18e5ba638> . ################################################################################ # METHOD DESCRIPTIONS # ################################################################################ ll_algo:Normalised-Soundex a ll:MatchingAlgorithm ; dcterms:description \u201c\u201d\u201d \u201cSoundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for ho- mophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first let- ter\u201d [ https://en.wikipedia.org/wiki/Soundex ]. In the Lenticular Lens, Soundex is used as a normaliser in the sense that an edit distance is run over the soundex code version of a name. For example, the in the table below, the normalisation of both Louijs Roc- ourt and `Lowis Ricourt becomes L200 R263 leading to an edit distance of 0 and a relative strength of 1. However, computing the same names using directly an edit distance results in an edit distance of 3 and a relative matching strength of 0. 79. -------------- -- Example -- THE USE OF SOUNDEX CODE FOR STRING APPROXIMATION -------------- The example below shows the implementation of Soundex Distance in the Lenticular Lens and how it compares with Edit Distance over the original names (no soundex-based normalisation). ------------------------------------------------------------------------------------------------------------------------------------------------------ Source Target E. Dist Rel. distance Source soundex Target soundex Code E. Dist Code Rel. Dist ------------------------------------------------------------------------------------------------------------------------------------------------------ Jasper Cornelisz. Lodder Jaspar Cornelisz Lodder 2 0.92 J216 C654 L360 J216 C654 L360 0 1.0 Barent Teunis Barent Teunisz gen. Drent 12 0.52 B653 T520 B653 T520 G500 D653 10 0.47 Louijs Rocourt Louys Rocourt 2 0.86 L200 R263 L200 R263 0 1.0 Louijs Rocourt Lowis Ricourt 3 0.79 L200 R263 L200 R263 0 1.0 Louys Rocourt Lowis Ricourt 3 0.77 L200 R263 L200 R263 0 1.0 Cornelis Dircksz. Clapmus Cornelis Clapmuts 10 0.6 C654 D620 C415 C654 C415 5 0.64 Geertruydt van den Breemde Geertruijd van den Bremde 4 0.85 G636 V500 D500 B653 G636 V500 D500 B653 \"\"\"@en . ll_algo:Exact a ll:MatchingAlgorithm ; dcterms:description \u201c\u201d\u201d Aligns source and target\u2019s IRIs whenever their respective user selected property values are identical.\u201d\u201c\u201d@en . ll_algo:Time-Delta a ll:MatchingAlgorithm ; dcterms:description \u201c\u201d\u201d 10.1 Time Delta. This function allows for finding co-referent entities on the basis of a minimum time dif- ference between the times reported by the source and the target entities. For example, if the value zero is assigned to the time difference parameter, then, for a matched to be found, the time of the target and the one of the source are to be the exact same times. While accounting for margins of error, one may consider a pair of entities to be co-referent if the real entities are born lambda days, months or years apart among other-things (similar name, place..). \u201c\u201d\u201c@en . ################################################################################ # DATASET AND ENTITY SELECTIONS # ################################################################################","title":"ll_algo:Time-Delta"},{"location":"05.LinkConstruction/#entity-selection-source-n0-1","text":"http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc a ll:EntitySelection ; ll:has-dataset https://data.goldenagents.org/datasets/ufab7d657a250e3461361c982ce9b38f3816e0c4b/frick_collection_montias_data_20200604 ; ll:has-entity-type https://data.goldenagents.org/datasets/SAA/ontology/Person .","title":"ENTITY SELECTION [SOURCE] N0: 1"},{"location":"05.LinkConstruction/#entity-selection-source-n0-2","text":"http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36 a ll:EntitySelection ; ll:has-dataset https://data.goldenagents.org/datasets/ufab7d657a250e3461361c982ce9b38f3816e0c4b/getty_provenance_index_montias_data_20200604 ; ll:has-entity-type https://data.goldenagents.org/datasets/SAA/ontology/Person .","title":"ENTITY SELECTION [SOURCE] N0: 2"},{"location":"05.LinkConstruction/#entity-selection-target-n0-3","text":"http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 a ll:EntitySelection ; ll:has-dataset https://data.goldenagents.org/datasets/ufab7d657a250e3461361c982ce9b38f3816e0c4b/index_op_notarieel_archief_enriched_20191202 ; ll:has-entity-type https://w3id.org/pnv#PersonName . ################################################################################ # PREDICATE SELECTIONS # ################################################################################","title":"ENTITY SELECTION [TARGET] N0: 3"},{"location":"05.LinkConstruction/#predicate-selected-source-n0-1","text":"http://data.goldenagents.org/resource/PredicateSelection-PH914a94c6f3c93b4 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc ; ll:has-predicate http://www.w3.org/2000/01/rdf-schema#label .","title":"PREDICATE SELECTED [SOURCE]  N0: 1"},{"location":"05.LinkConstruction/#predicate-selected-source-n0-2","text":"http://data.goldenagents.org/resource/PredicateSelection-PH10aaeebb6832fdf a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36 ; ll:has-predicate http://www.w3.org/2000/01/rdf-schema#label .","title":"PREDICATE SELECTED [SOURCE]  N0: 2"},{"location":"05.LinkConstruction/#predicate-selected-target-n0-3","text":"http://data.goldenagents.org/resource/PredicateSelection-PH7879419327d373d a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://www.w3.org/2000/01/rdf-schema#label .","title":"PREDICATE SELECTED [TARGET]  N0: 3"},{"location":"05.LinkConstruction/#predicate-selected-source-n0-4","text":"http://data.goldenagents.org/resource/PredicateSelection-PH46d91d4f6e2209e a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36 ; ll:has-predicate http://data.goldenagents.org/resource/PH1df7dbfdf1d1eb8 . http://data.goldenagents.org/resource/PH1df7dbfdf1d1eb8 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Inventory ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/documentedIn ; rdf:_4 https://data.goldenagents.org/datasets/SAA/ontology/InventoryBook ; rdf:_5 https://data.goldenagents.org/datasets/SAA/ontology/inventoryNumber .","title":"PREDICATE SELECTED [SOURCE]  N0: 4"},{"location":"05.LinkConstruction/#predicate-selected-source-n0-5","text":"http://data.goldenagents.org/resource/PredicateSelection-PH0df0471cb5df515 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc ; ll:has-predicate http://data.goldenagents.org/resource/PH1df7dbfdf1d1eb8 .","title":"PREDICATE SELECTED [SOURCE]  N0: 5"},{"location":"05.LinkConstruction/#predicate-selected-target-n0-6","text":"http://data.goldenagents.org/resource/PredicateSelection-PHe3cb3236c5b11b1 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PH7334cc09b832a17 . http://data.goldenagents.org/resource/PH7334cc09b832a17 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/HuwelijkseVoorwaarden ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/inventoryNumber .","title":"PREDICATE SELECTED [TARGET]  N0: 6"},{"location":"05.LinkConstruction/#predicate-selected-target-n0-7","text":"http://data.goldenagents.org/resource/PredicateSelection-PHb2a681013fbb430 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PH00d91362d72928f . http://data.goldenagents.org/resource/PH00d91362d72928f a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Boedelinventaris ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/inventoryNumber .","title":"PREDICATE SELECTED [TARGET]  N0: 7"},{"location":"05.LinkConstruction/#predicate-selected-target-n0-8","text":"http://data.goldenagents.org/resource/PredicateSelection-PH9f7bd41ea902bd0 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PH9f325d76d5fa623 . http://data.goldenagents.org/resource/PH9f325d76d5fa623 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Boedelscheiding ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/inventoryNumber .","title":"PREDICATE SELECTED [TARGET]  N0: 8"},{"location":"05.LinkConstruction/#predicate-selected-source-n0-9","text":"http://data.goldenagents.org/resource/PredicateSelection-PHeb98e7f77b22fce a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PHb97c2bc9d29ba36 ; ll:has-predicate http://data.goldenagents.org/resource/PH22c48ebe6b24223 . http://data.goldenagents.org/resource/PH22c48ebe6b24223 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Inventory ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/registrationDate .","title":"PREDICATE SELECTED [SOURCE]  N0: 9"},{"location":"05.LinkConstruction/#predicate-selected-source-n0-10","text":"http://data.goldenagents.org/resource/PredicateSelection-PHf741a098569afb1 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH4ba00e26b03e5dc ; ll:has-predicate http://data.goldenagents.org/resource/PH22c48ebe6b24223 .","title":"PREDICATE SELECTED [SOURCE]  N0: 10"},{"location":"05.LinkConstruction/#predicate-selected-target-n0-11","text":"http://data.goldenagents.org/resource/PredicateSelection-PHee886bb3d021a6a a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PHda68158d0b9f392 . http://data.goldenagents.org/resource/PHda68158d0b9f392 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/HuwelijkseVoorwaarden ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/registrationDate .","title":"PREDICATE SELECTED [TARGET]  N0: 11"},{"location":"05.LinkConstruction/#predicate-selected-target-n0-12","text":"http://data.goldenagents.org/resource/PredicateSelection-PH715d032180bd40c a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PH956023596f37d1b . http://data.goldenagents.org/resource/PH956023596f37d1b a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Boedelinventaris ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/registrationDate .","title":"PREDICATE SELECTED [TARGET]  N0: 12"},{"location":"05.LinkConstruction/#predicate-selected-target-n0-13","text":"http://data.goldenagents.org/resource/PredicateSelection-PHe349eb18e5ba638 a ll:PropertySelection ; ll:has-entity-selection http://data.goldenagents.org/resource/EntitySelection-PH769c39438419b10 ; ll:has-predicate http://data.goldenagents.org/resource/PH0ef342da86f3226 . http://data.goldenagents.org/resource/PH0ef342da86f3226 a ll:SequenceSelection ; rdf:_1 https://data.goldenagents.org/datasets/SAA/ontology/isInRecord ; rdf:_2 https://data.goldenagents.org/datasets/SAA/ontology/Boedelscheiding ; rdf:_3 https://data.goldenagents.org/datasets/SAA/ontology/registrationDate .","title":"PREDICATE SELECTED [TARGET]  N0: 13"},{"location":"05.LinkConstruction/#annotated-linkset","text":"","title":"ANNOTATED LINKSET"},{"location":"06.LinkManipulation/","text":".katex img { display: block; position: absolute; width: 100%; height: inherit; } LINK MANIPULATION \u00b6 Work in progress After generating a number of sets of linksets, one may want to merge, split or filter out some links while others may want to infer new links using existing alignments. The LENS menu provides means for exactly doing that. The LENS menu makes it possible to apply set like operators such as UNION, INTERSECTION, DIFFERENCE, TRANSITIVE and IN-SET over alignments in ways that suit best the user according to her envisaged final integration goal. For the purpose of illustration, we provide two linksets ( ex:DC-Heroes-Identity , ex:Marvel-Heroes-Identity ) of superheroes from the DC and Marvel Universe. According to the metadata of these linksets, they originate from an attempt to de-duplicate the datasets of each universe. This can be observed as the source and target entities composing the links in each linkset are respectively from the same dataset. From the metadata we also observe that, the value of the property void:triples highlights that each linkset is composed of ten identity links. Furthermore, When taking a look at the links within each linkset, the RDF* format of the triples informs that a link validation took place for each link in the set. Theses sets will be used for illustrating the set-like operators discussed in this section. Example 1: Linksets originating from deduplications. ex: DC-Heroes-Identity a void: Linkset ; dcterms: description \"Identifying DC's comics superheroes\" ; ll: subjectsTarget dataset: DC-Comics ; ll: objectsTarget dataset: DC-Comics ; void: triples 10 ; \u2022 \u2022 \u2022 ex: Marvel-Heroes-Identity a void: Linkset ; dcterms: description \"Identifying Marvel Universes' superheroes\" ; ll: subjectsTarget dataset: Marvel-Universe ; ll: objectsTarget dataset: Marvel-Universe ; void: triples 10 ; \u2022 \u2022 \u2022 ex: DC-Heroes-Identity { << hero: Superman owl: sameAs person: Clark-Kent >> validation: status \"accepted\" . << hero: Batman owl: sameAs person: Bruce-Wane >> validation: status \"accepted\" . << hero: Flash owl: sameAs person: Barry-Allen >> validation: status \"accepted\" . << hero: GreenLantern owl: sameAs person: Alan-Scott >> validation: status \"accepted\" . << hero: WonderWoman owl: sameAs person: Diana-Prince >> validation: status \"accepted\" . << hero: Aquaman owl: sameAs person: Arthur-Curry >> validation: status \"accepted\" . << hero: GreenArrow owl: sameAs person: OliverQueen >> validation: status \"accepted\" . << hero: BoosterGold owl: sameAs person: Michael-Jon-Carter >> validation: status \"accepted\" . << hero: Spider-man owl: sameAs person: Peter-Parker >> validation: status \"rejected\" . << hero: Iron-man owl: sameAs person: Tony-Stark >> validation: status \"rejected\" . } ex: Marvel-Heroes-Identity { << hero: Captain-Marvel owl: sameAs person: Carol-Danvers >> validation: status \"accepted\" . << hero: Captain-America owl: sameAs person: Steve-Rogers >> validation: status \"accepted\" . << hero: Deadpool owl: sameAs person: Wade-Wilson >> validation: status \"accepted\" . << hero: Black-Panther owl: sameAs person: T-Challa >> validation: status \"accepted\" . << hero: Spider-man owl: sameAs person: Peter-Parker >> validation: status \"accepted\" . << hero: Iron-man owl: sameAs person: Tony-Stark >> validation: status \"accepted\" . << hero: Ant-man owl: sameAs person: Tony-Stark >> validation: status \"accepted\" . << hero: Black-Widow owl: sameAs person: Natasha-Romanoff >> validation: status \"accepted\" . << hero: Hulk owl: sameAs person: Bruce-Banner >> validation: status \"accepted\" . << hero: Hawkeye owl: sameAs person: Clint-Barton >> validation: status \"accepted\" . } 1. Union \u00b6 Imagine having three linksets ex:Marvel-Heroes-Identity-1 , ex:Marvel-Heroes-Identity-2 and ex:DC-Heroes-Identity . Example 2: Sample of three linksets to merge. ############################################################## # MARVEL: Annotated Linkset of 6 Identity Statements # ############################################################## # Superheroes vs Fictitious-Persons ex: Marvel-Heroes-Identity-1 a void: Linkset ; dcterms: subject \"Fictitious Heroes\" ; dcterms: description \"Identifying Marvel Universes' superheroes\" ; ll: subjectsTarget dataset: Marvel-Universe ; ll: objectsTarget dataset: Marvel-Universe ; void: triples 6 . ex: Marvel-Heroes-Identity-1 { << hero: Black-Widow owl: sameAs person: Natasha-Romanoff >> validation: status true . << person: Bruce-Banner owl: sameAs hero: Hulk >> validation: status true . << hero: Captain-America owl: sameAs person: Steve-Rogers >> validation: status true . << hero: Captain-Marvel owl: sameAs person: Carol-Danvers >> validation: status true . << hero: Deadpool owl: sameAs person: Wade-Wilson >> validation: status true . << hero: Black-Panther owl: sameAs person: T-Challa >> validation: status true . } ############################################################## # MARVEL: Annotated Linkset of 4 Identity Statements # ############################################################## ex: Marvel-Heroes-Identity-2 a void: Linkset ; dcterms: subject \"Fictitious Heroes\" ; dcterms: description \"Identifying Marvel Universes' superheroes\" ; ll: subjectsTarget dataset: Marvel-Universe ; ll: objectsTarget dataset: Marvel-Universe ; void: triples 4 . ex: Marvel-Heroes-Identity-2 { << person: Peter-Parker owl: sameAs hero: Spider-man >> validation: status true . << person: Tony-Stark owl: sameAs hero: Iron-man >> validation: status true . << person: Tony-Stark owl: sameAs hero: Ant-man >> validation: status False . << person: Clint-Barton owl: sameAs hero: Hawkeye >> validation: status true . } ############################################################## # DC-COMICS: Non Annotated Linkset of 10 Identity Statements # ############################################################## ex: DC-Heroes-Identity a void: Linkset ; dcterms: subject \"Fictitious Heroes\" ; dcterms: description \"Identifying DC Comics Universes' superheroes\" ; void: triples 9 . ex: DC-Heroes-Identity { hero: Superman owl: sameAs person: Clark-Kent . hero: Batman owl: sameAs person: Bruce-Wane . hero: GreenLantern owl: sameAs person: Alan-Scott . hero: WonderWoman owl: sameAs person: Diana-Prince . hero: Aquaman owl: sameAs person: Arthur-Curry . hero: GreenArrow owl: sameAs person: OliverQueen . hero: BoosterGold owl: sameAs person: Michael-Jon-Carter . hero: Spider-man owl: sameAs person: Peter-Parker . hero: Iron-man owl: sameAs person: Tony-Stark . } As the above linksets individually make an attempt to align superheroes and fictitious persons in their respective universe, how about unifying these sets as the set of links identifying superheroes in the limited universe of DC and Marvel. To generate such a lens, the UNION set-like operator is required. Using such operator over the three linksets generates the ex:Union-Marvel-DC-Heroes lens. Here, each link is annotated with its provenance as they all originate from one or more linksets. For example, the identity triple hero:Iron-man owl:sameAs person:Tony-Stark is derive from the linksets ex:Marvel-Heroes-Identity-2 and ex:DC-Heroes-Identity . This enables the links in the newly created lens to carry their own annotations while being able to still use the annotations present it their respective linkset of origin (linkset they are derived from). Example 3: UNION of three linksets. With the UNION operator, the same link appearing in various sets (linksets and/or lenses) regardless of its direction is represented only once in the resulting lens of UNION . In a linkset for example, the subject and object targets are explicitly defined. As such, \u27e8 ex:e1 owl:sameAs ex:e2 \u27e9 \\lang \\text{ ex:e1 owl:sameAs ex:e2 } \\rang \u27e8 ex:e1 owl:sameAs ex:e2 \u27e9 and \u27e8 ex:e2 owl:sameAs ex:e1 \u27e9 \\lang \\text{ ex:e2 owl:sameAs ex:e1 } \\rang \u27e8 ex:e2 owl:sameAs ex:e1 \u27e9 are different and stem from different linksets. However, in the resulting UNION , a distinction between the two triples is not explicitly made, meaning that the direction is not of much importance here. This can be observed in the metadata of the union where only the property void:target is used instead of void:subjectsTarget and void:objectsTarget . ### Lens metadata ex: Union-Marvel-DC-Heroes a voidPlus: Lens ; voidPlus: has-lens-operator voidPlus: Union ; dcterms: description \"Identifying superheroes from DC and Marvel Universes'\" ; ll: target ex: Marvel-Heroes-Identity-1 ; ll: target ex: Marvel-Heroes-Identity-2 ; ll: target ex: DC-Heroes-Identity-1 ; void: triples 10 . ### The annotated Lens ex: Union-Marvel-Heroes { << hero: Black-Panther owl: sameAs person: T-Challa >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Black-Widow owl: sameAs person: Natasha-Romanoff >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Captain-America owl: sameAs person: Steve-Rogers >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Captain-Marvel owl: sameAs person: Carol-Danvers >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Deadpool owl: sameAs person: Wade-Wilson >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Hulk owl: sameAs person: Bruce-Banner >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Ant-man owl: sameAs person: Tony-Stark >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-2 . << hero: Hawkeye owl: sameAs person: Clint-Barton >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-2 . << hero: Iron-man owl: sameAs person: Tony-Stark >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-2 ; prov: wasDerivedFrom ex: DC-Heroes-Identity . << hero: Spider-man owl: sameAs person: Peter-Parker >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-2 ; prov: wasDerivedFrom ex: DC-Heroes-Identity . << hero: Superman owl: sameAs person: Clark-Kent >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: Batman owl: sameAs person: Bruce-Wane >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: Flash owl: sameAs person: Barry-Allen >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: GreenLantern owl: sameAs person: Alan-Scott >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: WonderWoman owl: sameAs person: Diana-Prince >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: Aquaman owl: sameAs person: Arthur-Curry >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: GreenArrow owl: sameAs person: OliverQueen >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: BoosterGold owl: sameAs person: Michael-Jon-Carter >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . } 2. Intersection \u00b6 Intersecting the DC ( ex:DC-Heroes-Identity ) and Marvel ( ex:Marvel-Heroes-Identity ) linksets of superheroes results in ex:Comics-Heroes-Identity , a lens which reveals the only two links shared by the sets. The first link in this lens establishes an identity relation between hero:Spider-man the hero and person:Peter-Parker the fictitious character, while the second link asserts that the superhero hero:Iron-man and the fictitious entity representing Tony Stark are co-referent entities. Example 4: Intersection: Extracting shared links. ex: Comics-Heroes-Identity a voidPlus: Lens ; voidPlus: has-lens-operator voidPlus: Intersection ; dcterms: description \"Quality Check: Identifying superheroes from both Marvel and DC comics\" ; voidPlus: expression \"ex:DC-Heroes-Identity INTERSECTION ex:Marvel-Heroes-Identity\" ; void: target ex: DC-Heroes-Identity , ex: Marvel-Heroes-Identity ; void: triples 4 ; \u2022 \u2022 \u2022 ex: Comics-Heroes-Identity { << hero: Spider-man owl: sameAs person: Peter-Parker >> prov: wasDerivedFrom ex: DC-Heroes-Identity ; prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: Iron-man owl: sameAs person: Tony-Stark >> prov: wasDerivedFrom ex: DC-Heroes-Identity ; prov: wasDerivedFrom ex: Marvel-Heroes-Identity . } 3. Difference \u00b6 Contrarily to the UNION and INTERSECTION operators, the DIFFERENCE operator requires assigning values to the following two properties: void:subjectsTarget and void:objectsTarget . This allows for the explicit and accurate documentation of the lens resulting from performing the Difference manipulation over linksets, lenses or the combination of both a linkset and a lens. This specification is not to put a particular emphasis on the direction of the links instead, it is for knowing the linkset or lens from which the resulting links will stem from. Example 5: DIFFERENCE for extracting triples only existing in void:subjectsTarget . ex: Comics-Heroes-Identity a voidPlus: Lens ; voidPlus: has-lens-operator voidPlus: Difference ; dcterms: description \"Quality Check: Identifying superheroes from both Marvel and DC comics\" ; voidPlus: expression \"ex:DC-Heroes-Identity DIFFERENCE ex:Marvel-Heroes-Identity\" ; void: target ex: DC-Heroes-Identity , ex: Marvel-Heroes-Identity ; void: triples 8 ; \u2022 \u2022 \u2022 ex: DC-Heroes-Identity { << hero: Superman owl: sameAs person: Clark-Kent >> validation: status \"True\" . << hero: Batman owl: sameAs person: Bruce-Wane >> validation: status \"True\" . << hero: Flash owl: sameAs person: Barry-Allen >> validation: status \"True\" . << hero: GreenLantern owl: sameAs person: Alan-Scott >> validation: status \"True\" . << hero: WonderWoman owl: sameAs person: Diana-Prince >> validation: status \"True\" . << hero: Aquaman owl: sameAs person: Arthur-Curry >> validation: status \"True\" . << hero: GreenArrow owl: sameAs person: OliverQueen >> validation: status \"True\" . << hero: BoosterGold owl: sameAs person: Michael-Jon-Carter >> validation: status \"True\" . } 4. Composition \u00b6 5. In Set \u00b6 Imagine being a movie fan particularly interested in knowing more on a set of ten fictional characters carefully handpicked. This intellectual knowledge includes discovering for example knowing in what movies a fictional character played in, what was the rate of the movies, and so on\u2026 Example 6 exhaustively lists these movie characters of interest. Example 6: ex:preference , set of movie characters. ex: preference rdf : _ 4735532571260424746 hero: Captain-Marvel , person: Carol-Danvers , hero: Captain-America , person: Steve-Rogers , hero: Deadpool , person: Wade-Wilson , hero: Black-Panther , person: T-Challa , hero: Spider-man , person: Peter-Parker , hero: Iron-man , person: Tony-Stark , hero: Ant-man , person: Tony-Stark , hero: Black-Widow , person: Natasha-Romanoff , hero: Hulk , person: Bruce-Banner , hero: Hawkeye , person: Clint-Barton . To know more about these handpicked characters, we have isolated ex:MovieInfo , a dataset where information such as movie\u2019s fictional character and movie\u2019s rate are documented. Example 7: ex:MovieCharacter , a movie database. ex: MovieInfo { movie: WonderWoman schema: character hero: WonderWoman ; schema: contentRating 7.4 . movie: WonderWoman-Bloodlines schema: character hero : WonderWoman ; schema: contentRating 5.8 . movie: Superman schema: character hero: Superman ; schema: contentRating 7.3 . movie: Superman-Returns schema: character hero: Superman ; schema: contentRating 6 . movie: Man-of-Steel schema: character hero: Superman ; schema: contentRating 7 . movie: Batman schema: character hero: Batman ; schema: contentRating 7.5 . movie: Batman-Returns schema: character hero: Batman ; schema: contentRating 7 . movie: Batman-Robin schema: character hero: Batman ; schema: contentRating 3.7 . movie: Spider-man schema: character hero: Spider-man ; schema: contentRating 7.3 . movie: The-amazing-Spider-man schema: character hero: Spider-man ; schema: contentRating 6.5 . movie: Spider-man-home-coming schema: character hero: Spider-man ; schema: contentRating 7.4 . } From the dataset ex:MovieInfo , an embedded linkset defined by the link predicate schema:character is extracted and presented in Example 8. In this example, four fictional characters (hero:WonderWoman, Superman, Batman and Spider-man) are linked to various superhero-movies leading to a total of eleven links. Example 8: ex:LinksetMovieCharacter , the movie character linkset. ex: LinksetMovieCharacter { movie: WonderWoman schema: character hero: WonderWoman . movie: WonderWoman-Bloodlines schema: character hero: WonderWoman . movie: Superman schema: character hero: Superman . movie: Superman-Return schema: character hero: Superman . movie: Man-of-Steel schema: character hero: Superman . movie: Batman schema: character hero: Batman . movie: Batman-Returns schema: character hero: Batman . movie: Batman-Robin schema: character hero: Batman . movie: Spider-man schema: character hero: Spider-man . movie: The-amazing-Spider-man schema: character hero: Spider-man . movie: Spider-man-home-coming schema: character hero: Spider-man . } A close look at Example 6 reveals that the set of handpicked characters is a subset of the Marvel Universe. Now, overlaying this set ( ex:preference ) onto the linkset of movie characters ( ex:LinksetMovieCharacter ) implies extracting the links in ex:LinksetMovieCharacter where entities at the subject or object position of the link can be find in ex:preference . Formally, this can be translated into: ex:LinksetMovieCharacter InSet ex:preference . Example 9: Movie Characters from ex:LinksetMovieCharacter that are derived from Marvel\u2019s matched universe set. ex: MovieCharacter { movie: Spider-man schema: character hero: Spider-man . movie: The-amazing-Spider-man schema: character hero: Spider-man . movie: Spider-man-home-coming schema: character hero: Spider-man . }","title":"7. Lens - Link Manipulation"},{"location":"06.LinkManipulation/#link-manipulation","text":"Work in progress After generating a number of sets of linksets, one may want to merge, split or filter out some links while others may want to infer new links using existing alignments. The LENS menu provides means for exactly doing that. The LENS menu makes it possible to apply set like operators such as UNION, INTERSECTION, DIFFERENCE, TRANSITIVE and IN-SET over alignments in ways that suit best the user according to her envisaged final integration goal. For the purpose of illustration, we provide two linksets ( ex:DC-Heroes-Identity , ex:Marvel-Heroes-Identity ) of superheroes from the DC and Marvel Universe. According to the metadata of these linksets, they originate from an attempt to de-duplicate the datasets of each universe. This can be observed as the source and target entities composing the links in each linkset are respectively from the same dataset. From the metadata we also observe that, the value of the property void:triples highlights that each linkset is composed of ten identity links. Furthermore, When taking a look at the links within each linkset, the RDF* format of the triples informs that a link validation took place for each link in the set. Theses sets will be used for illustrating the set-like operators discussed in this section. Example 1: Linksets originating from deduplications. ex: DC-Heroes-Identity a void: Linkset ; dcterms: description \"Identifying DC's comics superheroes\" ; ll: subjectsTarget dataset: DC-Comics ; ll: objectsTarget dataset: DC-Comics ; void: triples 10 ; \u2022 \u2022 \u2022 ex: Marvel-Heroes-Identity a void: Linkset ; dcterms: description \"Identifying Marvel Universes' superheroes\" ; ll: subjectsTarget dataset: Marvel-Universe ; ll: objectsTarget dataset: Marvel-Universe ; void: triples 10 ; \u2022 \u2022 \u2022 ex: DC-Heroes-Identity { << hero: Superman owl: sameAs person: Clark-Kent >> validation: status \"accepted\" . << hero: Batman owl: sameAs person: Bruce-Wane >> validation: status \"accepted\" . << hero: Flash owl: sameAs person: Barry-Allen >> validation: status \"accepted\" . << hero: GreenLantern owl: sameAs person: Alan-Scott >> validation: status \"accepted\" . << hero: WonderWoman owl: sameAs person: Diana-Prince >> validation: status \"accepted\" . << hero: Aquaman owl: sameAs person: Arthur-Curry >> validation: status \"accepted\" . << hero: GreenArrow owl: sameAs person: OliverQueen >> validation: status \"accepted\" . << hero: BoosterGold owl: sameAs person: Michael-Jon-Carter >> validation: status \"accepted\" . << hero: Spider-man owl: sameAs person: Peter-Parker >> validation: status \"rejected\" . << hero: Iron-man owl: sameAs person: Tony-Stark >> validation: status \"rejected\" . } ex: Marvel-Heroes-Identity { << hero: Captain-Marvel owl: sameAs person: Carol-Danvers >> validation: status \"accepted\" . << hero: Captain-America owl: sameAs person: Steve-Rogers >> validation: status \"accepted\" . << hero: Deadpool owl: sameAs person: Wade-Wilson >> validation: status \"accepted\" . << hero: Black-Panther owl: sameAs person: T-Challa >> validation: status \"accepted\" . << hero: Spider-man owl: sameAs person: Peter-Parker >> validation: status \"accepted\" . << hero: Iron-man owl: sameAs person: Tony-Stark >> validation: status \"accepted\" . << hero: Ant-man owl: sameAs person: Tony-Stark >> validation: status \"accepted\" . << hero: Black-Widow owl: sameAs person: Natasha-Romanoff >> validation: status \"accepted\" . << hero: Hulk owl: sameAs person: Bruce-Banner >> validation: status \"accepted\" . << hero: Hawkeye owl: sameAs person: Clint-Barton >> validation: status \"accepted\" . }","title":"LINK MANIPULATION"},{"location":"06.LinkManipulation/#1-union","text":"Imagine having three linksets ex:Marvel-Heroes-Identity-1 , ex:Marvel-Heroes-Identity-2 and ex:DC-Heroes-Identity . Example 2: Sample of three linksets to merge. ############################################################## # MARVEL: Annotated Linkset of 6 Identity Statements # ############################################################## # Superheroes vs Fictitious-Persons ex: Marvel-Heroes-Identity-1 a void: Linkset ; dcterms: subject \"Fictitious Heroes\" ; dcterms: description \"Identifying Marvel Universes' superheroes\" ; ll: subjectsTarget dataset: Marvel-Universe ; ll: objectsTarget dataset: Marvel-Universe ; void: triples 6 . ex: Marvel-Heroes-Identity-1 { << hero: Black-Widow owl: sameAs person: Natasha-Romanoff >> validation: status true . << person: Bruce-Banner owl: sameAs hero: Hulk >> validation: status true . << hero: Captain-America owl: sameAs person: Steve-Rogers >> validation: status true . << hero: Captain-Marvel owl: sameAs person: Carol-Danvers >> validation: status true . << hero: Deadpool owl: sameAs person: Wade-Wilson >> validation: status true . << hero: Black-Panther owl: sameAs person: T-Challa >> validation: status true . } ############################################################## # MARVEL: Annotated Linkset of 4 Identity Statements # ############################################################## ex: Marvel-Heroes-Identity-2 a void: Linkset ; dcterms: subject \"Fictitious Heroes\" ; dcterms: description \"Identifying Marvel Universes' superheroes\" ; ll: subjectsTarget dataset: Marvel-Universe ; ll: objectsTarget dataset: Marvel-Universe ; void: triples 4 . ex: Marvel-Heroes-Identity-2 { << person: Peter-Parker owl: sameAs hero: Spider-man >> validation: status true . << person: Tony-Stark owl: sameAs hero: Iron-man >> validation: status true . << person: Tony-Stark owl: sameAs hero: Ant-man >> validation: status False . << person: Clint-Barton owl: sameAs hero: Hawkeye >> validation: status true . } ############################################################## # DC-COMICS: Non Annotated Linkset of 10 Identity Statements # ############################################################## ex: DC-Heroes-Identity a void: Linkset ; dcterms: subject \"Fictitious Heroes\" ; dcterms: description \"Identifying DC Comics Universes' superheroes\" ; void: triples 9 . ex: DC-Heroes-Identity { hero: Superman owl: sameAs person: Clark-Kent . hero: Batman owl: sameAs person: Bruce-Wane . hero: GreenLantern owl: sameAs person: Alan-Scott . hero: WonderWoman owl: sameAs person: Diana-Prince . hero: Aquaman owl: sameAs person: Arthur-Curry . hero: GreenArrow owl: sameAs person: OliverQueen . hero: BoosterGold owl: sameAs person: Michael-Jon-Carter . hero: Spider-man owl: sameAs person: Peter-Parker . hero: Iron-man owl: sameAs person: Tony-Stark . } As the above linksets individually make an attempt to align superheroes and fictitious persons in their respective universe, how about unifying these sets as the set of links identifying superheroes in the limited universe of DC and Marvel. To generate such a lens, the UNION set-like operator is required. Using such operator over the three linksets generates the ex:Union-Marvel-DC-Heroes lens. Here, each link is annotated with its provenance as they all originate from one or more linksets. For example, the identity triple hero:Iron-man owl:sameAs person:Tony-Stark is derive from the linksets ex:Marvel-Heroes-Identity-2 and ex:DC-Heroes-Identity . This enables the links in the newly created lens to carry their own annotations while being able to still use the annotations present it their respective linkset of origin (linkset they are derived from). Example 3: UNION of three linksets. With the UNION operator, the same link appearing in various sets (linksets and/or lenses) regardless of its direction is represented only once in the resulting lens of UNION . In a linkset for example, the subject and object targets are explicitly defined. As such, \u27e8 ex:e1 owl:sameAs ex:e2 \u27e9 \\lang \\text{ ex:e1 owl:sameAs ex:e2 } \\rang \u27e8 ex:e1 owl:sameAs ex:e2 \u27e9 and \u27e8 ex:e2 owl:sameAs ex:e1 \u27e9 \\lang \\text{ ex:e2 owl:sameAs ex:e1 } \\rang \u27e8 ex:e2 owl:sameAs ex:e1 \u27e9 are different and stem from different linksets. However, in the resulting UNION , a distinction between the two triples is not explicitly made, meaning that the direction is not of much importance here. This can be observed in the metadata of the union where only the property void:target is used instead of void:subjectsTarget and void:objectsTarget . ### Lens metadata ex: Union-Marvel-DC-Heroes a voidPlus: Lens ; voidPlus: has-lens-operator voidPlus: Union ; dcterms: description \"Identifying superheroes from DC and Marvel Universes'\" ; ll: target ex: Marvel-Heroes-Identity-1 ; ll: target ex: Marvel-Heroes-Identity-2 ; ll: target ex: DC-Heroes-Identity-1 ; void: triples 10 . ### The annotated Lens ex: Union-Marvel-Heroes { << hero: Black-Panther owl: sameAs person: T-Challa >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Black-Widow owl: sameAs person: Natasha-Romanoff >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Captain-America owl: sameAs person: Steve-Rogers >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Captain-Marvel owl: sameAs person: Carol-Danvers >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Deadpool owl: sameAs person: Wade-Wilson >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Hulk owl: sameAs person: Bruce-Banner >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-1 . << hero: Ant-man owl: sameAs person: Tony-Stark >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-2 . << hero: Hawkeye owl: sameAs person: Clint-Barton >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-2 . << hero: Iron-man owl: sameAs person: Tony-Stark >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-2 ; prov: wasDerivedFrom ex: DC-Heroes-Identity . << hero: Spider-man owl: sameAs person: Peter-Parker >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity-2 ; prov: wasDerivedFrom ex: DC-Heroes-Identity . << hero: Superman owl: sameAs person: Clark-Kent >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: Batman owl: sameAs person: Bruce-Wane >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: Flash owl: sameAs person: Barry-Allen >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: GreenLantern owl: sameAs person: Alan-Scott >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: WonderWoman owl: sameAs person: Diana-Prince >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: Aquaman owl: sameAs person: Arthur-Curry >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: GreenArrow owl: sameAs person: OliverQueen >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: BoosterGold owl: sameAs person: Michael-Jon-Carter >> prov: wasDerivedFrom ex: Marvel-Heroes-Identity . }","title":"1. Union"},{"location":"06.LinkManipulation/#2-intersection","text":"Intersecting the DC ( ex:DC-Heroes-Identity ) and Marvel ( ex:Marvel-Heroes-Identity ) linksets of superheroes results in ex:Comics-Heroes-Identity , a lens which reveals the only two links shared by the sets. The first link in this lens establishes an identity relation between hero:Spider-man the hero and person:Peter-Parker the fictitious character, while the second link asserts that the superhero hero:Iron-man and the fictitious entity representing Tony Stark are co-referent entities. Example 4: Intersection: Extracting shared links. ex: Comics-Heroes-Identity a voidPlus: Lens ; voidPlus: has-lens-operator voidPlus: Intersection ; dcterms: description \"Quality Check: Identifying superheroes from both Marvel and DC comics\" ; voidPlus: expression \"ex:DC-Heroes-Identity INTERSECTION ex:Marvel-Heroes-Identity\" ; void: target ex: DC-Heroes-Identity , ex: Marvel-Heroes-Identity ; void: triples 4 ; \u2022 \u2022 \u2022 ex: Comics-Heroes-Identity { << hero: Spider-man owl: sameAs person: Peter-Parker >> prov: wasDerivedFrom ex: DC-Heroes-Identity ; prov: wasDerivedFrom ex: Marvel-Heroes-Identity . << hero: Iron-man owl: sameAs person: Tony-Stark >> prov: wasDerivedFrom ex: DC-Heroes-Identity ; prov: wasDerivedFrom ex: Marvel-Heroes-Identity . }","title":"2. Intersection"},{"location":"06.LinkManipulation/#3-difference","text":"Contrarily to the UNION and INTERSECTION operators, the DIFFERENCE operator requires assigning values to the following two properties: void:subjectsTarget and void:objectsTarget . This allows for the explicit and accurate documentation of the lens resulting from performing the Difference manipulation over linksets, lenses or the combination of both a linkset and a lens. This specification is not to put a particular emphasis on the direction of the links instead, it is for knowing the linkset or lens from which the resulting links will stem from. Example 5: DIFFERENCE for extracting triples only existing in void:subjectsTarget . ex: Comics-Heroes-Identity a voidPlus: Lens ; voidPlus: has-lens-operator voidPlus: Difference ; dcterms: description \"Quality Check: Identifying superheroes from both Marvel and DC comics\" ; voidPlus: expression \"ex:DC-Heroes-Identity DIFFERENCE ex:Marvel-Heroes-Identity\" ; void: target ex: DC-Heroes-Identity , ex: Marvel-Heroes-Identity ; void: triples 8 ; \u2022 \u2022 \u2022 ex: DC-Heroes-Identity { << hero: Superman owl: sameAs person: Clark-Kent >> validation: status \"True\" . << hero: Batman owl: sameAs person: Bruce-Wane >> validation: status \"True\" . << hero: Flash owl: sameAs person: Barry-Allen >> validation: status \"True\" . << hero: GreenLantern owl: sameAs person: Alan-Scott >> validation: status \"True\" . << hero: WonderWoman owl: sameAs person: Diana-Prince >> validation: status \"True\" . << hero: Aquaman owl: sameAs person: Arthur-Curry >> validation: status \"True\" . << hero: GreenArrow owl: sameAs person: OliverQueen >> validation: status \"True\" . << hero: BoosterGold owl: sameAs person: Michael-Jon-Carter >> validation: status \"True\" . }","title":"3. Difference"},{"location":"06.LinkManipulation/#4-composition","text":"","title":"4. Composition"},{"location":"06.LinkManipulation/#5-in-set","text":"Imagine being a movie fan particularly interested in knowing more on a set of ten fictional characters carefully handpicked. This intellectual knowledge includes discovering for example knowing in what movies a fictional character played in, what was the rate of the movies, and so on\u2026 Example 6 exhaustively lists these movie characters of interest. Example 6: ex:preference , set of movie characters. ex: preference rdf : _ 4735532571260424746 hero: Captain-Marvel , person: Carol-Danvers , hero: Captain-America , person: Steve-Rogers , hero: Deadpool , person: Wade-Wilson , hero: Black-Panther , person: T-Challa , hero: Spider-man , person: Peter-Parker , hero: Iron-man , person: Tony-Stark , hero: Ant-man , person: Tony-Stark , hero: Black-Widow , person: Natasha-Romanoff , hero: Hulk , person: Bruce-Banner , hero: Hawkeye , person: Clint-Barton . To know more about these handpicked characters, we have isolated ex:MovieInfo , a dataset where information such as movie\u2019s fictional character and movie\u2019s rate are documented. Example 7: ex:MovieCharacter , a movie database. ex: MovieInfo { movie: WonderWoman schema: character hero: WonderWoman ; schema: contentRating 7.4 . movie: WonderWoman-Bloodlines schema: character hero : WonderWoman ; schema: contentRating 5.8 . movie: Superman schema: character hero: Superman ; schema: contentRating 7.3 . movie: Superman-Returns schema: character hero: Superman ; schema: contentRating 6 . movie: Man-of-Steel schema: character hero: Superman ; schema: contentRating 7 . movie: Batman schema: character hero: Batman ; schema: contentRating 7.5 . movie: Batman-Returns schema: character hero: Batman ; schema: contentRating 7 . movie: Batman-Robin schema: character hero: Batman ; schema: contentRating 3.7 . movie: Spider-man schema: character hero: Spider-man ; schema: contentRating 7.3 . movie: The-amazing-Spider-man schema: character hero: Spider-man ; schema: contentRating 6.5 . movie: Spider-man-home-coming schema: character hero: Spider-man ; schema: contentRating 7.4 . } From the dataset ex:MovieInfo , an embedded linkset defined by the link predicate schema:character is extracted and presented in Example 8. In this example, four fictional characters (hero:WonderWoman, Superman, Batman and Spider-man) are linked to various superhero-movies leading to a total of eleven links. Example 8: ex:LinksetMovieCharacter , the movie character linkset. ex: LinksetMovieCharacter { movie: WonderWoman schema: character hero: WonderWoman . movie: WonderWoman-Bloodlines schema: character hero: WonderWoman . movie: Superman schema: character hero: Superman . movie: Superman-Return schema: character hero: Superman . movie: Man-of-Steel schema: character hero: Superman . movie: Batman schema: character hero: Batman . movie: Batman-Returns schema: character hero: Batman . movie: Batman-Robin schema: character hero: Batman . movie: Spider-man schema: character hero: Spider-man . movie: The-amazing-Spider-man schema: character hero: Spider-man . movie: Spider-man-home-coming schema: character hero: Spider-man . } A close look at Example 6 reveals that the set of handpicked characters is a subset of the Marvel Universe. Now, overlaying this set ( ex:preference ) onto the linkset of movie characters ( ex:LinksetMovieCharacter ) implies extracting the links in ex:LinksetMovieCharacter where entities at the subject or object position of the link can be find in ex:preference . Formally, this can be translated into: ex:LinksetMovieCharacter InSet ex:preference . Example 9: Movie Characters from ex:LinksetMovieCharacter that are derived from Marvel\u2019s matched universe set. ex: MovieCharacter { movie: Spider-man schema: character hero: Spider-man . movie: The-amazing-Spider-man schema: character hero: Spider-man . movie: Spider-man-home-coming schema: character hero: Spider-man . }","title":"5. In Set"},{"location":"07.LinkVisualisation/","text":".katex img { display: block; position: absolute; width: 100%; height: inherit; } Visualisation Support for Identity Networks \u00b6 Work in progress For a single real world entity e e e , there exists in the digital world multiple representations. In the Semantic Web, these representations are called resources and are materialised using Internationalised Resource Identifiers also known as IRIs. To name a few examples, Rembrandt Harmenszoon van Rijn the painter is https://www.wikidata.org/wiki/Q5598 in Wikidata, http://vocab.getty.edu/ulan/500011051 in Getty, https://viaf.org/viaf/64013650 in VIAF, and http://dbpedia.org/resource/Rembrandt in Dbpedia. Like this, co-referents of Rembrandt Harmenszoon van Rijn are identified across these for datasets and many more. Certainly, this bridging of resources across several datasources brings us closer to a more complete comprehensive view on the real life entities under investigation rather than the information available in a single or fewer datasources. This practice of finding sets of co-referent resources across data-sources that together point to the same and unique real world is the task of entity matching algorithms. During this process, matching entities then consists in creating identity links \u2013 pairwise identity relationships between co-referent resources \u2013 based on their computed matching degree of confidence using often times the owl:sameAs predicate [ Raad2018 , Achichi2017 , Volz2009 ]. In this scope, the set of co-referent resources connected through such identity relationships forms what we denote in this paper as an Identity Link Network ( iln ) and define as an undirected network of nodes denoting resources or IRIs, where edges representing identity relationships between nodes may or may not come with weight, which stands as a degree of confidence between matched resources . However, the focus of this work is on weighted undirected identity networks. Fig. 1 provides a real life example of an iln bridging four datasources depicted in the four node colours [ see Section XXX ], where the plain black lines denote full confidence according to the underlined matching criteria, in contrast with dotted red lines that denote less confidence. Fig. 1: An identity link network connecting nodes from four datasets (color of the node). This a candidate set of co-referent resources potentially pointing to \u201cKipshaven Lucas\u201d Depending on the data at hand, finding co-referents can turn out either trivial or complex. Across real-life data however, this often appears to be a rather complex task [ Raad2019 ], leading to incorrect co-referents. Clearly, it is agreable that where there is a high likelihood of error (see the motivation section), there is a prominent need for correction as such error could lead to misleading analysis, wrong conclusions, bad-decision making, etc. From this perspective, a raising concern can be transliterate into \u201c how to facilitate the detection of errors? \u201d. In other words, \u201c how to efficiently validate links? \u201d or \u201c how to ensure that the resources in an iln indeed are co-referent? \u201d. Answering this specific question is philosophically complex [ Frege1952 ], but out of the scope of this paper. A simple solution that we adopt here is that a correct iln is a set of resources pointing to the same real life object . On the one hand, links are crucial for a successful integration of Semantic Web data. As such, it is paramount that false positive links are not part of the constituent body of identity links granting a successful integration. On the other, results of entity matching bring about statement uncertainty (links). Although this uncertainty does not make a statement untrue, it does nonetheless raise the need for statement verification. A strategically quick way for investigating an identity network is by process of elimination. Among others, we argue that this can be done visually by first exploiting the hint a bridge constitutes (i.e. first look for the existence of bridges, and indeed investigate it in the event that such link exist) and if necessary investigate the rest. Our contribution to the semi-automation of identity-statement verification is the visualisation of the resulting networks in a meaningfully summarised structure where bridges are gradually highlighted as the size and look of identity networks can revealed daunting. For such purpose and in the context of entity matching, we argue that the weight of a link can help in a meaningful summarisation of ilns . 1. Motivation \u00b6 An ideal identity link network ( iln ) can be expressed as a full mesh network structure, meaning that if a real-life entity has n co-referents in m datasets, all the possible n ( n \u2212 1 ) 2 \\frac{n(n-1)}{2} 2 n ( n \u2212 1 ) \u200b undirected connections among them are captured (with high-confidence), leaving one with no much to investigate. However, with real-life data, this is a different story as a high-confidence full mesh structure is not always observed. Instead, partial meshes are the ones often observed (e.g. due to incomplete knowledge), which may include one or more bridges , indicating potential identity disagreement . In practice, implying identity correctness from a full mesh network is not always possible (as vagueness and uncertainty may be at the core of the network generation), let alone from a network which includes bridges. This suggests that identity networks generated from real-life data require investigation. The underlined need for further validation is strengthened by the following: The high likelihood of matching results to be imperfect An iln can be constructed automatically (through entity matching algorithms) or manually. Either way, ilns may have their quality compromised by the inclusion of false positives (i.e. resources that are in the same iln but do not denote the same real-world entity). Here, we highlight a few reasons, either obvious or hidden. Obvious cases in which false positives are more likely to happen are, for example, (a) the low quality of data. Often enough, data come with several issues that may impair the matching process. For example, it can be of low quality ; it can be incomplete in the sense that entities to disambiguate are not described at the same granularity; it may contain inconsistencies / errors such as misspellings, spelling variances, contradicting values; or yet it may rely on different calendars, among others. Further examples include (b) the problematic context-independent semantics of owl:sameAs or (c) the inadvertent misuse of the owl:sameAs predicate as highlighted in [ Halpin2010 ], and (d) the inadequacy of matching algorithms to tackle the data-sources at hand. In less obvious cases, false positives are the results of challenges that need to be addressed across the matching process. These challenges can be categorised as ways of (i) modelling how to compare the values of entities\u2019 properties , and (ii) inferring co-reference or identity . (i) The modelling of ways to find supporting evidence for isolating potential matching candidates is crucial to inferring identity for a pair of resources. In this endeavour, some approaches may produce discrete / precise results while others may produce continuous / vague results. For example, comparing if two resources have the same name can be precise if exact-match is the method of choice, but the same comparison can also be evaluated with vagueness if a similarity approximation method is chosen instead. In the former case, comparing (Alessandra, Sandra) produces false or 0 as result, while in the second approach the comparison result-space is in the interval [0, 1] to describe the degree of truth such that (Alessandra and Alexandra) is more \u201ctrue\u201d than (Alessandra, Sandra), which is more \u201ctrue\u201d than (Alessandra, John). As expected, this too is likely to lead to incorrect matchings. Although the precise modelling produces more reliable results, it appears limited [ Schoenfisch2014 ] as unable to cope with vagueness and thereby reducing the chance of finding all correct matches, leading to an increase of false negatives . On the other hand, although the modelling of vagueness opens a range of possibilities, it also increases the injection of false positives (noise). (ii) Properly inferring identity for a pair of resources depends not only on how well the aforementioned precision versus vagueness issue is addressed as computed evidence (i) but also on how strong is the discriminating power of combined evidence to infer identity agreement (co-referents) or disagreement between a pair of entities. This is often worsen by data incompleteness: the less complete the less discriminative. For example inferring that entities e 1 e_1 e 1 \u200b and e 2 e_2 e 2 \u200b are the same whenever their share similar names and place of birth is a low discriminating criteria as there exists multiple entities materialised as Jan Jans and born in Amsterdam . Mind you that here too, the solution space of such comparison is a sliding scale within the unit intervbal [0, 1] corresponding to a degree of confidence known as uncertainty . This means that inferring if a resource Jan Jans born in Amsterdam is co-referent of another resource Jan Janzoon born in Amsterdam is either true or false (not vague) begs for a level of confidence that reflects both the vague comparison of their names (almost the same) but also the lack of strong discriminating criteria to be sure about the inference. The impracticality of manually correcting large ilns Data are getting larger by the years with the event of better technologies and the need for more digitisation. When it comes to entity matching, a consequence of large data is the discovery of a large amount of links mingled with a surplus of false positives. Mind you, it does so even more given the low quality of input data for matching approaches. But, this is not all! The goal of entity matching in the Semantic Web is data integration, meaning that the concern is likely more ambitious than the discovery of links within a single data-source or even just two data-sources, but among several sources that often describe different/complimentary views on the same entities. In this setting, the size and the number of derived iln also increase. The complexity of the validation process itself which is certainly not exempt from errors if automated Even though automation can be used, it too comes with imperfection and uncertainty. Ultimately, the most reliable solution is manual validation, though not exempted from error. Opting for a semi-automated approach (human-computer interaction) however may combine benefits of both approaches. 2. Related work \u00b6 A considerable body of scholars in the field of community detection have organised nodes on the belief that the more edges a group of node share the greater the likelihood that they are similar . 3. A Visual Vocabulary For ILNS \u00b6 As a network increases in size, it gets harder to make sense of its hidden structure, its visualisation becomes difficult and rendering the data to display becomes expensive as the network resembles a meaningless cloud of points [ Alhajj20014 ]. For an intuitive understanding of a graph\u2019s hidden structure and for reducing the amount of data to render, we display a compressed version of an original graph based on the weight of its links. This section presents the basic vocabulary for interpreting such a compressed visual representation of a network based on the weight-annotation of its links. Details on how to construct such a network and its use to detect identify communities is provided in Section XXX . 3.1 Compact with Satellites \u00b6 The term compact node is defined as a compressed representation of linked-nodes where the link(s) share the same weight-annotation . Such an annotation is perceive as an annotation-pattern . In this context, the shared annotation-pattern is the weight-annotation with the highest strength . A compact node \u2013 the blue node in the figure on the right \u2013 is given a color , a label (one of the labels of the nodes it represents) and its size is proportional to the number of nodes compressed. It also comes with two additional numerical data: N is the number of compressed nodes and L shows the number of links in the network as a fraction N \u2217 ( N \u2212 1 ) 2 \\frac{N*(N-1)}{2} 2 N \u2217 ( N \u2212 1 ) \u200b of all possible links. In addition, the main color of the node is overlaid by a white circular color indicating the proportion of missing links, indicating the ``incompleteness\u2019\u2018 of the sub-network represented by the compact node. S indicates the shared weight-annotation. A satellite node \u2013 orange nodes in the figure \u2013 is a single node that connects to one or more compact nodes and do not share a pattern. Such node is also given a color , a label , a fix size where N is always 1 and no L. Fig. 1 illustrates an identity network of 30 nodes connected by 176 links annotated with strength value of 0.5 or 1 (discrete value annotation). The compact node coloured in blue compressed 20 nodes (N:20) that are all connected with links of strength 1 indicated by S:[1.0]. The white overlay color indicates the proportion of missing links and the number of exact missing links can be computed using the numeric data provided by L:166/190. In this case, 24 links are missing. The other 10 nodes in the figure are viewed as first level satellites of the compact node. In this example, the compressed representation reduces the number of initial nodes to be displayed from 30 to 11 and and the label pairs from 166 links to 10, showing the strength of our compressed visualisation. 3.2 Community mode \u00b6 In community mode , satellites are not allowed because they are direct neighbours of a compact node and are not network themselves (N=1). Whenever such a phenomenon occurs, in community mode, the compact node pulls in all satellites whenever they exist. It then updates accordingly its size and its counts of child-nodes and links. Finally, it turns its original plain black border into a dashed border to indicate the existence of satellites. Fig. 2 shows Fig. 1 in a community mode. It high-lights that in the original network of 30 nodes, no secondary subnetwork exists, as all its satellites got pulled in. This is reflected in its dashed border as well as in its child and link counts, in contrast with the information displayed in Fig. 1. 3.3 Interlink \u00b6 An interlink is a compressed link that (i) connects a satellite node (N=1) to a compact node (N \u2265 \\geq \u2265 2) or, (ii) connects two compact nodes. When links are being compressed, the thickness of the interlink is proportionally increased. With quantitative weight-annotation in the interval ]0, 1], (i) dashed red links are of value ]0, 1[, (ii) black plain lines are of value 1, and (iii) the weaker the strength, the more sparse is the dashed red link. The Figure on the right illustrates three interconnected communities: Orange (north), Green (north-east) and Blue (south). Orange and Green are small, composed of 3 children each and no satellite (no dash border). While Orange is fully connected (L=3), Green has one missing link indicated by the white overlay color and L=\u2154. Blue, the biggest community node is of 24 children. It connects to Orange using 1 interlink and connects to the Green using 10 links hinted by the thicker interlink. Here, an initial network of of 30 nodes and 75 links is now reduced to 3 nodes and 2 links. 4. Weight-Based Identity Agreement \u00b6 Section 3 presents a visualisation approach for large networks by grouping nodes that share the same strength (possibly grouped by interval) as a single compact node. It is intended to efficiently visualise Identity Link Networks with the goal of validation. In this section, we first elaborate on the supporting rational for opting for the merging / compression of nodes in an identity network based on the weight of their examined common traits and then describe the approach. 4.1 Rational \u00b6 In an identity graph, common traits between pair of nodes have been examined (by expert or computer) and possibly result in an identity link. Ideally, all nodes of an identity graph are co-referent of the same real object. Consequently, these nodes should densely connect, and it should not be possible to separate such an identity graph into identity disagreement groups (for example by applying a community detection algorithm). However, in real life, the computations of the links of an identity network is subject to errors and incompleteness, leading to the potential inclusion of noise (false positives nodes and links) and the absence of numerous true positive links. It has been shown in [ Raad2018 , Raad2020 ] that community detection algorithms can be used to detect sub-groups in a candidate identity link network that points to one or more real world objects. In such community detection approaches or related tasks such as graph compression / summarisation, nodes merging is the shared commonality. Only, the rational for the merge differs. Some merging technics ensure that decompression is optimised [ Toivonen2011 ], some focus on link density [ xxx ] while others focus on structure conservation [ Shen2010 , Jin2018 ]. In this paper, the rational for merging nodes in the context of identity link network and link validation revolves around the meaning of a link and of its weight as the belief is that the combined meaning these two shed light on a different merging perspective. To understand this, let take a look at the network of eight nodes in Fig. XXX. First, suppose that Fig. XXX illustrates a social network of friendship. Although nodes 1 and 2 share a friendship, the rest of the friends of 1 do not interact with 2. The same goes for the friends of 2. Furthermore, while some of the friends of 1 know each-other, the opposite is observed with 2 meaning that his friends interact with no one else but him. Interpretation wise, these observations may imply that 1 and his friends (except 2) share more traits than 2 does with his friends and that 1 and 2 may be friends for something that only the two of them share. Strengthening the interpretation by also including the interpretation of the weights will mainly highlight the quality of friendships but will not change the intuition of the presence of two groups of friends. In this first scenario, the meaning of a link allows the intuition that the density of the links may indeed suggest two subgroups of friends and may also suggest a need for structure conservation. However, if we now assume that Fig. XXX denotes instead a network of an identity agreement, it is not necessarily the case that two sub-networks can be inferred (only one instead) because all the nodes are supposedly collapsable into a single one as they are in this new context instantiations of the same real world object. In other words, in an identity network, a link implies that connected nodes can be merged as they are co-referent while the link\u2019s weight comes as a merging countermeasure that emphasises the confidence for indeed merging connected nodes if needs be . It follows that, in an iln , nodes that share the same weight (or weight interval) are also more likely to cluster together. With this rational, we now introduce in the next section how to proceed with merging nodes based on shared weight. 4.2 Approach \u00b6 We use Fig. 1, a real life example of an identity network of 10 nodes and 42 links to illustrate the heuristic behind the proposed approach. Here, the score of a link is computed on the basis of name similarity and the network connects instances of \u201cKipshaven Lucas\u201d from four datasets (Burial -two red nodes-, Marriage -one blue node-, Baptism -six green nodes- and Ecartico -one orange node-) using plain black links (S=1) or red dotted links ( S \\in ]0, 1] S \\in ]0, 1] ). The weaker the link, the more sparse the red dotted line is. At first glance, this network structure looks uninformative. However when carefully looking at it, a pattern strikes the eyes: nodes connected with the plane line can be grouped together intuitively . This suggests (i) the use of weight-annotation for grouping nodes and preferably, (ii) starting from the links with the highest weight. So, if we organise the link strengths in Fig. 1 from the most important (S=1) to the least important (S=0.91), pop the highest weighted link in the current iteration and temporarily strip the network from inferior links, we then end up building new sub-networks where all nodes are connected at least once with links annotated with the current strength at each iteration (selection without replacement). For example, if we strip Fig. 1 from all other links but those of strength 1 (plain black lines) at first, we end up with three distinct sub-graphs (respectively of five, two and three nodes) where all the nodes are connected by links of strength 1. Note that, if more nodes remain, this process is repeated following the ordered link\u2019s strength. Once all sub-networks are found, The remaining links can now be used to reconnect the detected sub-graphs as illustrated in Fig. 6. Using this approach, we can now informatively visualise a large network of 439 nodes and 7,614 links representing the identity network of Obama which is now displayed in Fig. 7 with 264 nodes less. Nevertheless, the approach could be improved for more compression. To address this, our solution is to enable a compact node to swallow its satellites nodes of size 1 by default . Note that this adjustment allows for setting the maximum size of the satellite node (it can be set to 10 instead of 1 for example) to be swallowed on demand. As a consequence, if the size of the satellite is set to x > 1, all compact nodes of size y \\in ]1, x] y \\in ]1, x] get the status of non valid community . In case a satellite is shared by more than one compact node, which is the case for two nodes in Fig. 6, a tie is broken using progressively 1 (1) the compact node with the strongest link to the satellite node; (2) the biggest compact node; (3) the node with the longest URI to be deterministic. We test this idea by comparing its results to a number of renown community detection algorithms on our benchmark and reports the findings in \\autoref{sec_pakage}. Fig. 7: A compact identity network of Obama which is originally a network of 439 nodes and 7614 links. Stop whenever the tie is broken. No need to test all tie breaker conditions. \u21a9","title":"8. Visualising link Network"},{"location":"07.LinkVisualisation/#visualisation-support-for-identity-networks","text":"Work in progress For a single real world entity e e e , there exists in the digital world multiple representations. In the Semantic Web, these representations are called resources and are materialised using Internationalised Resource Identifiers also known as IRIs. To name a few examples, Rembrandt Harmenszoon van Rijn the painter is https://www.wikidata.org/wiki/Q5598 in Wikidata, http://vocab.getty.edu/ulan/500011051 in Getty, https://viaf.org/viaf/64013650 in VIAF, and http://dbpedia.org/resource/Rembrandt in Dbpedia. Like this, co-referents of Rembrandt Harmenszoon van Rijn are identified across these for datasets and many more. Certainly, this bridging of resources across several datasources brings us closer to a more complete comprehensive view on the real life entities under investigation rather than the information available in a single or fewer datasources. This practice of finding sets of co-referent resources across data-sources that together point to the same and unique real world is the task of entity matching algorithms. During this process, matching entities then consists in creating identity links \u2013 pairwise identity relationships between co-referent resources \u2013 based on their computed matching degree of confidence using often times the owl:sameAs predicate [ Raad2018 , Achichi2017 , Volz2009 ]. In this scope, the set of co-referent resources connected through such identity relationships forms what we denote in this paper as an Identity Link Network ( iln ) and define as an undirected network of nodes denoting resources or IRIs, where edges representing identity relationships between nodes may or may not come with weight, which stands as a degree of confidence between matched resources . However, the focus of this work is on weighted undirected identity networks. Fig. 1 provides a real life example of an iln bridging four datasources depicted in the four node colours [ see Section XXX ], where the plain black lines denote full confidence according to the underlined matching criteria, in contrast with dotted red lines that denote less confidence. Fig. 1: An identity link network connecting nodes from four datasets (color of the node). This a candidate set of co-referent resources potentially pointing to \u201cKipshaven Lucas\u201d Depending on the data at hand, finding co-referents can turn out either trivial or complex. Across real-life data however, this often appears to be a rather complex task [ Raad2019 ], leading to incorrect co-referents. Clearly, it is agreable that where there is a high likelihood of error (see the motivation section), there is a prominent need for correction as such error could lead to misleading analysis, wrong conclusions, bad-decision making, etc. From this perspective, a raising concern can be transliterate into \u201c how to facilitate the detection of errors? \u201d. In other words, \u201c how to efficiently validate links? \u201d or \u201c how to ensure that the resources in an iln indeed are co-referent? \u201d. Answering this specific question is philosophically complex [ Frege1952 ], but out of the scope of this paper. A simple solution that we adopt here is that a correct iln is a set of resources pointing to the same real life object . On the one hand, links are crucial for a successful integration of Semantic Web data. As such, it is paramount that false positive links are not part of the constituent body of identity links granting a successful integration. On the other, results of entity matching bring about statement uncertainty (links). Although this uncertainty does not make a statement untrue, it does nonetheless raise the need for statement verification. A strategically quick way for investigating an identity network is by process of elimination. Among others, we argue that this can be done visually by first exploiting the hint a bridge constitutes (i.e. first look for the existence of bridges, and indeed investigate it in the event that such link exist) and if necessary investigate the rest. Our contribution to the semi-automation of identity-statement verification is the visualisation of the resulting networks in a meaningfully summarised structure where bridges are gradually highlighted as the size and look of identity networks can revealed daunting. For such purpose and in the context of entity matching, we argue that the weight of a link can help in a meaningful summarisation of ilns .","title":"Visualisation Support for Identity Networks"},{"location":"07.LinkVisualisation/#1-motivation","text":"An ideal identity link network ( iln ) can be expressed as a full mesh network structure, meaning that if a real-life entity has n co-referents in m datasets, all the possible n ( n \u2212 1 ) 2 \\frac{n(n-1)}{2} 2 n ( n \u2212 1 ) \u200b undirected connections among them are captured (with high-confidence), leaving one with no much to investigate. However, with real-life data, this is a different story as a high-confidence full mesh structure is not always observed. Instead, partial meshes are the ones often observed (e.g. due to incomplete knowledge), which may include one or more bridges , indicating potential identity disagreement . In practice, implying identity correctness from a full mesh network is not always possible (as vagueness and uncertainty may be at the core of the network generation), let alone from a network which includes bridges. This suggests that identity networks generated from real-life data require investigation. The underlined need for further validation is strengthened by the following: The high likelihood of matching results to be imperfect An iln can be constructed automatically (through entity matching algorithms) or manually. Either way, ilns may have their quality compromised by the inclusion of false positives (i.e. resources that are in the same iln but do not denote the same real-world entity). Here, we highlight a few reasons, either obvious or hidden. Obvious cases in which false positives are more likely to happen are, for example, (a) the low quality of data. Often enough, data come with several issues that may impair the matching process. For example, it can be of low quality ; it can be incomplete in the sense that entities to disambiguate are not described at the same granularity; it may contain inconsistencies / errors such as misspellings, spelling variances, contradicting values; or yet it may rely on different calendars, among others. Further examples include (b) the problematic context-independent semantics of owl:sameAs or (c) the inadvertent misuse of the owl:sameAs predicate as highlighted in [ Halpin2010 ], and (d) the inadequacy of matching algorithms to tackle the data-sources at hand. In less obvious cases, false positives are the results of challenges that need to be addressed across the matching process. These challenges can be categorised as ways of (i) modelling how to compare the values of entities\u2019 properties , and (ii) inferring co-reference or identity . (i) The modelling of ways to find supporting evidence for isolating potential matching candidates is crucial to inferring identity for a pair of resources. In this endeavour, some approaches may produce discrete / precise results while others may produce continuous / vague results. For example, comparing if two resources have the same name can be precise if exact-match is the method of choice, but the same comparison can also be evaluated with vagueness if a similarity approximation method is chosen instead. In the former case, comparing (Alessandra, Sandra) produces false or 0 as result, while in the second approach the comparison result-space is in the interval [0, 1] to describe the degree of truth such that (Alessandra and Alexandra) is more \u201ctrue\u201d than (Alessandra, Sandra), which is more \u201ctrue\u201d than (Alessandra, John). As expected, this too is likely to lead to incorrect matchings. Although the precise modelling produces more reliable results, it appears limited [ Schoenfisch2014 ] as unable to cope with vagueness and thereby reducing the chance of finding all correct matches, leading to an increase of false negatives . On the other hand, although the modelling of vagueness opens a range of possibilities, it also increases the injection of false positives (noise). (ii) Properly inferring identity for a pair of resources depends not only on how well the aforementioned precision versus vagueness issue is addressed as computed evidence (i) but also on how strong is the discriminating power of combined evidence to infer identity agreement (co-referents) or disagreement between a pair of entities. This is often worsen by data incompleteness: the less complete the less discriminative. For example inferring that entities e 1 e_1 e 1 \u200b and e 2 e_2 e 2 \u200b are the same whenever their share similar names and place of birth is a low discriminating criteria as there exists multiple entities materialised as Jan Jans and born in Amsterdam . Mind you that here too, the solution space of such comparison is a sliding scale within the unit intervbal [0, 1] corresponding to a degree of confidence known as uncertainty . This means that inferring if a resource Jan Jans born in Amsterdam is co-referent of another resource Jan Janzoon born in Amsterdam is either true or false (not vague) begs for a level of confidence that reflects both the vague comparison of their names (almost the same) but also the lack of strong discriminating criteria to be sure about the inference. The impracticality of manually correcting large ilns Data are getting larger by the years with the event of better technologies and the need for more digitisation. When it comes to entity matching, a consequence of large data is the discovery of a large amount of links mingled with a surplus of false positives. Mind you, it does so even more given the low quality of input data for matching approaches. But, this is not all! The goal of entity matching in the Semantic Web is data integration, meaning that the concern is likely more ambitious than the discovery of links within a single data-source or even just two data-sources, but among several sources that often describe different/complimentary views on the same entities. In this setting, the size and the number of derived iln also increase. The complexity of the validation process itself which is certainly not exempt from errors if automated Even though automation can be used, it too comes with imperfection and uncertainty. Ultimately, the most reliable solution is manual validation, though not exempted from error. Opting for a semi-automated approach (human-computer interaction) however may combine benefits of both approaches.","title":"1. Motivation"},{"location":"07.LinkVisualisation/#2-related-work","text":"A considerable body of scholars in the field of community detection have organised nodes on the belief that the more edges a group of node share the greater the likelihood that they are similar .","title":"2. Related work"},{"location":"07.LinkVisualisation/#3-a-visual-vocabulary-for-ilns","text":"As a network increases in size, it gets harder to make sense of its hidden structure, its visualisation becomes difficult and rendering the data to display becomes expensive as the network resembles a meaningless cloud of points [ Alhajj20014 ]. For an intuitive understanding of a graph\u2019s hidden structure and for reducing the amount of data to render, we display a compressed version of an original graph based on the weight of its links. This section presents the basic vocabulary for interpreting such a compressed visual representation of a network based on the weight-annotation of its links. Details on how to construct such a network and its use to detect identify communities is provided in Section XXX .","title":"3. A Visual Vocabulary For ILNS"},{"location":"07.LinkVisualisation/#31-compact-with-satellites","text":"The term compact node is defined as a compressed representation of linked-nodes where the link(s) share the same weight-annotation . Such an annotation is perceive as an annotation-pattern . In this context, the shared annotation-pattern is the weight-annotation with the highest strength . A compact node \u2013 the blue node in the figure on the right \u2013 is given a color , a label (one of the labels of the nodes it represents) and its size is proportional to the number of nodes compressed. It also comes with two additional numerical data: N is the number of compressed nodes and L shows the number of links in the network as a fraction N \u2217 ( N \u2212 1 ) 2 \\frac{N*(N-1)}{2} 2 N \u2217 ( N \u2212 1 ) \u200b of all possible links. In addition, the main color of the node is overlaid by a white circular color indicating the proportion of missing links, indicating the ``incompleteness\u2019\u2018 of the sub-network represented by the compact node. S indicates the shared weight-annotation. A satellite node \u2013 orange nodes in the figure \u2013 is a single node that connects to one or more compact nodes and do not share a pattern. Such node is also given a color , a label , a fix size where N is always 1 and no L. Fig. 1 illustrates an identity network of 30 nodes connected by 176 links annotated with strength value of 0.5 or 1 (discrete value annotation). The compact node coloured in blue compressed 20 nodes (N:20) that are all connected with links of strength 1 indicated by S:[1.0]. The white overlay color indicates the proportion of missing links and the number of exact missing links can be computed using the numeric data provided by L:166/190. In this case, 24 links are missing. The other 10 nodes in the figure are viewed as first level satellites of the compact node. In this example, the compressed representation reduces the number of initial nodes to be displayed from 30 to 11 and and the label pairs from 166 links to 10, showing the strength of our compressed visualisation.","title":"3.1 Compact with Satellites"},{"location":"07.LinkVisualisation/#32-community-mode","text":"In community mode , satellites are not allowed because they are direct neighbours of a compact node and are not network themselves (N=1). Whenever such a phenomenon occurs, in community mode, the compact node pulls in all satellites whenever they exist. It then updates accordingly its size and its counts of child-nodes and links. Finally, it turns its original plain black border into a dashed border to indicate the existence of satellites. Fig. 2 shows Fig. 1 in a community mode. It high-lights that in the original network of 30 nodes, no secondary subnetwork exists, as all its satellites got pulled in. This is reflected in its dashed border as well as in its child and link counts, in contrast with the information displayed in Fig. 1.","title":"3.2 Community mode"},{"location":"07.LinkVisualisation/#33-interlink","text":"An interlink is a compressed link that (i) connects a satellite node (N=1) to a compact node (N \u2265 \\geq \u2265 2) or, (ii) connects two compact nodes. When links are being compressed, the thickness of the interlink is proportionally increased. With quantitative weight-annotation in the interval ]0, 1], (i) dashed red links are of value ]0, 1[, (ii) black plain lines are of value 1, and (iii) the weaker the strength, the more sparse is the dashed red link. The Figure on the right illustrates three interconnected communities: Orange (north), Green (north-east) and Blue (south). Orange and Green are small, composed of 3 children each and no satellite (no dash border). While Orange is fully connected (L=3), Green has one missing link indicated by the white overlay color and L=\u2154. Blue, the biggest community node is of 24 children. It connects to Orange using 1 interlink and connects to the Green using 10 links hinted by the thicker interlink. Here, an initial network of of 30 nodes and 75 links is now reduced to 3 nodes and 2 links.","title":"3.3 Interlink"},{"location":"07.LinkVisualisation/#4-weight-based-identity-agreement","text":"Section 3 presents a visualisation approach for large networks by grouping nodes that share the same strength (possibly grouped by interval) as a single compact node. It is intended to efficiently visualise Identity Link Networks with the goal of validation. In this section, we first elaborate on the supporting rational for opting for the merging / compression of nodes in an identity network based on the weight of their examined common traits and then describe the approach.","title":"4. Weight-Based Identity Agreement"},{"location":"07.LinkVisualisation/#41-rational","text":"In an identity graph, common traits between pair of nodes have been examined (by expert or computer) and possibly result in an identity link. Ideally, all nodes of an identity graph are co-referent of the same real object. Consequently, these nodes should densely connect, and it should not be possible to separate such an identity graph into identity disagreement groups (for example by applying a community detection algorithm). However, in real life, the computations of the links of an identity network is subject to errors and incompleteness, leading to the potential inclusion of noise (false positives nodes and links) and the absence of numerous true positive links. It has been shown in [ Raad2018 , Raad2020 ] that community detection algorithms can be used to detect sub-groups in a candidate identity link network that points to one or more real world objects. In such community detection approaches or related tasks such as graph compression / summarisation, nodes merging is the shared commonality. Only, the rational for the merge differs. Some merging technics ensure that decompression is optimised [ Toivonen2011 ], some focus on link density [ xxx ] while others focus on structure conservation [ Shen2010 , Jin2018 ]. In this paper, the rational for merging nodes in the context of identity link network and link validation revolves around the meaning of a link and of its weight as the belief is that the combined meaning these two shed light on a different merging perspective. To understand this, let take a look at the network of eight nodes in Fig. XXX. First, suppose that Fig. XXX illustrates a social network of friendship. Although nodes 1 and 2 share a friendship, the rest of the friends of 1 do not interact with 2. The same goes for the friends of 2. Furthermore, while some of the friends of 1 know each-other, the opposite is observed with 2 meaning that his friends interact with no one else but him. Interpretation wise, these observations may imply that 1 and his friends (except 2) share more traits than 2 does with his friends and that 1 and 2 may be friends for something that only the two of them share. Strengthening the interpretation by also including the interpretation of the weights will mainly highlight the quality of friendships but will not change the intuition of the presence of two groups of friends. In this first scenario, the meaning of a link allows the intuition that the density of the links may indeed suggest two subgroups of friends and may also suggest a need for structure conservation. However, if we now assume that Fig. XXX denotes instead a network of an identity agreement, it is not necessarily the case that two sub-networks can be inferred (only one instead) because all the nodes are supposedly collapsable into a single one as they are in this new context instantiations of the same real world object. In other words, in an identity network, a link implies that connected nodes can be merged as they are co-referent while the link\u2019s weight comes as a merging countermeasure that emphasises the confidence for indeed merging connected nodes if needs be . It follows that, in an iln , nodes that share the same weight (or weight interval) are also more likely to cluster together. With this rational, we now introduce in the next section how to proceed with merging nodes based on shared weight.","title":"4.1 Rational"},{"location":"07.LinkVisualisation/#42-approach","text":"We use Fig. 1, a real life example of an identity network of 10 nodes and 42 links to illustrate the heuristic behind the proposed approach. Here, the score of a link is computed on the basis of name similarity and the network connects instances of \u201cKipshaven Lucas\u201d from four datasets (Burial -two red nodes-, Marriage -one blue node-, Baptism -six green nodes- and Ecartico -one orange node-) using plain black links (S=1) or red dotted links ( S \\in ]0, 1] S \\in ]0, 1] ). The weaker the link, the more sparse the red dotted line is. At first glance, this network structure looks uninformative. However when carefully looking at it, a pattern strikes the eyes: nodes connected with the plane line can be grouped together intuitively . This suggests (i) the use of weight-annotation for grouping nodes and preferably, (ii) starting from the links with the highest weight. So, if we organise the link strengths in Fig. 1 from the most important (S=1) to the least important (S=0.91), pop the highest weighted link in the current iteration and temporarily strip the network from inferior links, we then end up building new sub-networks where all nodes are connected at least once with links annotated with the current strength at each iteration (selection without replacement). For example, if we strip Fig. 1 from all other links but those of strength 1 (plain black lines) at first, we end up with three distinct sub-graphs (respectively of five, two and three nodes) where all the nodes are connected by links of strength 1. Note that, if more nodes remain, this process is repeated following the ordered link\u2019s strength. Once all sub-networks are found, The remaining links can now be used to reconnect the detected sub-graphs as illustrated in Fig. 6. Using this approach, we can now informatively visualise a large network of 439 nodes and 7,614 links representing the identity network of Obama which is now displayed in Fig. 7 with 264 nodes less. Nevertheless, the approach could be improved for more compression. To address this, our solution is to enable a compact node to swallow its satellites nodes of size 1 by default . Note that this adjustment allows for setting the maximum size of the satellite node (it can be set to 10 instead of 1 for example) to be swallowed on demand. As a consequence, if the size of the satellite is set to x > 1, all compact nodes of size y \\in ]1, x] y \\in ]1, x] get the status of non valid community . In case a satellite is shared by more than one compact node, which is the case for two nodes in Fig. 6, a tie is broken using progressively 1 (1) the compact node with the strongest link to the satellite node; (2) the biggest compact node; (3) the node with the longest URI to be deterministic. We test this idea by comparing its results to a number of renown community detection algorithms on our benchmark and reports the findings in \\autoref{sec_pakage}. Fig. 7: A compact identity network of Obama which is originally a network of 439 nodes and 7614 links. Stop whenever the tie is broken. No need to test all tie breaker conditions. \u21a9","title":"4.2 Approach"},{"location":"08.LinkValidation/","text":"LINK VALIDATION \u00b6 Work in progress Imagine a researcher in the need of integrating data on publications, authors and education institutions. Wrongly executing this integration task could mean, for example, assigning the h-index of author A to author B or wrongly saying that author A is affiliated to an institution he has never been involved with. Hardly any matching algorithm returns only perfect results. In light of this, to reach a solid investigation result, it is of importance to support human validation once links have been created and optionally improved. 1. Uncertainty vs. Vagueness \u00b6 Referring to propositions ( the sky is blue, the bottle is full, the tomato is ripe\u2026 ) in logic, the use of degree of (un)certainty, belief or confidence is to emphasise an evaluation of the assignment of a truth value to a proposition which may or may not qualify as vague ( \u201cthe tomato is ripe\u201d ). For example, how certain are we in saying that the proposition \u201cthe tomato is ripe\u201d is true ? As [Lukasiewicz2008] illustrates, saying that \u201cJohn is a teacher\u201d and \u201cJohn is a student\u201d with respectively 0.3 and 0.7 as degrees of certainty is roughly saying that \u201cJohn is either a teacher or a student, but more likely a student\u201d . However, the vague statement \u201c John is tall\u201d with the assignment of 0.9 as degree of truth can be roughly translated as \u201cJohn is quite tall\u201d . The transition from assigning boolean truth values {0, 1} to assigning continuous values in the unit interval [0, 1] to propositions (event) is clearly the change from classical logic to fuzzy logic which is in the modelling paradigm of many-valued Logics . The latter truth space (unit interval) is motivated by the presence of vague concepts in a proposition, making it hard or sometimes even impossible to establish whether the proposition is completely true or false [ Lukasiewicz2008 ]. Now, whatever truth value is assigned to an event, sometimes, one wonders about the chance that the even will happen (Probability) or might happen (Possibility)[ Kovalerchuk2017 ]. Degrees of Truth is not to be confused with the Degrees of Certainty / Confidence as the latter is not an assignment of truth value as opposed to the former, but rather an evaluation of a weighted proposition (proposition with an assigned truth value) regardless of its truth value space ({0, 1} or [0, 1]). 1. Validation and Links\u2019 Context \u00b6 intrinsic properties the purpose or task for which the links are used. 2. Voting Strategies \u00b6 In the Semantic Web, the standard semantic OWL/DL interpretation of identity between two resources entails full equality , i.e. they are necessarily the same and share all their properties. Such semantic applies independently of context and therefore no validation or dispute apply since that semantic does not take any of it into account: things are either the same or they are not. [ Idrissou 2017 ] In real life problems, the equality between resources may depend (i) not only on their intrinsic properties (ii) but also on the purpose or task for which they are used. For example, an organisation A and B are the same in context 1 but not not the same in context 2. Instead of the rigid owl:sameAs standard semantics, if an alternative semantics is considered where context is taken into account, then things can be the same or not depending on the context that applies. Moreover, within a common context there can still be divergences when, for example, there is not enough information for reaching a correct (indisputable) outcome. In other words, for the same identity link, multiple \u201ctruths\u201d (multiple possible interpretations) can co-exist once we agree on the context in which these \u201ctruths\u201d are cast. As a result, the Lenticular Lens tool allows for a single link to be both (i) established within a context and (ii) validated countless times, and then, by continuity, it allows for a collection of links (linkset or lens) to have various validations by different users. Even thought this is a desirable feature, before using the links for integration one needs to make a decision about their validity. This brings us to what we denote as Voting which is the merging of Validations sharing the same context. The Lenticular Lens tool provides five ways for merging validations. These include: Accepted Once , Rejected Once , Majority , Disputed , Weighted Experts. and Highest Ranking Experts . In the next section, we discuss the aforementioned options. 2.1 Consistent Validations \u00b6 This is the best scenario. A link has been validated several times with the same outcome, meaning not once a contradiction has occurred. The validation is consistent and therefore remains as it is. 2.2 Disputed Validations \u00b6 Disputes occur when a link has been validated several times but with contradicting truth statements (inconsistently). For example, when a links has been ACCEPTED three times but REJECTED twice. In this scenario, we follow a simple protocol: The link with contradicting validations is flagged as DISPUTED . The following options can be chosen for reaching an agreement: Majority. Consider the link as ACCEPTED if it has been accepted more times than being rejected, otherwise consider the link as REJECTED . Rejected Once. Consider the link as REJECTED if it has been rejected at least once. Accepted Once. Consider the link as ACCEPTED if it has been accepted at least once. Weighted Experts. In this situation, a weighted sum is performed within each voting group ( ACCEPTED vs REJECTED ). The group with the highest weighted sum gets to cast its vote. Highest Ranking Experts. Deciding on how to merge several validations on the basis of the authority with the highest rank is also possible. The decision to accept or reject the link is solely made by the highest ranking expert. It is easy to see here that there can be still disputes among highest ranked experts. In this case, the majority among experts applies. No matter the option, in the event of a tie , the link is REJECTED unless stated otherwise by the user. 3. Validation Support \u00b6 3.1 Individual Validation \u00b6 3.2 Group Validation \u00b6 3.2.1 Linkset \u00b6 3.2.2 Lens \u00b6 3.2.3 Cluster-based validation \u00b6 3.3 Visualisation \u00b6","title":"9. Link Validation"},{"location":"08.LinkValidation/#link-validation","text":"Work in progress Imagine a researcher in the need of integrating data on publications, authors and education institutions. Wrongly executing this integration task could mean, for example, assigning the h-index of author A to author B or wrongly saying that author A is affiliated to an institution he has never been involved with. Hardly any matching algorithm returns only perfect results. In light of this, to reach a solid investigation result, it is of importance to support human validation once links have been created and optionally improved.","title":"LINK VALIDATION"},{"location":"08.LinkValidation/#1-uncertainty-vs-vagueness","text":"Referring to propositions ( the sky is blue, the bottle is full, the tomato is ripe\u2026 ) in logic, the use of degree of (un)certainty, belief or confidence is to emphasise an evaluation of the assignment of a truth value to a proposition which may or may not qualify as vague ( \u201cthe tomato is ripe\u201d ). For example, how certain are we in saying that the proposition \u201cthe tomato is ripe\u201d is true ? As [Lukasiewicz2008] illustrates, saying that \u201cJohn is a teacher\u201d and \u201cJohn is a student\u201d with respectively 0.3 and 0.7 as degrees of certainty is roughly saying that \u201cJohn is either a teacher or a student, but more likely a student\u201d . However, the vague statement \u201c John is tall\u201d with the assignment of 0.9 as degree of truth can be roughly translated as \u201cJohn is quite tall\u201d . The transition from assigning boolean truth values {0, 1} to assigning continuous values in the unit interval [0, 1] to propositions (event) is clearly the change from classical logic to fuzzy logic which is in the modelling paradigm of many-valued Logics . The latter truth space (unit interval) is motivated by the presence of vague concepts in a proposition, making it hard or sometimes even impossible to establish whether the proposition is completely true or false [ Lukasiewicz2008 ]. Now, whatever truth value is assigned to an event, sometimes, one wonders about the chance that the even will happen (Probability) or might happen (Possibility)[ Kovalerchuk2017 ]. Degrees of Truth is not to be confused with the Degrees of Certainty / Confidence as the latter is not an assignment of truth value as opposed to the former, but rather an evaluation of a weighted proposition (proposition with an assigned truth value) regardless of its truth value space ({0, 1} or [0, 1]).","title":"1. Uncertainty vs. Vagueness"},{"location":"08.LinkValidation/#1-validation-and-links-context","text":"intrinsic properties the purpose or task for which the links are used.","title":"1. Validation and Links' Context"},{"location":"08.LinkValidation/#2-voting-strategies","text":"In the Semantic Web, the standard semantic OWL/DL interpretation of identity between two resources entails full equality , i.e. they are necessarily the same and share all their properties. Such semantic applies independently of context and therefore no validation or dispute apply since that semantic does not take any of it into account: things are either the same or they are not. [ Idrissou 2017 ] In real life problems, the equality between resources may depend (i) not only on their intrinsic properties (ii) but also on the purpose or task for which they are used. For example, an organisation A and B are the same in context 1 but not not the same in context 2. Instead of the rigid owl:sameAs standard semantics, if an alternative semantics is considered where context is taken into account, then things can be the same or not depending on the context that applies. Moreover, within a common context there can still be divergences when, for example, there is not enough information for reaching a correct (indisputable) outcome. In other words, for the same identity link, multiple \u201ctruths\u201d (multiple possible interpretations) can co-exist once we agree on the context in which these \u201ctruths\u201d are cast. As a result, the Lenticular Lens tool allows for a single link to be both (i) established within a context and (ii) validated countless times, and then, by continuity, it allows for a collection of links (linkset or lens) to have various validations by different users. Even thought this is a desirable feature, before using the links for integration one needs to make a decision about their validity. This brings us to what we denote as Voting which is the merging of Validations sharing the same context. The Lenticular Lens tool provides five ways for merging validations. These include: Accepted Once , Rejected Once , Majority , Disputed , Weighted Experts. and Highest Ranking Experts . In the next section, we discuss the aforementioned options.","title":"2. Voting Strategies"},{"location":"08.LinkValidation/#21-consistent-validations","text":"This is the best scenario. A link has been validated several times with the same outcome, meaning not once a contradiction has occurred. The validation is consistent and therefore remains as it is.","title":"2.1 Consistent Validations"},{"location":"08.LinkValidation/#22-disputed-validations","text":"Disputes occur when a link has been validated several times but with contradicting truth statements (inconsistently). For example, when a links has been ACCEPTED three times but REJECTED twice. In this scenario, we follow a simple protocol: The link with contradicting validations is flagged as DISPUTED . The following options can be chosen for reaching an agreement: Majority. Consider the link as ACCEPTED if it has been accepted more times than being rejected, otherwise consider the link as REJECTED . Rejected Once. Consider the link as REJECTED if it has been rejected at least once. Accepted Once. Consider the link as ACCEPTED if it has been accepted at least once. Weighted Experts. In this situation, a weighted sum is performed within each voting group ( ACCEPTED vs REJECTED ). The group with the highest weighted sum gets to cast its vote. Highest Ranking Experts. Deciding on how to merge several validations on the basis of the authority with the highest rank is also possible. The decision to accept or reject the link is solely made by the highest ranking expert. It is easy to see here that there can be still disputes among highest ranked experts. In this case, the majority among experts applies. No matter the option, in the event of a tie , the link is REJECTED unless stated otherwise by the user.","title":"2.2 Disputed Validations"},{"location":"08.LinkValidation/#3-validation-support","text":"","title":"3. Validation Support"},{"location":"08.LinkValidation/#31-individual-validation","text":"","title":"3.1 Individual Validation"},{"location":"08.LinkValidation/#32-group-validation","text":"","title":"3.2 Group Validation"},{"location":"08.LinkValidation/#321-linkset","text":"","title":"3.2.1 Linkset"},{"location":"08.LinkValidation/#322-lens","text":"","title":"3.2.2 Lens"},{"location":"08.LinkValidation/#323-cluster-based-validation","text":"","title":"3.2.3 Cluster-based validation"},{"location":"08.LinkValidation/#33-visualisation","text":"","title":"3.3 Visualisation"},{"location":"09.LinkExport/","text":".katex img { display: block; position: absolute; width: 100%; height: inherit; } LINK EXPORT \u00b6 Work in progress At this point, linksets and/or Lenses have already been created and possibly validated. Now, new interesting questions come in mind: Could these links be exported for external usage? In what format are there available for export? Can metadata be exported in combination with the links? Can the linktype be modified prior to an export? All these questions will be answered here. 1. Link Metadata Structure \u00b6 The Lenticular Lens design imposes to itself and motivates its users to provide as much explicit settings as possible in support for describing why (clarity) and how (reproducibility) a set of links has been generated. Links metadata in the tool is organised into Generic and Specific metadata. Indeed, all links residing in the Lenticular Lens can be exported. However, prior to doing that, one has to choose whether the metadata is to be included or not. If it is to be included, further directives are needed as to whether it should include Generic and/or Specific metadata. Specific metadata is the annotation that applies to a single link. Generic metadata is the annotation that applies to a collection of links as a whole. 1.1 Specific Metadata \u00b6 Example-1 presents in a turtle format, a standard identity set of nine links where equivalent resources are linked with the well known linktype owl:sameAs . As shown in this example, the links are meant to reside in the default graph of a triplestore as no named graph is explicitly associated to them. Example 1: Unnamed identity-set. hero: BlackWidow owl: sameAs person: Nat . hero: BlackWidow owl: sameAs person: Romanoff . hero: BlackWidow owl: sameAs person: Natalia-Romanova . hero: BlackWidow owl: sameAs person: Natasha . hero: Spiderman owl: sameAs person: Peter-parker . hero: Spiderman owl: sameAs person: Tom-Holland . hero: Superman owl: sameAs person: Clark-Kent . hero: Superman owl: sameAs person: Joseph . hero: Superman owl: sameAs person: Kal-El . In Example-2, the triples of Example-1 are now presented with specific annotations . With this type of annotation, one can for example specify the confidence strength of each of the nine triples. In new example, seven out of the nine links are now annotated with a validation statement. The triple hero:Spiderman owl:sameAs person:Tom-Holland for example is the only triple annotated as a non valid statement followed by the rational supporting its rejection. Another added value beside being able to make new statements about a link is that, the annotation itself can be used as a way of filtering links. For example, one can select for example, the 8 validated triples or the 7 triples validated as \u201ctrue\u201d. Example 2: Annotation of individual links. << hero: BlackWidow owl: sameAs person: Nat >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Romanoff >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Natalia-Romanova >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Natasha >> validation: status \"True\" . << hero: Spiderman owl: sameAs person: Peter-parker >> validation: status \"True\" . << hero: Spiderman owl: sameAs person: Tom-Holland >> validation: status \"False\" . validation: Rational \"Tom-Holland does not have Spiderman properties which Peter-parker (fictitious) a.k.a Spiderman has.\" . << hero: Superman owl: sameAs person: Clark-Kent >> validation: status \"True\" . << hero: Superman owl: sameAs person: Joseph >> validation: status \"True\" . hero: Superman owl: sameAs person: Kal-El . 1.2 Generic Metadata \u00b6 The set of links illustrated in Example-1 as triples populating the default graph can now be referred to as ex:heroes-Identity in Example-3 as they have now been grouped in this named-graph. As illustrated in Example-4, this named-graph IRI can now be used to document any generic information deemed relevant such as the source and target datasets, the aligning method\u2026 As opposed to Example-1 and Example-2, Example-3 highlights that in need of generic metadata , links in the spotlight need a referent to be annotated. In other words, there is no need to export the links in a named graph (.trig extension) when meta data is not required by the user . However, there is a grate advantage when need is to gather all links at once. Example 3: Named identity-set. . ex: heroes-Identity { hero: BlackWidow owl: sameAs person: Nat . hero: BlackWidow owl: sameAs person: Romanoff . hero: BlackWidow owl: sameAs person: Natalia-Romanova . hero: BlackWidow owl: sameAs person: Natasha . hero: Spiderman owl: sameAs person: Peter-parker . hero: Spiderman owl: sameAs person: Tom-Holland . hero: Superman owl: sameAs person: Clark-Kent . hero: Superman owl: sameAs person: Joseph . hero: Superman owl: sameAs person: Kal-El . } Example-4: Annotation of a set of links in the Lenticular Lens. Generic Metadata. The linkset ex:heroes-Identity is presented with both generic and specific metadata. The generic metadata in the default graph (triple without a specific named graph) conveys information about the identity set: ex:heroes-Identity . This information includes among other, the type, subjects, license, description, format, number of triples (9), number of distinct entities (12), number of identity clusters (3)\u2026 As stated in the generic metadata of ex:heroes-Identity , the linkset contains 9 triples. Of these triples, the metadata informs us that 7 are validated as accepted, one as rejected and another one as remains (not validated). ex: heroes-Identity a void: Linkset ; dcterms: description \"Identifying Marvel's superheroes\" ; dcterms: license law: odc-public-domain-dedication-and-licence ; dcterms: subject <http://example.org/resource/Person> ; dcterms: subject <http://example.org/resource/Hero> ; void: subjectsTarget dataset: Fictive-Persons ; void: objectsTarget dataset: Superheroes ; void: feature format: Turtle ; void: linkPredicate owl: sameAs ; void: triples 9 ; void: entities 12 ; void: distinctSubjects 3 ; void: distinctObjects 9 ; voidPlus: clusters 3 ; validation: count 8 ; validation: accepted 7 ; validation: rejected 1 ; validation: remains 1 ; 1.3 Complete Metadata \u00b6 Example-3 illustrates the complete structure of linksets and lenses in the Lenticular Lens . Here, by default, a linkset/lens is annotated with both, generic and specific metadata. Example 5: Complete metadata in RDF* turtle syntax annotation. ex: heroes-Identity a void: Linkset ; dcterms: description \"Identifying Marvel's superheroes\" ; dcterms: license law: odc-public-domain-dedication-and-licence ; dcterms: subject <http://example.org/resource/Person> ; dcterms: subject <http://example.org/resource/Hero> ; void: subjectsTarget dataset: Fictive-Persons ; void: objectsTarget dataset: Superheroes ; void: feature format: Turtle ; void: linkPredicate owl: sameAs ; void: triples 9 ; void: entities 12 ; void: distinctSubjects 3 ; void: distinctObjects 9 ; voidPlus: clusters 3 ; validation: count 8 ; validation: accepted 7 ; validation: rejected 1 ; validation: remains 1 ; ex: heroes-Identity { << hero: BlackWidow owl: sameAs person: Nat >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Romanoff >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Natalia-Romanova >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Natasha >> validation: status \"True\" . << hero: Spiderman owl: sameAs person: Peter-parker >> validation: status \"True\" . << hero: Spiderman owl: sameAs person: Tom-Holland >> validation: status \"False\" . validation: Rational \"Tom-Holland does not have Spiderman properties which Peter-parker (fictitious) a.k.a Spiderman has.\" . << hero: Superman owl: sameAs person: Clark-Kent >> validation: status \"True\" . << hero: Superman owl: sameAs person: Joseph >> validation: status \"True\" . hero: Superman owl: sameAs person: Kal-El . } Separation of Concern. The linkset and lens presentation enable separation of concern in the sense that it provides the user with a number of options: Flat Representation: No reified triples and optionally within a named graph. Partial Representation: Choice of exclusively including either the generic or the specific metadata. Full Representation: as intended in the Lenticular Lens , set of links comes along with its generic and specific metadata if applicable. 2. RDF Link Reifications \u00b6 The verb reify is defined in Lexico , the Oxford supported dictionary, as a way to make (something abstract) more concrete or real . In RDF \u2013 a data model that allows for the description of a resource (subject position of a triple) in the form \u27e8 subject predicate object. \u27e9 \\lang \\text{subject predicate object.} \\rang \u27e8 subject predicate object. \u27e9 \u2013, reification offers means to define a triple as a resource as such that the triple could be described . Several reification approaches exist: N-ary relations, RDF reification, Rdfstar and Singleton properties. In the next subsections we briefly describe them. 2.1 N-ary relations \u00b6 The [ N-ary relations ] syntax provides a means to model a non binary relation, a relation that holds among more than two objects, as an object itself. Example 6: The Purchase relation into an N-ary relations. : Purchase_1 a : Purchase ; : has_buyer : John ; : has_object : Lenny_The_Lion ; : has_purpose : Birthday_Gift ; : has_amount 15 ; : has_seller : books . example . com . 2.2 Standard reification \u00b6 RDF reification is the standard reification proposed by W3C . As depicted in Example-6, it introduces a new resource of type rdf:Statement and three predicates to identify the rdf:subject, rdf:predicate, and rdf:object of the reified triple. Example 7: Standard RDF reification. ### BlackWidow Triple. hero: BlackWidow owl: sameAs person: Nat . ### Standard RDF reification of BlackWidow Triple ex: reification-1 rdf: type rdf: Statement ; rdf: subject hero: BlackWidow ; rdf: predicate owl: sameAs ; rdf: object person: Nat ; validation: status \"True\" . 2.3 Singleton \u00b6 The [ Singleton properties ] approach offers the latitude to make uniquely identifiable the predicate of the triple to reified where the new property is an rdf:singletonPropertyOf a more generic property Example 8: Singleton. ### BlackWidow Triple. hero: BlackWidow owl: sameAs person: Nat . ### Modification of BlackWidow Triple. hero: BlackWidow singleton: sameAs-1 person: Nat . ### Singleton reification of BlackWidow Triple. singleton: sameAs-1 rdf: type rdf: SingletonProperty ; rdf: singletonPropertyOf owl: sameAs . validation: status \"True\" . 2.4 RDFstar \u00b6 RDF * : An in-line simple reification syntax that requires RDF * and SPARQL * . Example 9: RDFstar / RDF * . ### BlackWidow Triple. hero: BlackWidow owl: sameAs person: Nat . ### RDFstar reification BlackWidow Triple. << hero: BlackWidow owl: sameAs person: Nat >> validation: status \"True\" . 3. Export Formats \u00b6 The Lenticular Lens offers a variety of four file formats for exporting a collection of links. These are: CSV (Example-10), JSON-LD (Example-11), RDF Turtle (Example-12), and RDF* Turtle (Example-13). 3.1 CSV file format \u00b6 In our attend to export a linkset/lens with its complete metadata, in a CSV format, the Lenticular Lens generates two CSV tables. As available in Example-9, the first table is an illustration of a linkset generic metadata while the second table is an illustration of links with specific metadata (annotated triples). Example 10: Linkset in a CSV format. Keep in mind that the table below are a visual representation of a CSV table -------------------- -- METADATA TABLE -- -------------------- --------------------------------------------------------------------------------------------------------- | Vocabulary Value | --------------------------------------------------------------------------------------------------------- http : //www . w 3 . org/ns/sparql-service-description #namedGraph http://example.org/#heroesIdentity http : //www . w 3 . org/ 1999 / 02 / 22 -rdf-syntax-ns #type http://rdfs.org/ns/void#Linkset http : //purl . org/dc/terms/description Identifying Marvel 's superheroes http : //purl . org/dc/terms/license law: odc-public-domain-dedication-and-licence http : //purl . org/dc/terms/subject http : //example . org/resource/Person http : //purl . org/dc/terms/subject http : //example . org/resource/Hero http : //rdfs . org/ns/void #subjectsTarget dataset:Fictive-Persons http : //rdfs . org/ns/void #objectsTarget dataset:Superheroes http : //rdfs . org/ns/void #feature format:Turtle http : //rdfs . org/ns/void #linkPredicate owl:sameAs http : //rdfs . org/ns/void #triples 9 http : //rdfs . org/ns/void #entities 12 http : //rdfs . org/ns/void #distinctSubjects 3 http : //rdfs . org/ns/void #distinctObjects 9 http : //vocabulary/voidPlus #clusters 3 http : //vocabulary/validation #count 8 http : //vocabulary/validation #accepted 7 http : //vocabulary/validation #rejected 1 http : //vocabulary/validation #remains 1 ------------------------ -- LINKSET/LENS TABLE -- ------------------------ ------------------------------------------------------------------------------------------------------------------------------------------------ | NamedGraph Source Target ValStatus ValRational | ------------------------------------------------------------------------------------------------------------------------------------------------ http : //example . org/ #heroesIdentity http://example.org/hero#BlackWidow http://example.org/person#Nat True http : //example . org/ #heroesIdentity http://example.org/hero#BlackWidow http://example.org/person#Romanoff True http : //example . org/ #heroesIdentity http://example.org/hero#BlackWidow http://example.org/person#Natalia-Romanova True http : //example . org/ #heroesIdentity http://example.org/hero#BlackWidow http://example.org/person#Natasha True http : //example . org/ #heroesIdentity http://example.org/hero#Spiderman http://example.org/person#Peter-parker True http : //example . org/ #heroesIdentity http://example.org/hero#Spiderman http://example.org/person#Tom-Holland False Tom-Holland does not have Spiderman properties which Peter-parker (fictitious) a.k.a Spiderman has. http : //example . org/ #heroesIdentity http://example.org/hero#Superman http://example.org/person#Clark-Kent True http : //example . org/ #heroesIdentity http://example.org/hero#Superman http://example.org/person#Joseph True http : //example . org/ #heroesIdentity http://example.org/hero#Superman http://example.org/person#Kal-El True 3.2 JSON-LD file format \u00b6 JSON-LD OUTPUT. TODO.... Example 11 Linkset in a JSON-LD format.. { \"@context\" : { \"name\" : \"http://xmlns.com/foaf/0.1/name\" , \"homepage\" : { \"@id\" : \"http://xmlns.com/foaf/0.1/workplaceHomepage\" , \"@type\" : \"@id\" }, \"Person\" : \"http://xmlns.com/foaf/0.1/Person\" }, \"@id\" : \"https://me.example.com\" , \"@type\" : \"Person\" , \"name\" : \"John Smith\" , \"homepage\" : \"https://www.example.com/\" } 3.3 RDF file formats \u00b6 Knowing the right RDF file format in which to export a collection of links depends on a number of parameters. Depending on whether the collection of links is unnamed , named and/or annotated , one can respectively have the export output in Turtle or Trig . A Trig file implies a named graph but it says little about the reification approach taken for annotating individual triples or the graph of collection. In the event of annotation, one can choose whether to go for an RDF standard reification , singleton properties or RDFstar . In this pile of options, our choice goes for always having a named-graph reified using the RDFstar syntax if one has RDF * and SPAQRL * in place. If not, our second option goes for singletons . Example 12: n Linkset in an RDF Turtle format using singletons. Our preference goes to an RDF* Turtle Output as illustrated in Example-6. However, because RDF* is not deployed on all triple stores, we provide the singleton, and RDF standard reification representation alternatives as well. An example of singletons in available here. ex: heroes-Identity a void: Linkset ; dcterms: description \"Identifying Marvel's heroes\" ; dcterms: license law: odc-public-domain-dedication-and-licence ; dcterms: subject <http://example.org/resource/Person> ; dcterms: subject <http://example.org/resource/Hero> ; void: subjectsTarget dataset: Fictive-Persons ; void: objectsTarget dataset: Superheroes ; void: feature format: Turtle ; void: linkPredicate owl: sameAs ; void: triples 9 ; void: entities 12 ; void: distinctSubjects 3 ; void: distinctObjects 9 ; voidPlus: clusters 3 ; validation: count 8 ; validation: accepted 7 ; validation: rejected 1 ; validation: remains 1 ; ex: heroes-Identity { hero: BlackWidow singleton: sameAs-1 person: Nat . hero: BlackWidow singleton: sameAs-2 person: Romanoff . hero: BlackWidow singleton: sameAs-3 person: Natalia-Romanova . hero: BlackWidow singleton: sameAs-4 person: Natasha . hero: Spiderman singleton: sameAs-5 person: Peter-parker . hero: Spiderman singleton: sameAs-6 person: Tom-Holland . hero: Superman singleton: sameAs-7 person: Clark-Kent . hero: Superman singleton: sameAs-8 person: Joseph . hero: Superman singleton: sameAs-9 person: Kal-El . } ex: heroes-Identity-singletons { singleton: sameAs-1 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-2 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-3 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-4 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-5 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-1 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-1 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . hero: Superman owl: sameAs person: Kal-El . singleton: sameAs-6 rdf: subPropertyOf owl: sameAs ; validation: status \"False\" ; validation: Rational \"Tom-Holland does not have Spiderman properties which Peter-parker (fictitious) a.k.a Spiderman has.\" . } 4. Link Restrictions \u00b6 Whenever links are annotated, the Lenticular Lens offers a way to take advantage of it. It provides the user with options to filter out links of no interest, an alternative to the inconvenience of having to download all links when not needed. Example-12 lists all possible link restrictions options. For example to make sure that only accepted links are exported, the options Accepted is chosen in Example-12. Example 13: Link Restrictions Options. All : Export all links Accepted : Export only accepted links. Rejected : Export only rejected links. Validated : Export rejected or validated links Not Validated : Export links with no accepted or rejected annotation. Threshold : Export links that pass a predefined threshold condition. This feature applies to links annotated with confidence values. 5. Default Export Table \u00b6 Example-14 summarises the various options for exporting a linkset or lens. In each set of selection category, a default selection is checked in. This means that, by default, an exported linkset or lens comes along with all its links and its complete annotation in a CSV format . Example 13: Export Table. METADATA STRUCTURE RDF REIFICATION SYNTAX LINK EXPORT FORMAT LINK RESTRICTIONS Partial:Generic Standard RDF reification RDF Turtle-trig All Partial:Specific RDFstar RDF Turtle Accepted Singleton JSON-LD Rejected CSV Validated Not Validated Threshold","title":"10. Link Export"},{"location":"09.LinkExport/#link-export","text":"Work in progress At this point, linksets and/or Lenses have already been created and possibly validated. Now, new interesting questions come in mind: Could these links be exported for external usage? In what format are there available for export? Can metadata be exported in combination with the links? Can the linktype be modified prior to an export? All these questions will be answered here.","title":"LINK EXPORT"},{"location":"09.LinkExport/#1-link-metadata-structure","text":"The Lenticular Lens design imposes to itself and motivates its users to provide as much explicit settings as possible in support for describing why (clarity) and how (reproducibility) a set of links has been generated. Links metadata in the tool is organised into Generic and Specific metadata. Indeed, all links residing in the Lenticular Lens can be exported. However, prior to doing that, one has to choose whether the metadata is to be included or not. If it is to be included, further directives are needed as to whether it should include Generic and/or Specific metadata. Specific metadata is the annotation that applies to a single link. Generic metadata is the annotation that applies to a collection of links as a whole.","title":"1. Link Metadata Structure"},{"location":"09.LinkExport/#11-specific-metadata","text":"Example-1 presents in a turtle format, a standard identity set of nine links where equivalent resources are linked with the well known linktype owl:sameAs . As shown in this example, the links are meant to reside in the default graph of a triplestore as no named graph is explicitly associated to them. Example 1: Unnamed identity-set. hero: BlackWidow owl: sameAs person: Nat . hero: BlackWidow owl: sameAs person: Romanoff . hero: BlackWidow owl: sameAs person: Natalia-Romanova . hero: BlackWidow owl: sameAs person: Natasha . hero: Spiderman owl: sameAs person: Peter-parker . hero: Spiderman owl: sameAs person: Tom-Holland . hero: Superman owl: sameAs person: Clark-Kent . hero: Superman owl: sameAs person: Joseph . hero: Superman owl: sameAs person: Kal-El . In Example-2, the triples of Example-1 are now presented with specific annotations . With this type of annotation, one can for example specify the confidence strength of each of the nine triples. In new example, seven out of the nine links are now annotated with a validation statement. The triple hero:Spiderman owl:sameAs person:Tom-Holland for example is the only triple annotated as a non valid statement followed by the rational supporting its rejection. Another added value beside being able to make new statements about a link is that, the annotation itself can be used as a way of filtering links. For example, one can select for example, the 8 validated triples or the 7 triples validated as \u201ctrue\u201d. Example 2: Annotation of individual links. << hero: BlackWidow owl: sameAs person: Nat >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Romanoff >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Natalia-Romanova >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Natasha >> validation: status \"True\" . << hero: Spiderman owl: sameAs person: Peter-parker >> validation: status \"True\" . << hero: Spiderman owl: sameAs person: Tom-Holland >> validation: status \"False\" . validation: Rational \"Tom-Holland does not have Spiderman properties which Peter-parker (fictitious) a.k.a Spiderman has.\" . << hero: Superman owl: sameAs person: Clark-Kent >> validation: status \"True\" . << hero: Superman owl: sameAs person: Joseph >> validation: status \"True\" . hero: Superman owl: sameAs person: Kal-El .","title":"1.1 Specific Metadata"},{"location":"09.LinkExport/#12-generic-metadata","text":"The set of links illustrated in Example-1 as triples populating the default graph can now be referred to as ex:heroes-Identity in Example-3 as they have now been grouped in this named-graph. As illustrated in Example-4, this named-graph IRI can now be used to document any generic information deemed relevant such as the source and target datasets, the aligning method\u2026 As opposed to Example-1 and Example-2, Example-3 highlights that in need of generic metadata , links in the spotlight need a referent to be annotated. In other words, there is no need to export the links in a named graph (.trig extension) when meta data is not required by the user . However, there is a grate advantage when need is to gather all links at once. Example 3: Named identity-set. . ex: heroes-Identity { hero: BlackWidow owl: sameAs person: Nat . hero: BlackWidow owl: sameAs person: Romanoff . hero: BlackWidow owl: sameAs person: Natalia-Romanova . hero: BlackWidow owl: sameAs person: Natasha . hero: Spiderman owl: sameAs person: Peter-parker . hero: Spiderman owl: sameAs person: Tom-Holland . hero: Superman owl: sameAs person: Clark-Kent . hero: Superman owl: sameAs person: Joseph . hero: Superman owl: sameAs person: Kal-El . } Example-4: Annotation of a set of links in the Lenticular Lens. Generic Metadata. The linkset ex:heroes-Identity is presented with both generic and specific metadata. The generic metadata in the default graph (triple without a specific named graph) conveys information about the identity set: ex:heroes-Identity . This information includes among other, the type, subjects, license, description, format, number of triples (9), number of distinct entities (12), number of identity clusters (3)\u2026 As stated in the generic metadata of ex:heroes-Identity , the linkset contains 9 triples. Of these triples, the metadata informs us that 7 are validated as accepted, one as rejected and another one as remains (not validated). ex: heroes-Identity a void: Linkset ; dcterms: description \"Identifying Marvel's superheroes\" ; dcterms: license law: odc-public-domain-dedication-and-licence ; dcterms: subject <http://example.org/resource/Person> ; dcterms: subject <http://example.org/resource/Hero> ; void: subjectsTarget dataset: Fictive-Persons ; void: objectsTarget dataset: Superheroes ; void: feature format: Turtle ; void: linkPredicate owl: sameAs ; void: triples 9 ; void: entities 12 ; void: distinctSubjects 3 ; void: distinctObjects 9 ; voidPlus: clusters 3 ; validation: count 8 ; validation: accepted 7 ; validation: rejected 1 ; validation: remains 1 ;","title":"1.2 Generic Metadata"},{"location":"09.LinkExport/#13-complete-metadata","text":"Example-3 illustrates the complete structure of linksets and lenses in the Lenticular Lens . Here, by default, a linkset/lens is annotated with both, generic and specific metadata. Example 5: Complete metadata in RDF* turtle syntax annotation. ex: heroes-Identity a void: Linkset ; dcterms: description \"Identifying Marvel's superheroes\" ; dcterms: license law: odc-public-domain-dedication-and-licence ; dcterms: subject <http://example.org/resource/Person> ; dcterms: subject <http://example.org/resource/Hero> ; void: subjectsTarget dataset: Fictive-Persons ; void: objectsTarget dataset: Superheroes ; void: feature format: Turtle ; void: linkPredicate owl: sameAs ; void: triples 9 ; void: entities 12 ; void: distinctSubjects 3 ; void: distinctObjects 9 ; voidPlus: clusters 3 ; validation: count 8 ; validation: accepted 7 ; validation: rejected 1 ; validation: remains 1 ; ex: heroes-Identity { << hero: BlackWidow owl: sameAs person: Nat >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Romanoff >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Natalia-Romanova >> validation: status \"True\" . << hero: BlackWidow owl: sameAs person: Natasha >> validation: status \"True\" . << hero: Spiderman owl: sameAs person: Peter-parker >> validation: status \"True\" . << hero: Spiderman owl: sameAs person: Tom-Holland >> validation: status \"False\" . validation: Rational \"Tom-Holland does not have Spiderman properties which Peter-parker (fictitious) a.k.a Spiderman has.\" . << hero: Superman owl: sameAs person: Clark-Kent >> validation: status \"True\" . << hero: Superman owl: sameAs person: Joseph >> validation: status \"True\" . hero: Superman owl: sameAs person: Kal-El . } Separation of Concern. The linkset and lens presentation enable separation of concern in the sense that it provides the user with a number of options: Flat Representation: No reified triples and optionally within a named graph. Partial Representation: Choice of exclusively including either the generic or the specific metadata. Full Representation: as intended in the Lenticular Lens , set of links comes along with its generic and specific metadata if applicable.","title":"1.3 Complete Metadata"},{"location":"09.LinkExport/#2-rdf-link-reifications","text":"The verb reify is defined in Lexico , the Oxford supported dictionary, as a way to make (something abstract) more concrete or real . In RDF \u2013 a data model that allows for the description of a resource (subject position of a triple) in the form \u27e8 subject predicate object. \u27e9 \\lang \\text{subject predicate object.} \\rang \u27e8 subject predicate object. \u27e9 \u2013, reification offers means to define a triple as a resource as such that the triple could be described . Several reification approaches exist: N-ary relations, RDF reification, Rdfstar and Singleton properties. In the next subsections we briefly describe them.","title":"2. RDF Link Reifications"},{"location":"09.LinkExport/#21-n-ary-relations","text":"The [ N-ary relations ] syntax provides a means to model a non binary relation, a relation that holds among more than two objects, as an object itself. Example 6: The Purchase relation into an N-ary relations. : Purchase_1 a : Purchase ; : has_buyer : John ; : has_object : Lenny_The_Lion ; : has_purpose : Birthday_Gift ; : has_amount 15 ; : has_seller : books . example . com .","title":"2.1 N-ary relations"},{"location":"09.LinkExport/#22-standard-reification","text":"RDF reification is the standard reification proposed by W3C . As depicted in Example-6, it introduces a new resource of type rdf:Statement and three predicates to identify the rdf:subject, rdf:predicate, and rdf:object of the reified triple. Example 7: Standard RDF reification. ### BlackWidow Triple. hero: BlackWidow owl: sameAs person: Nat . ### Standard RDF reification of BlackWidow Triple ex: reification-1 rdf: type rdf: Statement ; rdf: subject hero: BlackWidow ; rdf: predicate owl: sameAs ; rdf: object person: Nat ; validation: status \"True\" .","title":"2.2 Standard reification"},{"location":"09.LinkExport/#23-singleton","text":"The [ Singleton properties ] approach offers the latitude to make uniquely identifiable the predicate of the triple to reified where the new property is an rdf:singletonPropertyOf a more generic property Example 8: Singleton. ### BlackWidow Triple. hero: BlackWidow owl: sameAs person: Nat . ### Modification of BlackWidow Triple. hero: BlackWidow singleton: sameAs-1 person: Nat . ### Singleton reification of BlackWidow Triple. singleton: sameAs-1 rdf: type rdf: SingletonProperty ; rdf: singletonPropertyOf owl: sameAs . validation: status \"True\" .","title":"2.3 Singleton"},{"location":"09.LinkExport/#24-rdfstar","text":"RDF * : An in-line simple reification syntax that requires RDF * and SPARQL * . Example 9: RDFstar / RDF * . ### BlackWidow Triple. hero: BlackWidow owl: sameAs person: Nat . ### RDFstar reification BlackWidow Triple. << hero: BlackWidow owl: sameAs person: Nat >> validation: status \"True\" .","title":"2.4 RDFstar"},{"location":"09.LinkExport/#3-export-formats","text":"The Lenticular Lens offers a variety of four file formats for exporting a collection of links. These are: CSV (Example-10), JSON-LD (Example-11), RDF Turtle (Example-12), and RDF* Turtle (Example-13).","title":"3. Export Formats"},{"location":"09.LinkExport/#31-csv-file-format","text":"In our attend to export a linkset/lens with its complete metadata, in a CSV format, the Lenticular Lens generates two CSV tables. As available in Example-9, the first table is an illustration of a linkset generic metadata while the second table is an illustration of links with specific metadata (annotated triples). Example 10: Linkset in a CSV format. Keep in mind that the table below are a visual representation of a CSV table -------------------- -- METADATA TABLE -- -------------------- --------------------------------------------------------------------------------------------------------- | Vocabulary Value | --------------------------------------------------------------------------------------------------------- http : //www . w 3 . org/ns/sparql-service-description #namedGraph http://example.org/#heroesIdentity http : //www . w 3 . org/ 1999 / 02 / 22 -rdf-syntax-ns #type http://rdfs.org/ns/void#Linkset http : //purl . org/dc/terms/description Identifying Marvel 's superheroes http : //purl . org/dc/terms/license law: odc-public-domain-dedication-and-licence http : //purl . org/dc/terms/subject http : //example . org/resource/Person http : //purl . org/dc/terms/subject http : //example . org/resource/Hero http : //rdfs . org/ns/void #subjectsTarget dataset:Fictive-Persons http : //rdfs . org/ns/void #objectsTarget dataset:Superheroes http : //rdfs . org/ns/void #feature format:Turtle http : //rdfs . org/ns/void #linkPredicate owl:sameAs http : //rdfs . org/ns/void #triples 9 http : //rdfs . org/ns/void #entities 12 http : //rdfs . org/ns/void #distinctSubjects 3 http : //rdfs . org/ns/void #distinctObjects 9 http : //vocabulary/voidPlus #clusters 3 http : //vocabulary/validation #count 8 http : //vocabulary/validation #accepted 7 http : //vocabulary/validation #rejected 1 http : //vocabulary/validation #remains 1 ------------------------ -- LINKSET/LENS TABLE -- ------------------------ ------------------------------------------------------------------------------------------------------------------------------------------------ | NamedGraph Source Target ValStatus ValRational | ------------------------------------------------------------------------------------------------------------------------------------------------ http : //example . org/ #heroesIdentity http://example.org/hero#BlackWidow http://example.org/person#Nat True http : //example . org/ #heroesIdentity http://example.org/hero#BlackWidow http://example.org/person#Romanoff True http : //example . org/ #heroesIdentity http://example.org/hero#BlackWidow http://example.org/person#Natalia-Romanova True http : //example . org/ #heroesIdentity http://example.org/hero#BlackWidow http://example.org/person#Natasha True http : //example . org/ #heroesIdentity http://example.org/hero#Spiderman http://example.org/person#Peter-parker True http : //example . org/ #heroesIdentity http://example.org/hero#Spiderman http://example.org/person#Tom-Holland False Tom-Holland does not have Spiderman properties which Peter-parker (fictitious) a.k.a Spiderman has. http : //example . org/ #heroesIdentity http://example.org/hero#Superman http://example.org/person#Clark-Kent True http : //example . org/ #heroesIdentity http://example.org/hero#Superman http://example.org/person#Joseph True http : //example . org/ #heroesIdentity http://example.org/hero#Superman http://example.org/person#Kal-El True","title":"3.1 CSV file format"},{"location":"09.LinkExport/#32-json-ld-file-format","text":"JSON-LD OUTPUT. TODO.... Example 11 Linkset in a JSON-LD format.. { \"@context\" : { \"name\" : \"http://xmlns.com/foaf/0.1/name\" , \"homepage\" : { \"@id\" : \"http://xmlns.com/foaf/0.1/workplaceHomepage\" , \"@type\" : \"@id\" }, \"Person\" : \"http://xmlns.com/foaf/0.1/Person\" }, \"@id\" : \"https://me.example.com\" , \"@type\" : \"Person\" , \"name\" : \"John Smith\" , \"homepage\" : \"https://www.example.com/\" }","title":"3.2 JSON-LD file format"},{"location":"09.LinkExport/#33-rdf-file-formats","text":"Knowing the right RDF file format in which to export a collection of links depends on a number of parameters. Depending on whether the collection of links is unnamed , named and/or annotated , one can respectively have the export output in Turtle or Trig . A Trig file implies a named graph but it says little about the reification approach taken for annotating individual triples or the graph of collection. In the event of annotation, one can choose whether to go for an RDF standard reification , singleton properties or RDFstar . In this pile of options, our choice goes for always having a named-graph reified using the RDFstar syntax if one has RDF * and SPAQRL * in place. If not, our second option goes for singletons . Example 12: n Linkset in an RDF Turtle format using singletons. Our preference goes to an RDF* Turtle Output as illustrated in Example-6. However, because RDF* is not deployed on all triple stores, we provide the singleton, and RDF standard reification representation alternatives as well. An example of singletons in available here. ex: heroes-Identity a void: Linkset ; dcterms: description \"Identifying Marvel's heroes\" ; dcterms: license law: odc-public-domain-dedication-and-licence ; dcterms: subject <http://example.org/resource/Person> ; dcterms: subject <http://example.org/resource/Hero> ; void: subjectsTarget dataset: Fictive-Persons ; void: objectsTarget dataset: Superheroes ; void: feature format: Turtle ; void: linkPredicate owl: sameAs ; void: triples 9 ; void: entities 12 ; void: distinctSubjects 3 ; void: distinctObjects 9 ; voidPlus: clusters 3 ; validation: count 8 ; validation: accepted 7 ; validation: rejected 1 ; validation: remains 1 ; ex: heroes-Identity { hero: BlackWidow singleton: sameAs-1 person: Nat . hero: BlackWidow singleton: sameAs-2 person: Romanoff . hero: BlackWidow singleton: sameAs-3 person: Natalia-Romanova . hero: BlackWidow singleton: sameAs-4 person: Natasha . hero: Spiderman singleton: sameAs-5 person: Peter-parker . hero: Spiderman singleton: sameAs-6 person: Tom-Holland . hero: Superman singleton: sameAs-7 person: Clark-Kent . hero: Superman singleton: sameAs-8 person: Joseph . hero: Superman singleton: sameAs-9 person: Kal-El . } ex: heroes-Identity-singletons { singleton: sameAs-1 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-2 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-3 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-4 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-5 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-1 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . singleton: sameAs-1 rdf: subPropertyOf owl: sameAs ; validation: status \"True\" . hero: Superman owl: sameAs person: Kal-El . singleton: sameAs-6 rdf: subPropertyOf owl: sameAs ; validation: status \"False\" ; validation: Rational \"Tom-Holland does not have Spiderman properties which Peter-parker (fictitious) a.k.a Spiderman has.\" . }","title":"3.3 RDF file formats"},{"location":"09.LinkExport/#4-link-restrictions","text":"Whenever links are annotated, the Lenticular Lens offers a way to take advantage of it. It provides the user with options to filter out links of no interest, an alternative to the inconvenience of having to download all links when not needed. Example-12 lists all possible link restrictions options. For example to make sure that only accepted links are exported, the options Accepted is chosen in Example-12. Example 13: Link Restrictions Options. All : Export all links Accepted : Export only accepted links. Rejected : Export only rejected links. Validated : Export rejected or validated links Not Validated : Export links with no accepted or rejected annotation. Threshold : Export links that pass a predefined threshold condition. This feature applies to links annotated with confidence values.","title":"4. Link Restrictions"},{"location":"09.LinkExport/#5-default-export-table","text":"Example-14 summarises the various options for exporting a linkset or lens. In each set of selection category, a default selection is checked in. This means that, by default, an exported linkset or lens comes along with all its links and its complete annotation in a CSV format . Example 13: Export Table. METADATA STRUCTURE RDF REIFICATION SYNTAX LINK EXPORT FORMAT LINK RESTRICTIONS Partial:Generic Standard RDF reification RDF Turtle-trig All Partial:Specific RDFstar RDF Turtle Accepted Singleton JSON-LD Rejected CSV Validated Not Validated Threshold","title":"5. Default Export Table"},{"location":"10.DataIntegration/","text":"DATA INTEGRATION \u00b6 Work in progress The Lenticular Lens is a mean (link creation, manipulation and validation for data integration) to an end (data extraction for analysis). Here, we show how one can extract the right information for analysis given an integration model that can be achieve using existing linksets and lenses. 1. Integration Model \u00b6 2. Data Extraction \u00b6","title":"11. Data Integration"},{"location":"10.DataIntegration/#data-integration","text":"Work in progress The Lenticular Lens is a mean (link creation, manipulation and validation for data integration) to an end (data extraction for analysis). Here, we show how one can extract the right information for analysis given an integration model that can be achieve using existing linksets and lenses.","title":"DATA INTEGRATION "},{"location":"10.DataIntegration/#1-integration-model","text":"","title":"1. Integration Model "},{"location":"10.DataIntegration/#2-data-extraction","text":"","title":"2. Data Extraction"},{"location":"11.Installation/","text":"Using the Lenticular Lens \u00b6 1. Installation \u00b6 To install a local version of the Lenticular Lens, follow the installation steps described below. Make sure Docker and Docker Compose are installe For Windows and Mac users : install Docker Desktop . Use the provided docker-compose.yml as a baseline. Run docker-compose Visit http://localhost:8000 in your browser Note : This will create a folder pgdata with the database data. To clean up the database and start from scratch, simply remove this folder. 2. API \u00b6 Through Github, the Lenticular Lens API is made available with a set of functions and procedures allowing for the code to be reused and/or extended .","title":"12. Using Lenticular Lens"},{"location":"11.Installation/#using-the-lenticular-lens","text":"","title":"Using the Lenticular Lens"},{"location":"11.Installation/#1-installation","text":"To install a local version of the Lenticular Lens, follow the installation steps described below. Make sure Docker and Docker Compose are installe For Windows and Mac users : install Docker Desktop . Use the provided docker-compose.yml as a baseline. Run docker-compose Visit http://localhost:8000 in your browser Note : This will create a folder pgdata with the database data. To clean up the database and start from scratch, simply remove this folder.","title":"1. Installation"},{"location":"11.Installation/#2-api","text":"Through Github, the Lenticular Lens API is made available with a set of functions and procedures allowing for the code to be reused and/or extended .","title":"2. API"},{"location":"12.References/","text":"REFERENCES \u00b6 [Achichi2017] Achichi, M., Bellahsene, Z., & Todorov, K. (2017). Legato: Results for oaei 2017. In OM ISWC. [Alhajj20014] Alhajj, R., & Rokne, J. (2014). Encyclopedia of social network analysis and mining. Springer Publishing Company, Incorporated. [Beek2018] Beek, W., Raad, J., Wielemaker, J., & Van Harmelen, F. (2018, June). sameas. cc: The closure of 500m owl: sameas statements. In European semantic web conference (pp. 65-80). Springer, Cham. [Choi2009] Choi, H., Katake, A., Choi, S., Kang, Y., & Choe, Y. (2009, December). Probabilistic combination of multiple evidence. In International Conference on Neural Information Processing (pp. 302-311). Springer, Berlin, Heidelberg. [Euzenat2013] J. Euzenat and P. Shvaiko. Ontology matching. Springer-Verlag, Heidelberg (DE), 2 nd edition, 2013. [Frege1952] Frege, G., Geach, P., & Black, M. (1952). Translations from the philosophical writings of Gottlob Frege. Philosophical Library. [Halpin2010] Halpin, H., Hayes, P., McCusker, J., McGuinness, D., & Thompson, H. (2010). When owl: sameas isn\u2019t the same: An analysis of identity in linked data. In International semantic web conference (pp. 305\u2013320). [Idrissou2017] Idrissou, A. K. et al. (2017) \u2018Is my:sameAs the same as your:sameAs?\u2019, in Proceedings of the Knowledge Capture Conference on - K-CAP 2017. New York, New York, USA: ACM Press, pp. 1\u20138. doi: 10.1145/3148011.3148029. [Idrissou2018] Idrissou, A. K., van Harmelen, F. and van den Besselaar, P. (2018) \u2018Network metrics for assessing the quality of entity resolution between multiple datasets\u2019, in Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Springer Verlag, pp. 147\u2013162. doi: 10.1007/978-3-030-03667-6_10. [Idrissou2019] Idrissou, A. et al. (2019) \u2018Contextual entity disambiguation in domains with weak identity criteria: Disambiguating golden age amsterdamers\u2019, in K-CAP 2019 - Proceedings of the 10 th International Conference on Knowledge Capture. Association for Computing Machinery, Inc, pp. 259\u2013262. doi: 10.1145/3360901.3364440. [Jin2018] Jin, Y., & J\u00e1J\u00e1, J. F. (2018). Network summarization with preserved spectral properties. CoRR, abs/1802.04447. [Kovalerchuk2017] Kovalerchuk, B. (2017). Relationships Between Probability and Possibility Theories. In Uncertainty Modeling (pp. 97-122). Springer, Cham. [Linked Data] T. Berners-Lee. Linked Data \u2013 Design issues. 27 July 2006. URL: https://www.w3.org/DesignIssues/LinkedData.html [Lukasiewicz2008] Lukasiewicz, T., & Straccia, U. (2008). Managing uncertainty and vagueness in description logics for the semantic webJournal of Web Semantics, 6(4), 291\u2013308. [Nguyen2014] Nguyen, V., Bodenreider, O., & Sheth, A. (2014). Don\u2019t like RDF reification? making statements about statements using singleton property. In Proceedings of the 23 rd international conference on World wide web (pp. 759\u2013770). [N-ary Relations] Rector, A. and Noy, N. (2006) Defining N-ary Relations on the Semantic Web. Available at: https://www.w3.org/TR/2006/NOTE-swbp-n-aryRelations-20060412/ . [OpenPhacts] Batchelor, C. et al. (2014) \u2018Scientific Lenses to Support Multiple Views over Linked Chemistry Data\u2019, in. Springer International Publishing, pp. 98\u2013113. doi: 10.1007/978-3-319-11964-9_7. [Raad2018] Raad, J., Beek, W., Van Harmelen, F., Pernelle, N., & Sa\u00efs, F. (2018, October). Detecting erroneous identity links on the web using network metrics. In International semantic web conference (pp. 391-407). Springer, Cham. [Raad2020] Raad, J., Beek, W., van Harmelen, F., Wielemaker, J., Pernelle, N., & Sa\u00efs, F. (2020). Constructing and Cleaning Identity Graphs in the LOD Cloud. Data Intelligence, 2(3), 323-352. [RDF11-CONCEPTS] Richard Cyganiak, David Wood, Markus Lanthaler. RDF 1.1 Concepts and Abstract Syntax. W3C Recommendation, 25 February 2014. URL: http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/ . The latest edition is available at http://www.w3.org/TR/rdf11-concepts/ [RFC2141] R. Moats. URN Syntax. May 1997. RFC. URL: http://www.ietf.org/rfc/rfc2141.txt [RFC3305] M. Mealling; R. Denenberg. Report from the Joint W3C/IETF URI Planning Interest Group: Uniform Resource Identifiers (URIs), URLs, and Uniform Resource Names (URNs): Clarifications and Recommendations. August 2002. RFC. URL: http://www.ietf.org/rfc/rfc3305.txt [RFC3986] T. Berners-Lee; R. Fielding; L. Masinter. Uniform Resource Identifier (URI): Generic Syntax. January 2005. RFC. URL: http://www.ietf.org/rfc/rfc3986.txt [RFC3987] M. D\u00fcrst; M. Suignard. Internationalized Resource Identifiers (IRIs). January 2005. RFC. URL: http://www.ietf.org/rfc/rfc3987.txt [RDF * ] [Schoenfisch2014] Schoenfisch, J. (2014). Querying probabilistic ontologies with SPARQLInformatik 2014.s [Sentz2002] Sentz, K., & Ferson, S. (2002). Combination of evidence in Dempster-Shafer theory (Vol. 4015). Albuquerque: Sandia National Laboratories. [Shen2010] Shen, H. W., & Cheng, X. Q. (2010). Spectral methods for the detection of network community structure: a comparative analysis. Journal of Statistical Mechanics: Theory and Experiment, 2010(10), P10020. [Toivonen2011] Toivonen, H., Zhou, F., Hartikainen, A., & Hinkka, A. (2011). Compression of weighted graphs. In Proceedings of the 17 th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 965\u2013973). [VoID] K. Alexander, R. Cyganiak, M. Hausenblas, and J. Zhao. Describing LinkedDatasets with the VoID Vocabulary. Technical report, 2011. W3C Note. URL: https://www.w3.org/TR/void/ . [Volz2009] Volz, J., Bizer, C., Gaedke, M., & Kobilarov, G. (2009). Silk-a link discovery framework for the web of data.LDOW, 538, 53.","title":"13. References"},{"location":"12.References/#references","text":"[Achichi2017] Achichi, M., Bellahsene, Z., & Todorov, K. (2017). Legato: Results for oaei 2017. In OM ISWC. [Alhajj20014] Alhajj, R., & Rokne, J. (2014). Encyclopedia of social network analysis and mining. Springer Publishing Company, Incorporated. [Beek2018] Beek, W., Raad, J., Wielemaker, J., & Van Harmelen, F. (2018, June). sameas. cc: The closure of 500m owl: sameas statements. In European semantic web conference (pp. 65-80). Springer, Cham. [Choi2009] Choi, H., Katake, A., Choi, S., Kang, Y., & Choe, Y. (2009, December). Probabilistic combination of multiple evidence. In International Conference on Neural Information Processing (pp. 302-311). Springer, Berlin, Heidelberg. [Euzenat2013] J. Euzenat and P. Shvaiko. Ontology matching. Springer-Verlag, Heidelberg (DE), 2 nd edition, 2013. [Frege1952] Frege, G., Geach, P., & Black, M. (1952). Translations from the philosophical writings of Gottlob Frege. Philosophical Library. [Halpin2010] Halpin, H., Hayes, P., McCusker, J., McGuinness, D., & Thompson, H. (2010). When owl: sameas isn\u2019t the same: An analysis of identity in linked data. In International semantic web conference (pp. 305\u2013320). [Idrissou2017] Idrissou, A. K. et al. (2017) \u2018Is my:sameAs the same as your:sameAs?\u2019, in Proceedings of the Knowledge Capture Conference on - K-CAP 2017. New York, New York, USA: ACM Press, pp. 1\u20138. doi: 10.1145/3148011.3148029. [Idrissou2018] Idrissou, A. K., van Harmelen, F. and van den Besselaar, P. (2018) \u2018Network metrics for assessing the quality of entity resolution between multiple datasets\u2019, in Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Springer Verlag, pp. 147\u2013162. doi: 10.1007/978-3-030-03667-6_10. [Idrissou2019] Idrissou, A. et al. (2019) \u2018Contextual entity disambiguation in domains with weak identity criteria: Disambiguating golden age amsterdamers\u2019, in K-CAP 2019 - Proceedings of the 10 th International Conference on Knowledge Capture. Association for Computing Machinery, Inc, pp. 259\u2013262. doi: 10.1145/3360901.3364440. [Jin2018] Jin, Y., & J\u00e1J\u00e1, J. F. (2018). Network summarization with preserved spectral properties. CoRR, abs/1802.04447. [Kovalerchuk2017] Kovalerchuk, B. (2017). Relationships Between Probability and Possibility Theories. In Uncertainty Modeling (pp. 97-122). Springer, Cham. [Linked Data] T. Berners-Lee. Linked Data \u2013 Design issues. 27 July 2006. URL: https://www.w3.org/DesignIssues/LinkedData.html [Lukasiewicz2008] Lukasiewicz, T., & Straccia, U. (2008). Managing uncertainty and vagueness in description logics for the semantic webJournal of Web Semantics, 6(4), 291\u2013308. [Nguyen2014] Nguyen, V., Bodenreider, O., & Sheth, A. (2014). Don\u2019t like RDF reification? making statements about statements using singleton property. In Proceedings of the 23 rd international conference on World wide web (pp. 759\u2013770). [N-ary Relations] Rector, A. and Noy, N. (2006) Defining N-ary Relations on the Semantic Web. Available at: https://www.w3.org/TR/2006/NOTE-swbp-n-aryRelations-20060412/ . [OpenPhacts] Batchelor, C. et al. (2014) \u2018Scientific Lenses to Support Multiple Views over Linked Chemistry Data\u2019, in. Springer International Publishing, pp. 98\u2013113. doi: 10.1007/978-3-319-11964-9_7. [Raad2018] Raad, J., Beek, W., Van Harmelen, F., Pernelle, N., & Sa\u00efs, F. (2018, October). Detecting erroneous identity links on the web using network metrics. In International semantic web conference (pp. 391-407). Springer, Cham. [Raad2020] Raad, J., Beek, W., van Harmelen, F., Wielemaker, J., Pernelle, N., & Sa\u00efs, F. (2020). Constructing and Cleaning Identity Graphs in the LOD Cloud. Data Intelligence, 2(3), 323-352. [RDF11-CONCEPTS] Richard Cyganiak, David Wood, Markus Lanthaler. RDF 1.1 Concepts and Abstract Syntax. W3C Recommendation, 25 February 2014. URL: http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/ . The latest edition is available at http://www.w3.org/TR/rdf11-concepts/ [RFC2141] R. Moats. URN Syntax. May 1997. RFC. URL: http://www.ietf.org/rfc/rfc2141.txt [RFC3305] M. Mealling; R. Denenberg. Report from the Joint W3C/IETF URI Planning Interest Group: Uniform Resource Identifiers (URIs), URLs, and Uniform Resource Names (URNs): Clarifications and Recommendations. August 2002. RFC. URL: http://www.ietf.org/rfc/rfc3305.txt [RFC3986] T. Berners-Lee; R. Fielding; L. Masinter. Uniform Resource Identifier (URI): Generic Syntax. January 2005. RFC. URL: http://www.ietf.org/rfc/rfc3986.txt [RFC3987] M. D\u00fcrst; M. Suignard. Internationalized Resource Identifiers (IRIs). January 2005. RFC. URL: http://www.ietf.org/rfc/rfc3987.txt [RDF * ] [Schoenfisch2014] Schoenfisch, J. (2014). Querying probabilistic ontologies with SPARQLInformatik 2014.s [Sentz2002] Sentz, K., & Ferson, S. (2002). Combination of evidence in Dempster-Shafer theory (Vol. 4015). Albuquerque: Sandia National Laboratories. [Shen2010] Shen, H. W., & Cheng, X. Q. (2010). Spectral methods for the detection of network community structure: a comparative analysis. Journal of Statistical Mechanics: Theory and Experiment, 2010(10), P10020. [Toivonen2011] Toivonen, H., Zhou, F., Hartikainen, A., & Hinkka, A. (2011). Compression of weighted graphs. In Proceedings of the 17 th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 965\u2013973). [VoID] K. Alexander, R. Cyganiak, M. Hausenblas, and J. Zhao. Describing LinkedDatasets with the VoID Vocabulary. Technical report, 2011. W3C Note. URL: https://www.w3.org/TR/void/ . [Volz2009] Volz, J., Bizer, C., Gaedke, M., & Kobilarov, G. (2009). Silk-a link discovery framework for the web of data.LDOW, 538, 53.","title":"REFERENCES"}]}